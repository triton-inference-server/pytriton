{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#pytriton","title":"PyTriton","text":"<p>PyTriton is a Flask/FastAPI-like interface that simplifies Triton's deployment in Python environments. The library allows serving Machine Learning models directly from Python through NVIDIA's Triton Inference Server.</p>"},{"location":"#how-it-works","title":"How it works?","text":"<p>In PyTriton, as in Flask or FastAPI, you can define any Python function that executes a machine learning model prediction and exposes it through an HTTP/gRPC API. PyTriton installs Triton Inference Server in your environment and uses it for handling HTTP/gRPC requests and responses. Our library provides a Python API that allows attaching a Python function to Triton and a communication layer to send/receive data between Triton and the function. This solution helps utilize the performance features of Triton Inference Server, such as dynamic batching or response cache, without changing your model environment. Thus, it improves the performance of running inference on GPU for models implemented in Python. The solution is framework-agnostic and can be used along with frameworks like PyTorch, TensorFlow, or JAX.</p>"},{"location":"#serving-the-models","title":"Serving the models","text":"<p>PyTriton provides an option to serve your Python model using Triton Inference Server to handle HTTP/gRPC requests and pass the input/output tensors to and from the model. We use a blocking mode where the application is a long-lived process deployed in your cluster to serve the requests from clients.</p> <p>Before you run the model for serving the inference callback function, it has to be defined. The inference callback receives the inputs and should return the model outputs:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n\n\n@batch\ndef infer_fn(**inputs: np.ndarray):\n    input1, input2 = inputs.values()\n    outputs = model(input1, input2)\n    return [outputs]\n</code></pre> <p>The <code>infer_fn</code> receives the batched input data for the model and should return the batched outputs.</p> <p>In the next step, you need to create a connection between Triton and the model. For that purpose, the <code>Triton</code> class has to be used, and the <code>bind</code> method is required to be called to create a dedicated connection between Triton Inference Server and the defined <code>infer_fn</code>.</p> <p>In the blocking mode, we suggest using the <code>Triton</code> object as a context manager where multiple models can be loaded in the way presented below:</p> <pre><code>from pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\n\nwith Triton() as triton:\n    triton.bind(\n        model_name=\"MyModel\",\n        infer_func=infer_fn,\n        inputs=[\n            Tensor(dtype=bytes, shape=(1,)),  # sample containing single bytes value\n            Tensor(dtype=bytes, shape=(-1,)),  # sample containing vector of bytes\n        ],\n        outputs=[\n            Tensor(dtype=np.float32, shape=(-1,)),\n        ],\n        config=ModelConfig(max_batch_size=16),\n    )\n</code></pre> <p>At this point, you have defined how the model has to be handled by Triton and where the HTTP/gRPC requests for the model have to be directed. The last part for serving the model is to call the <code>serve</code> method on the Triton object:</p> <pre><code>with Triton() as triton:\n    # ...\n    triton.serve()\n</code></pre> <p>When the <code>.serve()</code> method is called on the <code>Triton</code> object, the inference queries can be sent to <code>localhost:8000/v2/models/MyModel</code>, and the <code>infer_fn</code> is called to handle the inference query.</p>"},{"location":"#working-in-the-jupyter-notebook","title":"Working in the Jupyter Notebook","text":"<p>The package provides an option to work with your model inside the Jupyter Notebook. We call it a background mode where the model is deployed on Triton Inference Server for handling HTTP/gRPC requests, but there are other actions that you want to perform after loading and starting serving the model.</p> <p>Having the <code>infer_fn</code> defined in the same way as described in the serving the models section, you can use the <code>Triton</code> object without a context:</p> <pre><code>from pytriton.triton import Triton\ntriton = Triton()\n</code></pre> <p>In the next step, the model has to be loaded for serving in Triton Inference Server (which is also the same as in the serving example):</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\n\n@batch\ndef infer_fn(**inputs: np.ndarray):\n    input1, input2 = inputs.values()\n    outputs = input1 + input2\n    return [outputs]\n\ntriton.bind(\n    model_name=\"MyModel\",\n    infer_func=infer_fn,\n    inputs=[\n        Tensor(shape=(1,), dtype=np.float32),\n        Tensor(shape=(-1,), dtype=np.float32),\n    ],\n    outputs=[Tensor(shape=(-1,), dtype=np.float32)],\n    config=ModelConfig(max_batch_size=16),\n)\n</code></pre> <p>Finally, to run the model in background mode, use the <code>run</code> method:</p> <pre><code>triton.run()\n</code></pre> <p>When the <code>.run()</code> method is called on the <code>Triton</code> object, the inference queries can be sent to <code>localhost:8000/v2/models/MyModel</code>, and the <code>infer_fn</code> is called to handle the inference query.</p> <p>The Triton server can be stopped at any time using the <code>stop</code> method:</p> <pre><code>triton.stop()\n</code></pre>"},{"location":"#in-depth-topics-and-examples","title":"In-depth Topics and Examples","text":""},{"location":"#model-deployment","title":"Model Deployment","text":"<p>Fine-tune your model deployment strategy with our targeted documentation:</p> <ul> <li>Initialize Triton for seamless startup.</li> <li>Bind your models to Triton for enhanced communication.</li> <li>Adjust your binding configurations for improved control.</li> <li>Expand your reach by deploying on clusters.</li> <li>Master the use of Triton in remote mode.</li> </ul>"},{"location":"#inference-management","title":"Inference Management","text":"<p>Hone your understanding of inference process management through PyTriton:</p> <ul> <li>Tailor the Inference Callable to your model's requirements.</li> <li>Use decorators to simplify your inference callbacks.</li> <li>Incorporate custom parameters/headers for flexibility. data.</li> </ul>"},{"location":"#dive-into-examples","title":"Dive into Examples","text":"<p>Visit the examples directory for a curated selection of use cases ranging from basic to advanced, including:</p> <ul> <li>Standard model serving scenarios with different frameworks: PyTorch, TensorFlow2, JAX.</li> <li>Advanced setups like online learning, multi-node execution, or Kubernetes deployments.</li> </ul>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any obstacles, our Known Issues page is a helpful resource for troubleshooting common challenges.</p>"},{"location":"#streaming-alpha","title":"Streaming (alpha)","text":"<p>We introduced new alpha feature to PyTriton that allows to stream partial responses from a model. It is based on NVIDIA Triton Inference deocoupled models feature. Look at example in examples/huggingface_dialogpt_streaming_pytorch.</p>"},{"location":"#profiling-model","title":"Profiling model","text":"<p>The Perf Analyzer can be used to profile models served through PyTriton. We have prepared an example of using the Perf Analyzer to profile the BART PyTorch model. The example code can be found in examples/perf_analyzer.</p> <p>Open Telemetry is a set of APIs, libraries, agents, and instrumentation to provide observability for cloud-native software. We have prepared an guide on how to use Open Telemetry with PyTriton.</p>"},{"location":"#what-next","title":"What next?","text":"<p>Read more about using PyTriton in the Quick Start, Examples and find more options on how to configure Triton, models, and deployment on a cluster in the Deploying Models section.</p> <p>The details about classes and methods can be found in the API Reference page.</p> <p>If there are any issues diffcult to invastigate, it is possible to use pytriton-check tool. Usage is described in the Basic Troubleshooting section.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#0513-2024-11-16","title":"0.5.13 (2024-11-16)","text":"<ul> <li>Fix: OpenTelemetry version enforced as 1.27.0 to avoid compatibility issues with tritonclient.</li> <li>Fix: Timeout error in async wait removed. Thanks @catwell.</li> <li> <p>Fix: Invalid call for <code>shutdown_default_executor</code> at Python 3.8 removed.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.51.0</p> </li> </ul>"},{"location":"CHANGELOG/#0512-2024-10-13","title":"0.5.12 (2024-10-13)","text":"<ul> <li> <p>New: Inference callable receives Request object with requested output names. Thanks @catwell.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.50.0</p> </li> </ul>"},{"location":"CHANGELOG/#0511-2024-09-18","title":"0.5.11 (2024-09-18)","text":"<ul> <li>Version of Triton Inference Server embedded in wheel: 2.49.0</li> </ul>"},{"location":"CHANGELOG/#0510-2024-08-02","title":"0.5.10 (2024-08-02)","text":"<ul> <li>Version of Triton Inference Server embedded in wheel: 2.48.0</li> </ul>"},{"location":"CHANGELOG/#059-2024-07-31","title":"0.5.9 (2024-07-31)","text":"<ul> <li>Version of Triton Inference Server embedded in wheel: 2.47.0</li> </ul>"},{"location":"CHANGELOG/#058-2024-06-27","title":"0.5.8 (2024-06-27)","text":"<ul> <li>New: PyTriton workspace clean raises warning <code>CleanupWarning</code> instead of <code>OSError</code> for file system failures</li> <li> <p>Fix: Set NumPy version upper limit <code>&lt;2.0.0</code></p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.46.0</p> </li> </ul>"},{"location":"CHANGELOG/#057-2024-06-19","title":"0.5.7 (2024-06-19)","text":"<ul> <li> <p>New: Open Telemetry propagation support for tracing</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.45.0</p> </li> </ul>"},{"location":"CHANGELOG/#056-2024-06-17","title":"0.5.6 (2024-06-17)","text":"<ul> <li>New: Add PyTriton Check Tool to perform preliminary checks on the environment where PyTriton is deployed.</li> <li>Change: limited the <code>tritonclient</code> pacakge extras to http and grpc only</li> <li>Fix: Pin grpc-tools version to handle grpc issue in tritonclient</li> <li>Build scripts update</li> <li>upgrade cmake version during build</li> <li> <p>automatically configure wheel name based on <code>glibc</code> version</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.44.0</p> </li> </ul>"},{"location":"CHANGELOG/#055-2024-04-15","title":"0.5.5 (2024-04-15)","text":"<ul> <li> <p>Fix: Performance improvements</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.44.0</p> </li> </ul>"},{"location":"CHANGELOG/#054-2024-04-09","title":"0.5.4 (2024-04-09)","text":"<ul> <li> <p>New: Python 3.12 support</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.44.0</p> </li> </ul>"},{"location":"CHANGELOG/#053-2024-03-09","title":"0.5.3 (2024-03-09)","text":"<ul> <li> <p>New: Relaxed wheel dependencies to avoid forced downgrading of protobuf and other packages in the NVIDIA 24.02 docker containers for PyTorch and other frameworks.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.43.0</p> </li> </ul>"},{"location":"CHANGELOG/#052-2024-02-29","title":"0.5.2 (2024-02-29)","text":"<ul> <li>Add: Add <code>TritonLifecyclePolicy</code> parameter to Triton class to control the lifecycle of the Triton Inference Server   (Triton Inference Server can be started at the beginning of the context - default behavior, or at the call of <code>run</code> or <code>serve</code> method),   second flag in this parameter indicates if model configs should be created in local filesystem or passed to Triton Inference Server and managed by it.</li> <li>Fix: ModelManager does not raise <code>tritonclient.grpc.InferenceServerException</code> for <code>stop</code> method when HTTP endpoint is disabled in Triton configuration.</li> <li> <p>Fix: Methods can be used as the inference callable.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.42.0</p> </li> </ul>"},{"location":"CHANGELOG/#051-2024-02-09","title":"0.5.1 (2024-02-09)","text":"<ul> <li> <p>Fix: ModelClient does not raise <code>gevent.exceptions.InvalidThreadUseError</code> when destroyed in a different thread.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.42.0</p> </li> </ul>"},{"location":"CHANGELOG/#050-2024-01-09","title":"0.5.0 (2024-01-09)","text":"<ul> <li>New: Decoupled models support</li> <li>New: AsyncioDecoupledModelClient, which works in async frameworks and decoupled Triton models like some Large Language Models.</li> <li> <p>Fix: Fixed a bug that prevented getting the log level when HTTP endpoint was disabled. Thanks @catwell.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.41.0</p> </li> </ul>"},{"location":"CHANGELOG/#042-2023-12-05","title":"0.4.2 (2023-12-05)","text":"<ul> <li>New: You can create a client from an existing client instance or model configuration to avoid loading model configuration from the server.</li> <li>New: Introduced warning system using the <code>warnings</code> module.</li> <li>Fix: Experimental client for decoupled models prevents sending another request, when responses from previous request are not consumed, blocks close until stream is stopped.</li> <li>Fix: Leak of ModelClient during Triton creation</li> <li>Fix: Fixed non-declared project dependencies (removed from use in code or added to package dependencies)</li> <li> <p>Fix: Remote model is being unloaded from Triton when RemoteTriton is closed.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.39.0</p> </li> </ul>"},{"location":"CHANGELOG/#041-2023-11-09","title":"0.4.1 (2023-11-09)","text":"<ul> <li>New: Place where workspaces with temporary Triton model repositories and communication file sockets can be configured by <code>$PYTRITON_HOME</code> environment variable</li> <li>Fix: Recover handling <code>KeyboardInterrupt</code> in <code>triton.serve()</code></li> <li>Fix: Remove limit for handling bytes dtype tensors</li> <li>Build scripts update</li> <li> <p>Added support for arm64 platform builds</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.39.0</p> </li> </ul>"},{"location":"CHANGELOG/#040-2023-10-20","title":"0.4.0 (2023-10-20)","text":"<ul> <li>New: Remote Mode - PyTriton can be used to connect to a remote Triton Inference Server</li> <li>Introduced RemoteTriton class which can be used to connect to a remote Triton Inference Server     running on the same machine, by passing triton url.</li> <li>Changed Triton lifecycle - now the Triton Inference Server is started while entering the context.     This allows to load models dynamically to the running server while calling the bind method.     It is still allowed to create Triton instance without entering the context and bind models before starting     the server (in this case the models are lazy loaded when calling run or serve method like it worked before).</li> <li>In RemoteTriton class, calling enter or connect method connects to triton server, so we can safely load models     while binding inference functions (if RemoteTriton is used without context manager, models are lazy loaded     when calling connect or serve method).</li> <li>Change: <code>@batch</code> decorator raises a <code>ValueError</code> if any of the outputs have a different batch size than expected.</li> <li> <p>fix: gevent resources leak in <code>FuturesModelClient</code></p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.36.0</p> </li> </ul>"},{"location":"CHANGELOG/#031-2023-09-26","title":"0.3.1 (2023-09-26)","text":"<ul> <li>Change: <code>KeyboardInterrupt</code> is now handled in <code>triton.serve()</code>. PyTriton hosting scripts return an exit code of 0 instead of 130 when they receive a SIGINT signal.</li> <li> <p>Fix: Addressed potential instability in shared memory management.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.36.0</p> </li> </ul>"},{"location":"CHANGELOG/#030-2023-09-05","title":"0.3.0 (2023-09-05)","text":"<ul> <li>new: Support for multiple Python versions starting from 3.8+</li> <li>new: Added support for decoupled models enabling to support streaming models (alpha state)</li> <li> <p>change: Upgraded Triton Inference Server binaries to version 2.36.0. Note that this Triton Inference Server requires glibc 2.35+ or a more recent version.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.36.0</p> </li> </ul>"},{"location":"CHANGELOG/#025-2023-08-24","title":"0.2.5 (2023-08-24)","text":"<ul> <li>new: Allow to execute multiple PyTriton instances in the same process and/or host</li> <li> <p>fix: Invalid flags for Proxy Backend configuration passed to Triton</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#024-2023-08-10","title":"0.2.4 (2023-08-10)","text":"<ul> <li>new: Introduced <code>strict</code> flag in <code>Triton.bind</code> which enables data types and shapes validation of inference callable outputs   against model config</li> <li>new: <code>AsyncioModelClient</code> which works in FastAPI and other async frameworks</li> <li>fix: <code>FuturesModelClient</code> do not raise <code>gevent.exceptions.InvalidThreadUseError</code></li> <li> <p>fix: Do not throw TimeoutError if could not connect to server during model verification</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#023-2023-07-21","title":"0.2.3 (2023-07-21)","text":"<ul> <li>Improved verification of Proxy Backend environment when running under same Python interpreter</li> <li> <p>Fixed pytriton.version to represent currently installed version</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#022-2023-07-19","title":"0.2.2 (2023-07-19)","text":"<ul> <li>Added <code>inference_timeout_s</code> parameters to client classes</li> <li>Renamed <code>PyTritonClientUrlParseError</code> to <code>PyTritonClientInvalidUrlError</code></li> <li><code>ModelClient</code> and <code>FuturesModelClient</code> methods raise <code>PyTritonClientClosedError</code> when used after client is closed</li> <li>Pinned tritonclient dependency due to issues with tritonclient &gt;= 2.34 on systems with glibc version lower than 2.34</li> <li> <p>Added warning after Triton Server setup and teardown while using too verbose logging level as it may cause a significant performance drop in model inference</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#021-2023-06-28","title":"0.2.1 (2023-06-28)","text":"<ul> <li>Fixed handling <code>TritonConfig.cache_directory</code> option - the directory was always overwritten with the default value.</li> <li>Fixed tritonclient dependency - PyTriton need tritonclient supporting http headers and parameters</li> <li> <p>Improved shared memory usage to match 64MB limit (default value for Docker, Kubernetes) reducing the initial size for PyTriton Proxy Backend.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#020-2023-05-30","title":"0.2.0 (2023-05-30)","text":"<ul> <li>Added support for using custom HTTP/gRPC request headers and parameters.</li> </ul> <p>This change breaks backward compatibility of the inference function signature.   The undecorated inference function now accepts a list of <code>Request</code> instances instead   of a list of dictionaries. The <code>Request</code> class contains data for inputs and parameters   for combined parameters and headers.</p> <p>See documentation for further information</p> <ul> <li>Added <code>FuturesModelClient</code> which enables sending inference requests in a parallel manner.</li> <li> <p>Added displaying documentation link after models are loaded.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#015-2023-05-12","title":"0.1.5 (2023-05-12)","text":"<ul> <li>Improved <code>pytriton.decorators.group_by_values</code> function</li> <li>Modified the function to avoid calling the inference callable on each individual sample when grouping by string/bytes input</li> <li>Added <code>pad_fn</code> argument for easy padding and combining of the inference results</li> <li>Fixed Triton binaries search</li> <li> <p>Improved Workspace management (remove workspace on shutdown)</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#014-2023-03-16","title":"0.1.4 (2023-03-16)","text":"<ul> <li>Add validation of the model name passed to Triton bind method.</li> <li> <p>Add monkey patching of <code>InferenceServerClient.__del__</code> method to prevent unhandled exceptions.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#013-2023-02-20","title":"0.1.3 (2023-02-20)","text":"<ul> <li> <p>Fixed getting model config in <code>fill_optionals</code> decorator.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#012-2023-02-14","title":"0.1.2 (2023-02-14)","text":"<ul> <li>Fixed wheel build to support installations on operating systems with glibc version 2.31 or higher.</li> <li>Updated the documentation on custom builds of the package.</li> <li>Change: TritonContext instance is shared across bound models and contains model_configs dictionary.</li> <li> <p>Fixed support of binding multiple models that uses methods of the same class.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#011-2023-01-31","title":"0.1.1 (2023-01-31)","text":"<ul> <li>Change: The <code>@first_value</code> decorator has been updated with new features:</li> <li>Renamed from <code>@first_values</code> to <code>@first_value</code></li> <li>Added a <code>strict</code> flag to toggle the checking of equality of values on a single selected input of the request. Default is True</li> <li>Added a <code>squeeze_single_values</code> flag to toggle the squeezing of single value ND arrays to scalars. Default is True</li> <li>Fix: <code>@fill_optionals</code> now supports non-batching models</li> <li>Fix: <code>@first_value</code> fixed to work with optional inputs</li> <li>Fix: <code>@group_by_values</code> fixed to work with string inputs</li> <li> <p>Fix: <code>@group_by_values</code> fixed to work per sample-wise</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#010-2023-01-12","title":"0.1.0 (2023-01-12)","text":"<ul> <li> <p>Initial release of PyTriton</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are much appreciated! Every little helps, and we will always give credit.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/triton-inference-server/pytriton/issues.</p> <p>When reporting a bug, please include the following information:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Browse through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>The PyTriton could always use more documentation, whether as part of the official PyTriton docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/triton-inference-server/pytriton/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> </ul>"},{"location":"CONTRIBUTING/#sign-your-work","title":"Sign your Work","text":"<p>We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have the rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not Signed-Off will not be accepted.</p> <p>To sign off on a commit, simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes:</p> <pre><code>$ git commit -s -m \"Add a cool feature.\"\n</code></pre> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>By doing this, you certify the following:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":""},{"location":"CONTRIBUTING/#local-development","title":"Local Development","text":"<p>Ready to contribute? Here's how to set up the <code>PyTriton</code> for local development.</p> <ol> <li>Fork the <code>PyTriton</code> repo on GitHub.</li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/pytriton.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, here's how you set up your fork for local development:</p> <pre><code>$ mkvirtualenv pytriton\n$ cd pytriton/\n</code></pre> <p>If you do not use the virtualenvwrapper package, you can initialize a virtual environment using the pure Python command:</p> <pre><code>$ python -m venv pytriton\n$ cd pytriton/\n$ source bin/activate\n</code></pre> <p>Once the virtualenv is activated, install the development dependencies:</p> <pre><code>$ make install-dev\n</code></pre> </li> <li> <p>Extract Triton Server to your environment so you can debug PyTriton while serving some models on Triton:</p> <pre><code>$ make extract-triton\n</code></pre> </li> <li> <p>Install pre-commit hooks:</p> <pre><code>$ pre-commit install\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass linters and the    tests, including testing other Python versions with tox:</p> <pre><code>$ make lint  # will run, among others, flake8 and pytype linters\n$ make test  # will run a test on your current virtualenv\n</code></pre> <p>To run a subset of tests:</p> <pre><code>$ pytest tests.test_subset\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -s -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, you should update the docs. Put your new functionality into a function with a docstring and add the feature to the list in README.md.</li> </ol>"},{"location":"CONTRIBUTING/#version-management","title":"Version management","text":"<p>PyTriton follows the Semantic Versioning scheme for versioning. Official releases can be found on PyPI and GitHub releases. The most up-to-date development version is available on the <code>main</code> branch, which may include hotfixes that have not yet been released through the standard channels. To install the latest development version, refer to the instructions in the Building binaries from source guide.</p>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Add/update docstrings as defined in Google Style Guide.</p>"},{"location":"CONTRIBUTING/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>PyTriton requires that all contributors (or their corporate entity) send a signed copy of the Contributor License Agreement to triton-cla@nvidia.com.</p> <p>NOTE: Contributors with no company affiliation can fill <code>N/A</code> in the <code>Corporation Name</code> and <code>Corporation Address</code> fields.</p>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"binding_configuration/","title":"Binding Configuration","text":""},{"location":"binding_configuration/#binding-configuration","title":"Binding Configuration","text":"<p>The additional configuration of binding the model for running a model through the Triton Inference Server can be provided in the <code>config</code> argument in the <code>bind</code> method. This section describes the possible configuration enhancements. The configuration of the model can be adjusted by overriding the defaults for the <code>ModelConfig</code> object.</p> <pre><code>from pytriton.model_config.common import DynamicBatcher\n\nclass ModelConfig:\n    batching: bool = True\n    max_batch_size: int = 4\n    batcher: DynamicBatcher = DynamicBatcher()\n    response_cache: bool = False\n</code></pre>"},{"location":"binding_configuration/#batching","title":"Batching","text":"<p>The batching feature collects one or more samples and passes them to the model together. The model processes multiple samples at the same time and returns the output for all the samples processed together.</p> <p>Batching can significantly improve throughput. Processing multiple samples at the same time leverages the benefits of utilizing GPU performance for inference.</p> <p>The Triton Inference Server is responsible for collecting multiple incoming requests into a single batch. The batch is passed to the model, which improves the inference performance (throughput and latency). This feature is called <code>dynamic batching</code>, which collects samples from multiple clients into a single batch processed by the model.</p> <p>On the PyTriton side, the <code>infer_fn</code> obtain the fully created batch by Triton Inference Server so the only responsibility is to perform computation and return the output.</p> <p>By default, batching is enabled for the model. The default behavior for Triton is to have dynamic batching enabled. If your model does not support batching, use <code>batching=False</code> to disable it in Triton.</p>"},{"location":"binding_configuration/#maximal-batch-size","title":"Maximal batch size","text":"<p>The maximal batch size defines the number of samples that can be processed at the same time by the model. This configuration has an impact not only on throughput but also on memory usage, as a bigger batch means more data loaded to the memory at the same time.</p> <p>The <code>max_batch_size</code> has to be a value greater than or equal to 1.</p>"},{"location":"binding_configuration/#dynamic-batching","title":"Dynamic batching","text":"<p>The dynamic batching is a Triton Inference Server feature and can be configured by defining the <code>DynamicBatcher</code> object:</p> <pre><code>from typing import Dict, Optional\nfrom pytriton.model_config.common import QueuePolicy\n\nclass DynamicBatcher:\n    max_queue_delay_microseconds: int = 0\n    preferred_batch_size: Optional[list] = None\n    preserve_ordering: bool = False\n    priority_levels: int = 0\n    default_priority_level: int = 0\n    default_queue_policy: Optional[QueuePolicy] = None\n    priority_queue_policy: Optional[Dict[int, QueuePolicy]] = None\n</code></pre> <p>More about dynamic batching can be found in the Triton Inference Server documentation and API spec</p>"},{"location":"binding_configuration/#decoupled-models","title":"Decoupled models","text":"<p>Triton can support models that send multiple responses for a request or zero responses for a request. A decoupled model may also send responses out-of-order relative to the order that the request batches are executed. This allows backend to deliver response whenever it deems fit.</p> <p>To enable this feature, set <code>decoupled=True</code> in <code>ModelConfig</code>.</p>"},{"location":"binding_configuration/#response-cache","title":"Response cache","text":"<p>The Triton Inference Server provides functionality to use a cached response for the model. To use the response cache:</p> <ul> <li>provide the <code>cache_config</code> in <code>TritonConfig</code></li> <li>set <code>response_cache=True</code> in <code>ModelConfig</code></li> </ul> <p>More about response cache can be found in the Triton Response Cache page.</p> <p>Example:</p> <pre><code>import numpy as np\n\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\n\ntriton_config = TritonConfig(\n    cache_config=[f\"local,size={1024 * 1024}\"],  # 1MB\n)\n\n@batch\ndef _add_sub(**inputs):\n    a_batch, b_batch = inputs.values()\n    add_batch = a_batch + b_batch\n    sub_batch = a_batch - b_batch\n    return {\"add\": add_batch, \"sub\": sub_batch}\n\nwith Triton(config=triton_config) as triton:\n    triton.bind(\n        model_name=\"AddSub\",\n        infer_func=_add_sub,\n        inputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\n        outputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\n        config=ModelConfig(max_batch_size=8, response_cache=True)\n    )\n    ...\n</code></pre>"},{"location":"binding_models/","title":"Binding Model to Triton","text":""},{"location":"binding_models/#binding-models-to-triton","title":"Binding Models to Triton","text":"<p>The Triton class provides methods to bind one or multiple models to the Triton server in order to expose HTTP/gRPC endpoints for inference serving:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n\n\n@batch\ndef infer_fn(**inputs: np.ndarray):\n    input1, input2 = inputs.values()\n    outputs = model(input1, input2)\n    return [outputs]\n\nwith Triton() as triton:\n  triton.bind(\n      model_name=\"ModelName\",\n      infer_func=infer_fn,\n      inputs=[\n          Tensor(shape=(1,), dtype=np.bytes_),  # sample containing single bytes value\n          Tensor(shape=(-1,), dtype=np.bytes_)  # sample containing vector of bytes\n      ],\n      outputs=[\n          Tensor(shape=(-1,), dtype=np.float32),\n      ],\n      config=ModelConfig(max_batch_size=8),\n      strict=True,\n  )\n</code></pre> <p>The <code>bind</code> method's mandatory arguments are:</p> <ul> <li><code>model_name</code>: defines under which name the model is available in Triton Inference Server</li> <li><code>infer_func</code>: function or Python <code>Callable</code> object which obtains the data passed in the request and returns the output</li> <li><code>inputs</code>: defines the number, types, and shapes for model inputs</li> <li><code>outputs</code>: defines the number, types, and shapes for model outputs</li> <li><code>config</code>: more customization for model deployment and behavior on the Triton server</li> <li><code>strict</code>: enable inference callable output validation of data types and shapes against provided model config (default: False)</li> </ul> <p>Once the <code>bind</code> method is called, the model is created in the Triton Inference Server model store under the provided <code>model_name</code>.</p>"},{"location":"binding_models/#inference-callable","title":"Inference Callable","text":"<p>The inference callable is an entry point for inference. This can be any callable that receives the data for model inputs in the form of a list of request dictionaries where input names are mapped into ndarrays. Input can be also adapted to different more convenient forms using a set of decorators. More details about designing inference callable and using of decorators can be found in Inference Callable page.</p> <p>In the simplest implementation for functionality that passes input data on output, a lambda can be used:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n\nwith Triton() as triton:\n  triton.bind(\n      model_name=\"Identity\",\n      infer_func=lambda requests: requests,\n      inputs=[Tensor(dtype=np.float32, shape=(1,))],\n      outputs=[Tensor(dtype=np.float32, shape=(1,))],\n      config=ModelConfig(max_batch_size=8)\n  )\n</code></pre>"},{"location":"binding_models/#multi-instance-model-inference","title":"Multi-instance model inference","text":"<p>Multi-instance model inference is a mechanism for loading multiple instances of the same model and calling them alternately (to hide transfer overhead).</p> <p>With the <code>Triton</code> class, it can be realized by providing the list of multiple inference callables to <code>Triton.bind</code> in the <code>infer_func</code> parameter.</p> <p>The example presents multiple instances of the Linear PyTorch model loaded on separate devices.</p> <p>First, define the wrapper class for the inference handler. The class initialization receives a model and device as arguments. The inference handling is done by method <code>__call__</code> where the <code>model</code> instance is called:</p> <pre><code>import torch\nfrom pytriton.decorators import batch\n\n\nclass _InferFuncWrapper:\n    def __init__(self, model: torch.nn.Module, device: str):\n        self._model = model\n        self._device = device\n\n    @batch\n    def __call__(self, **inputs):\n        (input1_batch,) = inputs.values()\n        input1_batch_tensor = torch.from_numpy(input1_batch).to(self._device)\n        output1_batch_tensor = self._model(input1_batch_tensor)\n        output1_batch = output1_batch_tensor.cpu().detach().numpy()\n        return [output1_batch]\n</code></pre> <p>Next, create a factory function where a model and instances of <code>_InferFuncWrapper</code> are created - one per each device:</p> <pre><code>def _infer_function_factory(devices):\n    infer_fns = []\n    for device in devices:\n        model = torch.nn.Linear(20, 30).to(device).eval()\n        infer_fns.append(_InferFuncWrapper(model=model, device=device))\n\n    return infer_fns\n</code></pre> <p>Finally, the list of callable objects is passed to <code>infer_func</code> parameter of the <code>Triton.bind</code> function:</p> <pre><code>import numpy as np\nfrom pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\n\nwith Triton() as triton:\n  triton.bind(\n      model_name=\"Linear\",\n      infer_func=_infer_function_factory(devices=[\"cuda\", \"cpu\"]),\n      inputs=[\n          Tensor(dtype=np.float32, shape=(-1,)),\n      ],\n      outputs=[\n          Tensor(dtype=np.float32, shape=(-1,)),\n      ],\n      config=ModelConfig(max_batch_size=16),\n  )\n  ...\n</code></pre> <p>Once the multiple callable objects are passed to <code>infer_func</code>, the Triton server gets information that multiple instances of the same model have been created. The incoming requests are distributed among created instances. In our case executing two instances of a <code>Linear</code> model loaded on CPU and GPU devices.</p>"},{"location":"binding_models/#defining-inputs-and-outputs","title":"Defining Inputs and Outputs","text":"<p>The integration of the Python model requires the inputs and outputs types of the model. This is required to correctly map the input and output data passed through the Triton Inference Server.</p> <p>The simplest definition of model inputs and outputs expects providing the type of data and the shape per input:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\n\ninputs = [\n    Tensor(dtype=np.float32, shape=(-1,)),\n]\noutput = [\n    Tensor(dtype=np.float32, shape=(-1,)),\n    Tensor(dtype=np.int32, shape=(-1,)),\n]\n</code></pre> <p>The provided configuration creates the following tensors:</p> <ul> <li>Single input:</li> <li>name: INPUT_1, data type: FLOAT32, shape=(-1,)</li> <li>Two outputs:</li> <li>name: OUTPUT_1, data type: FLOAT32, shape=(-1,)</li> <li>name: OUTPUT_2, data type: INT32, shape=(-1,)</li> </ul> <p>The <code>-1</code> means a dynamic shape of the input or output.</p> <p>To define the name of the input and its exact shape, the following definition can be used:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\n\ninputs = [\n    Tensor(name=\"image\", dtype=np.float32, shape=(224, 224, 3)),\n]\noutputs = [\n    Tensor(name=\"class\", dtype=np.int32, shape=(1000,)),\n]\n</code></pre> <p>This definition describes that the model has:</p> <ul> <li>a single input named <code>image</code> of size 224x224x3 and 32-bit floating-point data type</li> <li>a single output named <code>class</code> of size 1000 and 32-bit integer data type.</li> </ul> <p>The <code>dtype</code> parameter can be either <code>numpy.dtype</code>, <code>numpy.dtype.type</code>, or <code>str</code>. For example:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\n\ntensor1 = Tensor(name=\"tensor1\", shape=(-1,), dtype=np.float32),\ntensor2 = Tensor(name=\"tensor2\", shape=(-1,), dtype=np.float32().dtype),\ntensor3 = Tensor(name=\"tensor3\", shape=(-1,), dtype=\"float32\"),\n</code></pre> <p>dtype for bytes and string inputs/outputs</p> <p>When using the <code>bytes</code> dtype, NumPy removes trailing <code>\\x00</code> bytes. Therefore, for arbitrary bytes, it is required to use <code>object</code> dtype.</p> <pre><code>&gt; np.array([b\"\\xff\\x00\"])\narray([b'\\xff'], dtype='|S2')\n\n&gt; np.array([b\"\\xff\\x00\"], dtype=object)\narray([b'\\xff\\x00'], dtype=object)\n</code></pre> <p>For ease of use, for encoded string values, users might use <code>bytes</code> dtype.</p>"},{"location":"binding_models/#throwing-unrecoverable-errors","title":"Throwing Unrecoverable errors","text":"<p>When the model gets into a state where further inference is impossible, you can throw PyTritonUnrecoverableError from the inference callable. This will cause NVIDIA Triton Inference Server to shut down. This might be useful when the model is deployed on a cluster in a multi-node setup. In that case, to recover the model, you need to restart all \"workers\" on the cluster.</p> <pre><code>from typing import Dict\nimport numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.exceptions import PyTritonUnrecoverableError\n\n\n@batch\ndef infer_fn(**inputs: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    ...\n\n    try:\n        outputs = model(**inputs)\n    except Exception as e:\n        raise PyTritonUnrecoverableError(\n            \"Some unrecoverable error occurred, \"\n            \"thus no further inferences are possible.\"\n        ) from e\n\n    ...\n    return outputs\n</code></pre>"},{"location":"clients/","title":"Model Clients","text":""},{"location":"clients/#triton-clients","title":"Triton clients","text":"<p>PyTriton client is a user-friendly tool designed to communicate with the Triton Inference Server effortlessly. It manages the technical details for you, allowing you to concentrate on your data and the outcomes you aim to achieve. Here's how it assists:</p> <ol> <li> <p>Fetching Model Configuration: The client retrieves details about the model from the server, such as the shape and names of the input and output data tensors. This step is crucial for preparing your data correctly and for interpreting the response. This functionality is encapsulated in the ModelClient class.</p> </li> <li> <p>Sending Requests: Utilizing the model information, the client generates an inference request by mapping arguments you pass to the infer_sample or infer_batch methods to model inputs. It sends your data to the Triton server, requesting the model to perform inference. Arguments can be passed as positional or keyword arguments (mixing them is not allowed), and the client handles the rest.</p> </li> <li> <p>Returning Responses: It then delivers the model's response back to you. It decodes inputs as numpy arrays and maps model outputs to dictionary elements returned to you from the <code>infer_sample</code> or <code>infer_batch</code> methods. It also removes the batch dimension if it was added by the client.</p> </li> </ol> <p>This process might introduce a bit of delay due to the extra step of fetching model configuration. However, you can minimize this by reusing the PyTriton client for multiple requests or by setting it up with pre-loaded model configuration if you have it.</p> <p>PyTriton includes five specialized high-level clients to cater to different needs:</p> <ul> <li>ModelClient: A straightforward, synchronous client for simple request-response operations.</li> <li>FuturesModelClient: A multithreaded client that handles multiple requests in parallel, speeding up operations.</li> <li>DecoupledModelClient: A synchronous client designed for decoupled models, which allow for flexible interaction patterns with the Triton server.</li> <li>AsyncioModelClient: An asynchronous client that works well with Python's asyncio for efficient concurrent operations.</li> <li>AsyncioDecoupledModelClient: An asyncio-compatible client specifically for working with decoupled models asynchronously.</li> </ul> <p>PyTriton clients used tritonclient package from Triton. It is a Python client library for Triton Inference Server. It provides low level API for communicating with the server using HTTP or gRPC protocol. PyTriton clients are built on top of tritonclient and provide high level API for communicating with the server. Not all features of tritonclient are available in PyTriton clients. If you need more control over the communication with the server, you can use tritonclient directly.</p>"},{"location":"clients/#modelclient","title":"ModelClient","text":"<p>ModelClient is a simple client that can perform inference requests synchronously. You can use ModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the ModelClient object.</p> <p>You need <code>Linear</code> model described in quick_start. You should run it so client can connect to it.</p> <p>This example requires torch module:</p> <pre><code>pip install torch\n</code></pre> <p>For example, you can use ModelClient to send requests to a PyTorch model that performs linear regression:</p> <pre><code>import torch\nfrom pytriton.client import ModelClient\n\n# Create some input data as a numpy array\ninput1_data = torch.randn(128, 2).cpu().detach().numpy()\n\n# Create a ModelClient object with the server address and model name\nclient = ModelClient(\"localhost:8000\", \"Linear\")\n# Call the infer_batch method with the input data\nresult_dict = client.infer_batch(input1_data)\n# Close the client to release the resources\nclient.close()\n\n# Print the result dictionary\nprint(result_dict)\n</code></pre> <p>URL <code>localhost:8000</code> is the default address for Triton server HTTP protocol. If you have a different address, you should replace it with the correct one. You can also use the gRPC protocol by putting <code>grpc</code> in address string:</p> <pre><code>client = ModelClient(\"grpc://localhost\", \"Linear\")\n</code></pre> <p>You can omit port number if it is default for HTTP or gRPC protocol. Default port for HTTP is <code>8000</code> and for gRPC is <code>8001</code>.</p> <p>You can also use ModelClient to send requests to a model that performs image classification. The example assumes that a model takes in an image and returns the top 5 predicted classes. This model is not included in the PyTriton library.</p> <p>You need to convert the image to a numpy array and resize it to the expected input shape. You can use Pillow package to do this.</p> <p>You need to install Pillow package to run the example:</p> <pre><code>pip install Pillow\n</code></pre> <pre><code>import numpy as np\nfrom PIL import Image\nfrom pytriton.client import ModelClient\n\n# Create some input data as a numpy array of an image\nimg = Image.open(\"cat.jpg\")\nimg = img.resize((224, 224))\ninput_data = np.array(img)\n\n# Create a ModelClient object with the server address and model name\nclient = ModelClient(\"localhost:8000\", \"ImageNet\")\n# Call the infer_sample method with the input data\nresult_dict = client.infer_sample(input_data)\n# Close the client to release the resources\nclient.close()\n\n# Print the result dictionary\nprint(result_dict)\n</code></pre>"},{"location":"clients/#futuresmodelclient","title":"FuturesModelClient","text":"<p>FuturesModelClient is a concurrent.futures based client that can perform inference requests in a parallel way. You can use FuturesModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the FuturesModelClient object.</p> <p>For example, you can use FuturesModelClient to send multiple requests to a text generation model that takes in text prompts and returns generated texts. The TextGen model is not included in the PyTriton library. The example assumes that the model returns a single output tensor with the generated text. The example also assumes that the model takes in a list of text prompts and returns a list of generated texts.</p> <p>You need to convert the text prompts to numpy arrays of bytes using a tokenizer from transformers. You also need to detokenize the output texts using the same tokenizer:</p> <p>You need to install torch and transformers package to run the example:</p> <pre><code>pip install torch transformers\n</code></pre> <pre><code>import numpy as np\nfrom pytriton.client import FuturesModelClient\nfrom transformers import AutoTokenizer\n\n# Create some input data as a list of text prompts\ninput_data_list_text = [\"Write a haiku about winter.\", \"Summarize the article below in one sentence.\", \"Generate a catchy slogan for PyTriton.\"]\n\n# Create a tokenizer from transformers\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Convert the text prompts to numpy arrays of bytes using the tokenizer\ninput_data_list = [np.array(tokenizer.encode(prompt)) for prompt in input_data_list_text]\n\n# Create a FuturesModelClient object with the server address and model name\nwith FuturesModelClient(\"localhost:8000\", \"TextGen\") as client:\n    # Call the infer_sample method for each input data in the list and store the returned futures\n    output_data_futures = [client.infer_sample(input_data) for input_data in input_data_list]\n    # Wait for all the futures to complete and get the results\n    output_data_list = [output_data_future.result() for output_data_future in output_data_futures]\n\n# Print tokens\nprint(output_data_list)\n\n# Detokenize the output texts using the tokenizer and print them\noutput_texts = [tokenizer.decode(output_data[\"OUTPUT_1\"]) for output_data in output_data_list]\nfor output_text in output_texts:\n    print(output_text)\n</code></pre> <p>You can also use FuturesModelClient to send multiple requests to an image classification model that takes in image data and returns class labels or probabilities. The ImageNet model is described above.</p> <p>In this case, you can use the infer_batch method to send a batch of images as input and get a batch of outputs. You need to stack the images along the first dimension to form a batch. You can also print the class names corresponding to the output labels:</p> <pre><code>import numpy as np\nfrom PIL import Image\nfrom pytriton.client import FuturesModelClient\n\n# Create some input data as a list of lists of image arrays\ninput_data_list = []\nfor batch in [[\"cat.jpg\", \"dog.jpg\", \"bird.jpg\"], [\"car.jpg\", \"bike.jpg\", \"bus.jpg\"], [\"apple.jpg\", \"banana.jpg\", \"orange.jpg\"]]:\n  batch_data = []\n  for filename in batch:\n    img = Image.open(filename)\n    img = img.resize((224, 224))\n    img = np.array(img)\n    batch_data.append(img)\n  # Stack the images along the first dimension to form a batch\n  batch_data = np.stack(batch_data, axis=0)\n  input_data_list.append(batch_data)\n\n# Create a list of class names for ImageNet\nclass_names = [\"tench\", \"goldfish\", \"great white shark\", ...]\n\n# Create a FuturesModelClient object with the server address and model name\nwith FuturesModelClient(\"localhost:8000\", \"ImageNet\") as client:\n    # Call the infer_batch method for each input data in the list and store the returned futures\n    output_data_futures = [client.infer_batch(input_data) for input_data in input_data_list]\n    # Wait for all the futures to complete and get the results\n    output_data_list = [output_data_future.result() for output_data_future in output_data_futures]\n\n# Print the list of result dictionaries\nprint(output_data_list)\n\n# Print the class names corresponding to the output labels for each batch\nfor output_data in output_data_list:\n  output_labels = output_data[\"OUTPUT_1\"]\n  for output_label in output_labels:\n    class_name = class_names[output_label]\n    print(f\"The image is classified as {class_name}.\")\n</code></pre>"},{"location":"clients/#asynciomodelclient","title":"AsyncioModelClient","text":"<p>AsyncioModelClient is an asynchronous client that can perform inference requests using the asyncio library. You can use AsyncioModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the AsyncioModelClient object.</p> <p>For example, you can use AsyncioModelClient to send requests to a PyTorch model that performs linear regression:</p> <pre><code>import torch\nfrom pytriton.client import AsyncioModelClient\n\n# Make the code async by adding async before the function definition\nasync def main():\n    # Create some input data as a numpy array\n    input1_data = torch.randn(2).cpu().detach().numpy()\n\n    # Create an AsyncioModelClient object with the server address and model name\n    client = AsyncioModelClient(\"localhost:8000\", \"Linear\")\n    # Call the infer_sample method with the input data\n    result_dict = await client.infer_sample(input1_data)\n    # Close the client to release the resources\n    await client.close()\n\n    # Print the result dictionary\n    print(result_dict)\n\n# Run the code as a coroutine using asyncio.run()\nimport asyncio\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n</code></pre> <p>You can also use FastAPI to create a web application that exposes the results of inference at an HTTP endpoint. FastAPI is a modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.</p> <p>To use FastAPI, you need to install it with:</p> <pre><code>pip install fastapi\n</code></pre> <p>You also need an ASGI server, for production such as Uvicorn or Hypercorn.</p> <p>To install Uvicorn, run:</p> <pre><code>pip install uvicorn[standard]\n</code></pre> <p>The <code>uvicorn</code> uses port <code>8000</code> as default for web server. Triton server default port is also <code>8000</code> for HTTP protocol. You can change uvicorn port by using <code>--port</code> option. PyTriton also supports custom ports configuration for Triton server. The class <code>TritonConfig</code> contains parameters for ports configuration. You can pass it to <code>Triton</code> during initialization:</p> <pre><code>config = TritonConfig(http_port=8015)\ntriton_server = Triton(config=config)\n</code></pre> <p>You can use this <code>triton_server</code> object to bind your inference model and run HTTP endpoint from Triton Inference Server at port <code>8015</code>.</p> <p>Then you can create a FastAPI app that uses the AsyncioModelClient to perform inference and return the results as JSON:</p> <pre><code>from fastapi import FastAPI\nimport torch\nfrom pytriton.client import AsyncioModelClient\n\n# Create an AsyncioModelClient object with the server address and model name\nconfig_client = None\n\napp = FastAPI()\n\n@app.get(\"/predict\")\nasync def predict():\n    # Create some input data as a numpy array\n    input1_data = torch.randn(2).cpu().detach().numpy()\n\n    # Create an AsyncioModelClient object with the server address and model name and store it in a global variable\n    global config_client\n    if not config_client:\n        config_client = AsyncioModelClient(\"localhost:8000\", \"Linear\")\n        await config_client.model_config\n\n    # Create an AsyncioModelClient object from existing client to avoid pulling config from server\n    async with AsyncioModelClient.from_existing_client(config_client) as request_client:\n        # Call the infer_sample method with the input data\n        result_dict = await request_client.infer_sample(input1_data)\n\n    output_dict = {key: value.tolist() for key, value in result_dict.items()}\n\n    # Return the result dictionary as JSON\n    return output_dict\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown():\n    # Close the client to release the resources\n    await config_client.close()\n</code></pre> <p>Save this file as <code>main.py</code>.</p> <p>To run the app, use the command:</p> <pre><code>uvicorn main:app --reload --port 8015\n</code></pre> <p>You can then access the endpoint at <code>http://127.0.0.1:8015/predict</code> and see the JSON response.</p> <p>You can also check the interactive API documentation at <code>http://127.0.0.1:8015/docs</code>.</p> <p>You can test your server using curl:</p> <pre><code>curl -X 'GET' \\\n  'http://127.0.0.1:8015/predict' \\\n  -H 'accept: application/json'\n</code></pre> <p>Command will print three random numbers:</p> <pre><code>[-0.2608422636985779,-0.6435106992721558,-0.3492531180381775]\n</code></pre> <p>For more information about FastAPI and Uvicorn, check out these links:</p> <ul> <li>FastAPI documentation</li> <li>Uvicorn documentation</li> </ul>"},{"location":"clients/#decoupled-models-and-clients","title":"Decoupled models and clients","text":"<p>You can use the PyTriton library to create a decoupled model and client. A decoupled model is a model that is decoupled from batching and other features of the Triton Inference Server. It can receive many requests in parallel and perform inference on each request independently. A decoupled model can send multiple responses to single requests. A decoupled client is a client that can receive multiple responses from a single request. See document about decoupled models for more information.</p> <p>ModelClient, AsyncioModelClient, and FuturesModelClient refuse to send requests to decoupled models and raise an exception. You can use DecoupledModelClient and AsyncioDecoupledModelClient to send requests to decoupled models. You can only communicate over gRPC protocol with decoupled models.</p> <p>The Generate Extension is provisional and likely to change in future versions</p> <p>You can use generate stream endpoint in Triton Inference Server to send requests to decoupled models. See Generate Extension.</p> <p>For example, you can use DecoupledModelClient to send requests to GPT2 using stream endpoint:</p> <pre><code>import torch\nfrom pytriton.client import DecoupledModelClient\n\nclient = DecoupledModelClient(\"grpc://localhost\", \"streaming_bot\")\niterator = client.infer_sample(np.array([\"AI answers to\".encode('utf-8')]))\nresult = list(iterator)\nprint(result)\nclient.close()\n</code></pre> <p>Asynchronous version of DecoupledModelClient is AsyncioDecoupledModelClient. You can use it to send requests to decoupled models using asyncio library:</p> <pre><code>from pytriton.client import AsyncioDecoupledModelClient\n# Make the code async by adding async before the function definition\nasync def main():\n    client = AsyncioDecoupledModelClient(\"grpc://localhost\", \"streaming_bot\")\n    async for answer in client.infer_sample(np.array([\"AI answers to\".encode('utf-8')])):\n        print(answer)\n    await client.close()\n\n# Run the code as a coroutine using asyncio.run()\nimport asyncio\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n</code></pre> <p>This will print the following output: <pre><code>{'response': b'AI answers '},\n{'response': b'AI answers to '},\n{'response': b'AI answers to the '},\n{'response': b'AI answers to the '},\n{'response': b'AI answers to the question, '},\n{'response': b'AI answers to the question, '},\n{'response': b'AI answers to the question, \"What '},\n{'response': b'AI answers to the question, \"What is '},\n{'response': b'AI answers to the question, \"What is the '},\n{'response': b'AI answers to the question, \"What is the best '},\n{'response': b'AI answers to the question, \"What is the best way '},\n{'response': b'AI answers to the question, \"What is the best way to '},\n{'response': b'AI answers to the question, \"What is the best way to get '},\n{'response': b'AI answers to the question, \"What is the best way to get rid '},\n{'response': b'AI answers to the question, \"What is the best way to get rid of '},\n{'response': b'AI answers to the question, \"What is the best way to get rid of the '},\n{'response': b'AI answers to the question, \"What is the best way to get rid of the '},\n{'response': b'AI answers to the question, \"What is the best way to get rid of the problem?\"\\n'},\n{'response': b'AI answers to the question, \"What is the best way to get rid of the problem?\"\\n'}\n</code></pre></p> <p>Each response contains more tokens than the previous one. The last response contains the full generated text.</p>"},{"location":"clients/#client-timeouts","title":"Client timeouts","text":"<p>When creating a ModelClient or FuturesModelClient object, you can specify the timeout for waiting until the server and model are ready using the <code>init_timeout_s</code> parameter. By default, the timeout is set to 5 minutes (300 seconds).</p> <p>Example usage:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\n\ninput1_data = np.random.randn(128, 2)\nclient = ModelClient(\"localhost\", \"MyModel\", init_timeout_s=120)\n# Raises PyTritonClientTimeoutError if the server or model is not ready within the specified timeout\nresult_dict = client.infer_batch(input1_data)\nclient.close()\n\n\nwith FuturesModelClient(\"localhost\", \"MyModel\", init_timeout_s=120) as client:\n    future = client.infer_batch(input1_data)\n    #...\n    # It will raise `PyTritonClientTimeoutError` if the server is not ready and the model is not loaded within 120 seconds\n    # from the time `infer_batch` was called by a thread from `ThreadPoolExecutor`\n    result_dict = future.result()\n</code></pre> <p>You can disable the default behavior of waiting for the server and model to be ready during first inference request by setting <code>lazy_init</code> to <code>False</code>:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\n\ninput1_data = np.random.randn(128, 2)\n\n# will raise PyTritonClientTimeoutError if server is not ready and model loaded\n# within 120 seconds during intialization of client\nwith ModelClient(\"localhost\", \"MyModel\", init_timeout_s=120, lazy_init=False) as client:\n    result_dict = client.infer_batch(input1_data)\n</code></pre> <p>You can specify the timeout for the client to wait for the inference response from the server. The default timeout is 60 seconds. You can specify the timeout when creating the ModelClient or FuturesModelClient object:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\n\ninput1_data = np.random.randn(128, 2)\nclient = ModelClient(\"localhost\", \"MyModel\", inference_timeout_s=240)\n# Raises `PyTritonClientTimeoutError` if the server does not respond to inference request within 240 seconds\nresult_dict = client.infer_batch(input1_data)\nclient.close()\n\n\nwith FuturesModelClient(\"localhost\", \"MyModel\", inference_timeout_s=240) as client:\n    future = client.infer_batch(input1_data)\n    ...\n    # Raises `PyTritonClientTimeoutError` if the server does not respond within 240 seconds\n    # from the time `infer_batch` was called by a thread from `ThreadPoolExecutor`\n    result_dict = future.result()\n</code></pre> <p>gRPC client timeout not fully supported</p> <p>There are some missing features in the gRPC client that prevent it from working correctly with timeouts used during the wait for the server and model to be ready. This may cause the client to hang if the server doesn't respond with the current server or model state.</p> <p>Server side timeout not implemented</p> <p>Currently, there is no support for server-side timeout. The server will continue to process the request even if the client timeout is reached.</p>"},{"location":"examples/","title":"Tutorials","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>We provide simple examples on how to integrate PyTorch, TensorFlow2, JAX, and simple Python models with the Triton Inference Server using PyTriton. The examples are available in the GitHub repository.</p>"},{"location":"examples/#samples-models-deployment","title":"Samples Models Deployment","text":"<p>The list of example models deployments:</p> <ul> <li>Add-Sub Python model</li> <li>Add-Sub Python model Jupyter Notebook</li> <li>BART PyTorch from HuggingFace</li> <li>BERT JAX from HuggingFace</li> <li>Identity Python model</li> <li>Linear RAPIDS/CuPy model</li> <li>Linear RAPIDS/CuPy model Jupyter Notebook</li> <li>Linear PyTorch model</li> <li>Multi-Layer TensorFlow2</li> <li>Multi Instance deployment for ResNet50 PyTorch model</li> <li>Multi Model deployment for Python models</li> <li>NeMo Megatron GPT model with multi-node support</li> <li>OPT JAX from HuggingFace with multi-node support</li> <li>ResNet50 PyTorch from HuggingFace</li> <li>Stable Diffusion 1.5 from HuggingFace</li> <li>Using custom HTTP/gRPC headers and parameters</li> </ul>"},{"location":"examples/#profiling-models","title":"Profiling models","text":"<p>The Perf Analyzer can be used to profile the models served through PyTriton. We have prepared an example of using Perf Analyzer to profile BART PyTorch. See the example code in the GitHub repository.</p>"},{"location":"examples/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>The following examples contain a guide on how to deploy them on a Kubernetes cluster:</p> <ul> <li>BART PyTorch from HuggingFace</li> <li>OPT JAX from HuggingFace with multi-node support</li> <li>NeMo Megatron GPT model with multi-node support</li> <li>ResNet50 PyTorch from HuggingFace</li> <li>Stable Diffusion 1.5 from HuggingFace</li> </ul>"},{"location":"high_level_design/","title":"High-Level Design","text":""},{"location":"high_level_design/#high-level-design","title":"High-Level Design","text":"<p>The diagram below presents the schema of how the Python models are served through Triton Inference Server using PyTriton. The solution consists of two main components:</p> <ul> <li>Triton Inference Server: for exposing the HTTP/gRPC API and benefiting from performance features like dynamic batching or response cache.</li> <li>Python Model Environment: your environment where the Python model is executed.</li> </ul> <p>The Triton Inference Server binaries are provided as part of the PyTriton installation. The Triton Server is installed in your current environment (system or container). The PyTriton controls the Triton Server process through the <code>Triton Controller</code>.</p> <p>Exposing the model through PyTriton requires the definition of an <code>Inference Callable</code> - a Python function that is connected to Triton Inference Server and executes the model or ensemble for predictions. The integration layer binds the <code>Inference Callable</code> to Triton Server and exposes it through the Triton HTTP/gRPC API under a provided <code>&lt;model name&gt;</code>. Once the integration is done, the defined <code>Inference Callable</code> receives data sent to the HTTP/gRPC API endpoint <code>v2/models/&lt;model name&gt;/infer</code>. Read more about HTTP/gRPC interface in Triton Inference Server documentation.</p> <p>The HTTP/gRPC requests sent to <code>v2/models/&lt;model name&gt;/infer</code> are handled by Triton Inference Server. The server batches requests and passes them to the <code>Proxy Backend</code>, which sends the batched requests to the appropriate <code>Inference Callable</code>. The data is sent as a <code>numpy</code> array. Once the <code>Inference Callable</code> finishes execution of the model prediction, the result is returned to the <code>Proxy Backend</code>, and a response is created by Triton Server.</p> <p></p>"},{"location":"initialization/","title":"Triton Initialization","text":""},{"location":"initialization/#initialization","title":"Initialization","text":"<p>The following page provides more details about possible options for configuring the Triton Inference Server and working with block and non-blocking mode for tests and deployment.</p>"},{"location":"initialization/#configuring-triton","title":"Configuring Triton","text":"<p>Connecting Python models with Triton Inference Server working in the current environment requires creating a Triton object. This can be done by creating a context:</p> <pre><code>from pytriton.triton import Triton\n\nwith Triton() as triton:\n    ...\n</code></pre> <p>or simply creating an object:</p> <pre><code>from pytriton.triton import Triton\n\ntriton = Triton()\n</code></pre> <p>The Triton Inference Server behavior can be configured by passing config parameter:</p> <pre><code>import pathlib\nfrom pytriton.triton import Triton, TritonConfig\n\ntriton_config = TritonConfig(log_file=pathlib.Path(\"/tmp/triton.log\"))\nwith Triton(config=triton_config) as triton:\n    ...\n</code></pre> <p>and through environment variables, for example, set as in the command below:</p> <pre><code>PYTRITON_TRITON_CONFIG_LOG_VERBOSITY=4 python my_script.py\n</code></pre> <p>The order of precedence of configuration methods is:</p> <ul> <li>config defined through <code>config</code> parameter of Triton class <code>__init__</code> method</li> <li>config defined in environment variables</li> <li>default TritonConfig values</li> </ul>"},{"location":"initialization/#triton-lifecycle-policy","title":"Triton Lifecycle Policy","text":"<p>Triton class has additional <code>lifecycle_policy</code> parameter of type <code>TritonLifecyclePolicy</code>. It indicates how the Triton Server should be managed. It stores two flags: <code>TritonLifecyclePolicy.launch_triton_on_startup</code> and <code>TritonLifecyclePolicy.local_model_store</code>. First one indicates if the Triton Server should be started on Triton object creation and the second one indicates if the model store should be created in the local filesystem and polled by Triton or configuration should be passed to the Triton server and managed by it.</p> <pre><code>from pytriton.triton import Triton, TritonLifecyclePolicy\n\nlifecycle_policy = TritonLifecyclePolicy(launch_triton_on_startup=False, local_model_store=True)\nwith Triton(triton_lifecycle_policy=lifecycle_policy) as triton:\n    ...\n    triton.serve()\n</code></pre> <p>Default values for <code>TritonLifecyclePolicy</code> flags are <code>launch_triton_on_startup=True</code> and <code>local_model_store=False</code>. In this case Triton Server will be started on the Triton class instantiation (so user can bind models to running server interactively) and the model store will be created in Triton server's filesystem and managed by it.</p> <p>In some usage scenarios, it is necessary to prepare the model first in local model store and then start the Triton server (e.g. VertexAI flow). In this case we use VerexAI's <code>TritonLifecyclePolicy</code> to manage the Triton server lifecycle.</p> <p><code>VertextAILifecyclePolicy = TritonLifecyclePolicy(launch_triton_on_startup=False, local_model_store=True)</code></p> <p>For easy of use, it is automatically set when <code>TritonConfig</code> is created with <code>allow_vertex_ai</code> parameter set to <code>True</code>.</p> <p><code>config = TritonConfig(allow_http=True, allow_vertex_ai=True, vertex_ai_port=8080)</code></p> <p>For details on how to use <code>TritonLifecyclePolicy</code> with VertexAI, see example examples/add_sub_vertex_ai.</p>"},{"location":"initialization/#blocking-mode","title":"Blocking mode","text":"<p>The blocking mode will stop the execution of the current thread and wait for incoming HTTP/gRPC requests for inference execution. This mode makes your application behave as a pure server. The example of using blocking mode:</p> <pre><code>from pytriton.triton import Triton\n\nwith Triton() as triton:\n    ...  # Load models here\n    triton.serve()\n</code></pre>"},{"location":"initialization/#background-mode","title":"Background mode","text":"<p>The background mode runs Triton as a subprocess and does not block the execution of the current thread. In this mode, you can run Triton Inference Server and interact with it from the current context. The example of using background mode:</p> <pre><code>from pytriton.triton import Triton\n\ntriton = Triton()\n...  # Load models here\ntriton.run()  # Triton Server started\nprint(\"This print will appear\")\ntriton.stop()  # Triton Server stopped\n</code></pre>"},{"location":"initialization/#filesystem-usage","title":"Filesystem usage","text":"<p>PyTriton needs to access the filesystem for two purposes:</p> <ul> <li>to communicate with the Triton backend using file sockets,</li> <li>storing copy of Triton backend and its binary dependencies.</li> </ul> <p>PyTriton creates temporary folders called Workspaces, where it stores the file descriptors for these operations. By default, these folders are located in <code>$HOME/.cache/pytriton</code> directory. However, you can change this location by setting the <code>PYTRITON_HOME</code> environment variable.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This page explains how to install the library. We assume that you have a basic understanding of the Python programming language and are familiar with machine learning models.</p> <p>You should be comfortable with the Python programming language and know how to work with Machine Learning models. Using Docker is optional and not necessary.</p> <p>The library can be installed in any of the following ways:</p> <ul> <li>system environment</li> <li>virtualenv</li> <li>Docker image</li> </ul> <p>If you opt for using Docker, you can get NVIDIA optimized Docker images for Python frameworks from the NVIDIA NGC Catalog.</p> <p>To run model inference on NVIDIA GPU using the Docker runtime, we recommend that you install the NVIDIA Container Toolkit, which enables GPU acceleration for containers.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the library, ensure that you meet the following requirements:</p> <ul> <li>An operating system with glibc &gt;= <code>2.35</code>. Triton Inference Server and PyTriton have only been rigorously tested on Ubuntu 22.04.   Other supported operating systems include Ubuntu Debian 11+, Rocky Linux 9+, and Red Hat Universal Base Image 9+.</li> <li>to check your glibc version, run <code>ldd --version</code></li> <li>Python version &gt;= <code>3.8</code></li> <li><code>pip &gt;=</code>20.3`</li> <li><code>libpython3.*.so</code> available in the operating system (appropriate for Python version).</li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from <code>pypi</code>","text":"<p>You can install the package from pypi.org by running the following command:</p> <pre><code>pip install -U nvidia-pytriton\n</code></pre> <p>Triton Inference Server binaries</p> <p>The Triton Inference Server binaries are installed as part of the PyTriton package.</p>"},{"location":"installation/#setting-up-python-environment","title":"Setting Up Python Environment","text":"<p>The Triton Inference Server is automatically run with your Python interpreter version. To use Triton binary you need to make sure that <code>libpython3.*.so</code> library can be linked during PyTriton start. Install and provide location to <code>libpython3.*.so</code> library in LD_LIBRARY_PATH before you will run PyTriton. Below we presented some options on how to prepare your Python environment to run PyTriton with common tools.</p>"},{"location":"installation/#upgrading-pip-version","title":"Upgrading <code>pip</code> version","text":"<p>You need to have <code>pip</code> version 20.3 or higher. To upgrade an older version of pip, run this command:</p> <pre><code>pip install -U pip\n</code></pre>"},{"location":"installation/#using-system-interpreter","title":"Using system interpreter","text":"<p>When you are running PyTriton on Ubuntu 22.04 install the desired Python interpreter and <code>libpython3*so.</code> library. <pre><code># Install necessary packages\napt update -y\napt install -y software-properties-common\n\n# Add repository with various Python versions\nadd-apt-repository ppa:deadsnakes/ppa -y\n\n# Install Python 3.8\napt install -y python3.8 libpython3.8 python3.8-distutils python3-pip \\\n     build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev \\\n     libffi-dev curl libbz2-dev pkg-config make\n\n# Install library for interpreter\npython3.8 -m pip install nvidia-pytriton\n</code></pre></p>"},{"location":"installation/#creating-virtualenv-using-pyenv","title":"Creating virtualenv using <code>pyenv</code>","text":"<p>In order to install different version replace the <code>3.8</code> with desired Python version in the example below:</p> <pre><code># Install necessary packages\napt update -y\napt install -y python3 python3-distutils python-is-python3 git \\\n    build-essential libssl-dev zlib1g-dev \\\n    libbz2-dev libreadline-dev libsqlite3-dev curl \\\n    libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n\n# Install pyenv\ncurl https://pyenv.run | bash\n\n# Configure pyenv in current environment\nexport PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\neval \"$(pyenv virtualenv-init -)\"\n\n# Install Python 3.8 with shared library support\nenv PYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install 3.8\n\n# Create and activate virtualenv\npyenv virtualenv 3.8 venv\npyenv activate venv\n\n# export the library path\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pyenv virtualenv-prefix)/lib\n\n# Install library for interpreter\npip install nvidia-pytriton\n</code></pre>"},{"location":"installation/#creating-virtualenv-using-venv","title":"Creating virtualenv using <code>venv</code>","text":"<p>In order to install different version replace the <code>3.8</code> with desired Python version in the example below:</p> <pre><code># Install necessary packages\napt update -y\napt install -y software-properties-common\n\n# Add repository with various Python versions\nadd-apt-repository ppa:deadsnakes/ppa -y\n\n# Install Python 3.8\napt install -y python3.8 libpython3.8 python3.8-distutils python3.8-venv python3.8-pip python-is-python3 \\\n     build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev \\\n     libffi-dev curl libbz2-dev pkg-config make\n\n# Create and activate virtualenv\npython3.8 -m venv /opt/venv\nsource /opt/venv/bin/activate\n\n# Install library for interpreter\npip install nvidia-pytriton\n</code></pre>"},{"location":"installation/#creating-virtualenv-using-miniconda","title":"Creating virtualenv using <code>miniconda</code>","text":"<p>In order to install different version replace the <code>3.8</code> with desired Python version in the example below:</p> <pre><code># Install necessary packages\napt update -y\napt install -y python3 python3-distutils python-is-python3 curl\n\n# Download miniconda\nCONDA_VERSION=latest\nTARGET_MACHINE=x86_64\ncurl \"https://repo.anaconda.com/miniconda/Miniconda3-${CONDA_VERSION}-Linux-${TARGET_MACHINE}.sh\" --output miniconda.sh\n\n# Install miniconda and add to PATH\nbash miniconda.sh\nexport PATH=~/miniconda3/bin/:$PATH\n\n# Initialize bash\nconda init bash\nbash\n\n# Create and activate virtualenv\nconda create -c conda-forge -n venv python=3.8\nconda activate venv\n\n# Export the library path\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib\n\n# Install library for interpreter\npip install nvidia-pytriton\n</code></pre>"},{"location":"installation/#building-binaries-from-source","title":"Building binaries from source","text":"<p>The binary package can be built from the source, allowing access to unreleased hotfixes, the ability to modify the PyTriton code, and compatibility with various Triton Inference Server versions, including custom server builds. For further information on building the PyTriton binary, refer to the Building page.</p>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues-and-limitations","title":"Known Issues and Limitations","text":"<ul> <li>There is no one-to-one match between our solution and Triton Inference Server features, especially in terms of supporting a user model store.</li> <li>Running multiple scripts hosting PyTriton on the same machine or container is not feasible.</li> <li>Deadlocks may occur in some models when employing the NCCL communication library and multiple Inference Callables are triggered concurrently. This issue can be observed when deploying multiple instances of the same model or multiple models within a single server script. Additional information about this issue can be found here.</li> <li>Enabling verbose logging may cause a significant performance drop in model inference.</li> <li>GRPC ModelClient doesn't support timeouts for model configuration and model metadata requests due to a limitation in the underlying tritonclient library.</li> <li>HTTP ModelClient may not respect the specified timeouts for model initialization and inference requests, especially when they are smaller than 1 second, resulting in longer waiting times. This issue is related to the underlying implementation of HTTP protocol.</li> <li>L0_remote_life_cycle, L0_tritons_cohabitation tests fails with timeouts due to unknown reasons.</li> <li>Triton logs contain false nevative error <code>Failed to set config modification time: model_config_content_name_ is empty</code>. It can be ignored.</li> <li><code>L0_example_huggingface_bert_jax</code> and <code>L0_example_huggingface_opt_multinode_jax</code> tests fail with missing kubernetes features in JAX.</li> </ul>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>The prerequisite for this page is to install PyTriton, which can be found in the installation page.</p> <p>The Quick Start presents how to run a Python model in the Triton Inference Server without needing to change the current working environment. In this example, we are using a simple <code>Linear</code> PyTorch model.</p> <p>The integration of the model requires providing the following elements:</p> <ul> <li>The model - a framework or Python model or function that handles inference requests</li> <li>Inference Callable - function or class with <code>__call__</code> method, that handles the input data coming from Triton and returns the result</li> <li>Python function connection with Triton Inference Server - a binding for communication between Triton and the Inference Callable</li> </ul> <p>The requirement for the example is to have PyTorch installed in your environment. You can do this by running:</p> <pre><code>pip install torch\n</code></pre> <p>In the next step, define the <code>Linear</code> model:</p> <pre><code>import torch\n\nmodel = torch.nn.Linear(2, 3).to(\"cuda\").eval()\n</code></pre> <p>In the second step, create an inference callable as a function. The function obtains the HTTP/gRPC request data as an argument, which should be in the form of a NumPy array. The expected return object should also be a NumPy array. You can define an inference callable as a function that uses the <code>@batch</code> decorator from PyTriton. This decorator converts the input request into a more suitable format that can be directly passed to the model. You can read more about decorators here.</p> <p>Example implementation:</p> <pre><code>import numpy as np\nimport torch\n\nfrom pytriton.decorators import batch\n\n\n@batch\ndef infer_fn(**inputs: np.ndarray):\n    (input1_batch,) = inputs.values()\n    input1_batch_tensor = torch.from_numpy(input1_batch).to(\"cuda\")\n    output1_batch_tensor = model(input1_batch_tensor) # Calling the Python model inference\n    output1_batch = output1_batch_tensor.cpu().detach().numpy()\n    return [output1_batch]\n</code></pre> <p>In the next step, you can create the binding between the inference callable and Triton Inference Server using the <code>bind</code> method from PyTriton. This method takes the model name, the inference callable, the inputs and outputs tensors, and an optional model configuration object.</p> <pre><code>from pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n\n# Connecting inference callable with Triton Inference Server\nwith Triton() as triton:\n    triton.bind(\n        model_name=\"Linear\",\n        infer_func=infer_fn,\n        inputs=[\n            Tensor(dtype=np.float32, shape=(-1,)),\n        ],\n        outputs=[\n            Tensor(dtype=np.float32, shape=(-1,)),\n        ],\n        config=ModelConfig(max_batch_size=128)\n    )\n    ...\n</code></pre> <p>Finally, serve the model with the Triton Inference Server:</p> <pre><code>from pytriton.triton import Triton\n\nwith Triton() as triton:\n    ...  # Load models here\n    triton.serve()\n</code></pre> <p>The <code>bind</code> method creates a connection between the Triton Inference Server and the <code>infer_fn</code>, which handles the inference queries. The <code>inputs</code> and <code>outputs</code> describe the model inputs and outputs that are exposed in Triton. The config field allows more parameters for model deployment.</p> <p>The <code>serve</code> method is blocking, and at this point, the application waits for incoming HTTP/gRPC requests. From that moment, the model is available under the name <code>Linear</code> in the Triton server. The inference queries can be sent to <code>localhost:8000/v2/models/Linear/infer</code>, which are passed to the <code>infer_fn</code> function.</p> <p>If you would like to use Triton in the background mode, use <code>run</code>. More about that can be found in the Deploying Models page.</p> <p>Once the <code>serve</code> or <code>run</code> method is called on the <code>Triton</code> object, the server status can be obtained using:</p> <pre><code>curl -v localhost:8000/v2/health/live\n</code></pre> <p>The model is loaded right after the server starts, and its status can be queried using:</p> <pre><code>curl -v localhost:8000/v2/models/Linear/ready\n</code></pre> <p>Finally, you can send an inference query to the model:</p> <pre><code>curl -X POST \\\n  -H \"Content-Type: application/json\"  \\\n  -d @input.json \\\n  localhost:8000/v2/models/Linear/infer\n</code></pre> <p>The <code>input.json</code> with sample query:</p> <pre><code>{\n  \"id\": \"0\",\n  \"inputs\": [\n    {\n      \"name\": \"INPUT_1\",\n      \"shape\": [1, 2],\n      \"datatype\": \"FP32\",\n      \"parameters\": {},\n      \"data\": [[-0.04281254857778549, 0.6738349795341492]]\n    }\n  ]\n}\n</code></pre> <p>Read more about the HTTP/gRPC interface in the Triton Inference Server documentation.</p> <p>You can also validate the deployed model using a simple client that can perform inference requests:</p> <pre><code>import torch\nfrom pytriton.client import ModelClient\n\ninput1_data = torch.randn(128, 2).cpu().detach().numpy()\n\nwith ModelClient(\"localhost:8000\", \"Linear\") as client:\n    result_dict = client.infer_batch(input1_data)\n\nprint(result_dict)\n</code></pre> <p>The full example code can be found in examples/linear_random_pytorch.</p> <p>More information about running the server and models can be found in Deploying Models page.</p>"},{"location":"remote_triton/","title":"Triton Remote Mode","text":""},{"location":"remote_triton/#remote-mode","title":"Remote Mode","text":"<p>Remote mode is a way to use the PyTriton with the Triton Inference Server running remotely (at this moment it must be deployed on the same machine, but may be launched in a different container).</p> <p>To bind the model in remote mode, it is required to use the <code>RemoteTriton</code> class instead of <code>Triton</code>. Only difference of using <code>RemoteTriton</code> is that it requires the triton <code>url</code> argument in the constructor.</p>"},{"location":"remote_triton/#example-of-binding-a-model-in-remote-mode","title":"Example of binding a model in remote mode","text":"<p>Example below assumes that the Triton Inference Server is running on the same machine (launched with PyTriton in separate python script).</p> <p><code>RemoteTriton</code> binds remote model to existing Triton Inference Server. When <code>RemoteTriton</code> is closed, the model is unloaded from the server.</p> <pre><code>import numpy as np\n\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import RemoteTriton, TritonConfig\n\ntriton_config = TritonConfig(\n    cache_config=[f\"local,size={1024 * 1024}\"],  # 1MB\n)\n\n@batch\ndef _add_sub(**inputs):\n    a_batch, b_batch = inputs.values()\n    add_batch = a_batch + b_batch\n    sub_batch = a_batch - b_batch\n    return {\"add\": add_batch, \"sub\": sub_batch}\n\nwith RemoteTriton(url='localhost') as triton:\n    triton.bind(\n        model_name=\"AddSub\",\n        infer_func=_add_sub,\n        inputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\n        outputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\n        config=ModelConfig(max_batch_size=8, response_cache=True)\n    )\n    triton.serve()\n</code></pre>"},{"location":"guides/basic_troubleshooting/","title":"Basic Troubleshooting","text":""},{"location":"guides/basic_troubleshooting/#using-pytriton-check-tool","title":"Using PyTriton Check Tool","text":"<p>The PyTriton Check Tool is a command-line utility included with PyTriton, designed to perform preliminary checks on the environment where PyTriton is deployed. It collects vital information about the operating system version, socket statistics, disk and memory usage and GPU functionality, ensuring the environment is properly set up for PyTriton.</p>"},{"location":"guides/basic_troubleshooting/#features","title":"Features","text":"<ul> <li>Operating System Verification: Collects and verifies the OS version to ensure compatibility.</li> <li>Python version and compiler: Gathers information about python version, libc used in python and installed compiler version.</li> <li>Socket Statistics: Gathers socket statistics for network-related checks.</li> <li>Disk/Memory/CPU Statistics: Gathers information about disk, memory, vpu usage</li> <li>GPU Functionality: Runs <code>nvidia-smi</code> to verify GPU functionality.</li> <li>Inference Testing: Executes simple inference examples on PyTriton to ensure proper communication with the Triton Inference Server.</li> </ul>"},{"location":"guides/basic_troubleshooting/#how-to-use","title":"How to Use","text":"<p>To launch the tool, use one of the following commands:</p> <pre><code>pytriton check -w /path/to/non_existing_workspace_dir\n\n# or\n\npython -m pytriton check -w /path/to/non_existing_workspace_dir\n</code></pre>"},{"location":"guides/basic_troubleshooting/#command-explanation","title":"Command Explanation","text":"<ul> <li><code>-w /path/to/non_existing_workspace_dir</code>: Specifies the workspace directory where logs and results will be collected.</li> </ul>"},{"location":"guides/basic_troubleshooting/#default-behavior","title":"Default Behavior","text":"<p>If the <code>-w</code> option is not provided, the tool will:</p> <ol> <li>Create a default workspace folder in the system's temporary directory.</li> <li>Collect logs and results in this default workspace.</li> <li>Compress the logs into a zip file.</li> <li>Copy the zip file to the current working directory.</li> </ol>"},{"location":"guides/basic_troubleshooting/#more-detailed-output","title":"More Detailed Output","text":"<p>Some environmental checks may gather more system information if the tool is launched with administrative privileges. For example, when listing processes that listen on a port, process names can only be obtained if the tool is launched with <code>sudo</code>. To do this just for the environment check, use the following command:</p> <pre><code>sudo -E &lt;venv&gt;/bin/python -m pytriton env -w ...\n\n# or if your virtual environment is activated\n\nsudo -E $(which python3) -m pytriton env -w ...\n</code></pre> <p>If it is more convenient for you, you can also run the entire check with sudo, generating a single log with all the required information:</p> <pre><code>sudo -E &lt;venv&gt;/bin/python -m pytriton check -w ...\n\n# or if your virtual environment is activated\n\nsudo -E $(which python3) -m pytriton check -w ...\n</code></pre>"},{"location":"guides/building/","title":"Building binary package","text":""},{"location":"guides/building/#building-binary-package-from-source","title":"Building binary package from source","text":"<p>This guide provides an outline of the process for building the PyTriton binary package from source. It offers the flexibility to modify the PyTriton code and integrate it with various versions of the Triton Inference Server, including custom builds. Additionally, it allows you to incorporate hotfixes that have not yet been officially released.</p>"},{"location":"guides/building/#prerequisites","title":"Prerequisites","text":"<p>Before building the PyTriton binary package, ensure the following:</p> <ul> <li>Docker with buildx plugin is installed on the system. For more information, refer to the Docker documentation.</li> <li>Access to the Docker daemon is available from the system or container.</li> </ul> <p>If you plan to build <code>arm64</code> wheel on <code>amd64</code> machine we suggest to use QUEMU for emulation. To enable QUEMU on Ubuntu you need to: - Install the QEMU packages on your x86 machine: <pre><code>sudo apt-get install qemu binfmt-support qemu-user-static\n</code></pre> - Register the QEMU emulators for ARM architectures: <pre><code>docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n</code></pre></p>"},{"location":"guides/building/#building-pytriton-binary-package","title":"Building PyTriton binary package","text":"<p>To build the wheel binary package, follow these steps from the root directory of the project:</p> <pre><code>make install-dev\nmake dist\n</code></pre> <p>Note: The default build create wheel for <code>x86_64</code> architecture. If you would like to build the wheel for <code>aarch64</code> use <pre><code>make dist -e PLATFORM=linux/arm64\n</code></pre> We use Docker convention name for platforms. The supported options are <code>linux/amd64</code> and <code>linux/arm64</code>.</p> <p>The wheel package will be located in the <code>dist</code> directory. To install the library, run the following <code>pip</code> command:</p> <pre><code>pip install dist/nvidia_pytriton-*-py3-none-*.whl\n</code></pre> <p>Note: The wheel name would have <code>x86_64</code> or <code>aarch64</code> in name based on selected platform.</p>"},{"location":"guides/building/#building-for-a-specific-triton-inference-server-version","title":"Building for a specific Triton Inference Server version","text":"<p>Building for an unsupported OS or hardware platform is possible. PyTriton requires a Python backend and either an HTTP or gRPC endpoint. The build can be CPU-only, as inference is performed on Inference Handlers.</p> <p>For more information on the Triton Inference Server build process, refer to the building section of Triton Inference Server documentation.</p> <p>Untested Build</p> <p>The Triton Inference Server has only been rigorously tested on Ubuntu 22.04. Other OS and hardware platforms are not officially supported. You can test the build by following the steps outlined in the Triton Inference Server testing guide.</p> <p>By the following docker method steps you can create a <code>tritonserver:latest</code> Docker image that can be used to build PyTriton with the following command:</p> <pre><code>make dist -e TRITONSERVER_IMAGE_VERSION=latest -e TRITONSERVER_IMAGE_NAME=tritonserver:latest\n</code></pre>"},{"location":"guides/chunking/","title":"Chunking and batching","text":""},{"location":"guides/chunking/#how-to-use-pytriton-client-to-split-a-large-input-into-smaller-batches-and-send-them-to-the-server-in-parallel","title":"How to use PyTriton client to split a large input into smaller batches and send them to the server in parallel","text":"<p>In this article, you will learn how to use PyTriton clients to create a chunking client that can handle inputs that are larger than the maximum batch size of your model.</p> <p>First, you need to create a model that can process a batch of inputs and produce a batch of outputs. For simplicity, let's assume that your model can only handle two inputs at a time. We will call this model \"Batch2\" and run it on a local Triton server.</p> <p>Next, you need to create a client that can send requests to your model. In this article, we will use the FuturesModelClient, which returns a Future object for each request. A Future object is a placeholder that can be used to get the result or check the status of the request later.</p> <p>However, there is a problem with using the FuturesModelClient directly. If you try to send an input that is larger than the maximum batch size of your model, you will get an error. For example, the following code tries to send an input of size 4 to the \"Batch2\" model, which has a maximum batch size of 2:</p> <pre><code>import numpy as np\nfrom pytriton.client import FuturesModelClient\n\nwith FuturesModelClient(f\"localhost\", \"Batch2\") as client:\n    input_tensor = np.zeros((4, 1), dtype=np.int32)\n    print(client.infer_batch(input_tensor).result())\n</code></pre> <p>This code will raise an exception like this:</p> <pre><code>PyTritonClientInferenceServerError: Error occurred during inference request. Message: [request id: 0] inference request batch-size must be &lt;= 2 for 'Batch2'\n</code></pre> <p>To solve this problem, we can use a ChunkingClient class that inherits from FuturesModelClient and overrides the infer_batch method. The ChunkingClient class takes a chunking strategy as an argument, which is a function that takes the input dictionary and the maximum batch size as parameters and yields smaller dictionaries of inputs. The default chunking strategy simply splits the input along the first dimension according to the maximum batch size. For example, if the input is <code>{\"INPUT_1\": np.zeros((5, 1), dtype=np.int32)}</code> and the maximum batch size is 2, then the default chunking strategy will yield:</p> <pre><code>{\"INPUT_1\": np.zeros((2, 1), dtype=np.int32)}\n{\"INPUT_1\": np.zeros((2, 1), dtype=np.int32)}\n{\"INPUT_1\": np.zeros((1, 1), dtype=np.int32)}\n</code></pre> <p>You can also define your own chunking strategy if you have more complex logic for splitting your input.</p> <p><pre><code># Define a ChunkingClient class that inherits from FuturesModelClient and splits the input into smaller batches\nimport concurrent.futures\nfrom pytriton.client import FuturesModelClient\n\nclass ChunkingClient(FuturesModelClient):\n    def __init__(self, host, model_name, chunking_strategy=None, max_workers=None):\n        super().__init__(host, model_name, max_workers=max_workers)\n        self.chunking_strategy = chunking_strategy or self.default_chunking_strategy\n\n    def default_chunking_strategy(self, kwargs, max_batch_size):\n        # Split the input by the first dimension according to the max batch size\n        size_of_dimention_0 = self.find_size_0(kwargs)\n        for i in range(0, size_of_dimention_0, max_batch_size):\n            yield {key: value[i:i+max_batch_size] for key, value in kwargs.items()}\n\n    def find_size_0(self, kwargs):\n        # Check the size of the first dimension of each tensor and raise errors if they are not consistent or valid\n        size_of_dimention_0 = None\n        for key, value in kwargs.items():\n            if isinstance(value, np.ndarray):\n                if value.ndim &gt; 0:\n                    size = value.shape[0]\n                    if size_of_dimention_0 is None or size_of_dimention_0 == size:\n                        size_of_dimention_0 = size\n                    else:\n                        raise ValueError(\"The tensors have different sizes at the first dimension\")\n                else:\n                    raise ValueError(\"The tensor has no first dimension\")\n            else:\n                raise TypeError(\"The value is not a numpy tensor\")\n        return size_of_dimention_0\n\n    def infer_batch(self, *args, **kwargs):\n        max_batch_size = self.model_config().result().max_batch_size\n        # Send the smaller batches to the server in parallel and yield the futures with results\n        futures = [super(ChunkingClient, self).infer_batch(*args, **chunk) for chunk in self.chunking_strategy(kwargs, max_batch_size)]\n        for future in futures:\n            yield future\n</code></pre> To use the ChunkingClient class, you can create an instance of it and use it in a context manager. For example:</p> <pre><code># Use the ChunkingClient class with the default strategy to send an input of size 5 to the \"Batch2\" model\nimport numpy as np\nfrom pytriton.client import FuturesModelClient\nchunker_client = ChunkingClient(\"localhost\", \"Batch2\")\nresults = []\nwith chunker_client as client:\n    input_tensor = np.zeros((5, 1), dtype=np.int32)\n    # Print the results of each future without concatenating them\n    for future in client.infer_batch(INPUT_1=input_tensor):\n        results.append(future.result())\nprint(results)\n</code></pre> <p>This code will print:</p> <pre><code>{'OUTPUT_1': array([[0],\n       [0]], dtype=int32)}\n{'OUTPUT_1': array([[0],\n       [0]], dtype=int32)}\n{'OUTPUT_1': array([[0]], dtype=int32)}\n</code></pre> <p>You can see that the input is split into three batches of sizes 2, 2, and 1, and each batch is sent to the server in parallel. The results are returned as futures that can be accessed individually without concatenating them.</p>"},{"location":"guides/deploying_in_clusters/","title":"Deploying in Clusters","text":""},{"location":"guides/deploying_in_clusters/#deploying-in-cluster","title":"Deploying in Cluster","text":"<p>The library can be used inside containers and deployed on Kubernetes clusters. There are certain prerequisites and information that would help deploy the library in your cluster.</p>"},{"location":"guides/deploying_in_clusters/#health-checks","title":"Health checks","text":"<p>The library uses the Triton Inference Server to handle HTTP/gRPC requests. Triton Server provides endpoints to validate if the server is ready and in a healthy state. The following API endpoints can be used in your orchestrator to control the application ready and live states:</p> <ul> <li>Ready: <code>/v2/health/ready</code></li> <li>Live: <code>/v2/health/live</code></li> </ul>"},{"location":"guides/deploying_in_clusters/#exposing-ports","title":"Exposing ports","text":"<p>The library uses the Triton Inference Server, which exposes the HTTP, gRPC, and metrics ports for communication. In the default configuration, the following ports have to be exposed:</p> <ul> <li>8000 for HTTP</li> <li>8001 for gRPC</li> <li>8002 for metrics</li> </ul> <p>If the library is inside a Docker container, the ports can be exposed by passing an extra argument to the <code>docker run</code> command. An example of passing ports configuration:</p> <pre><code>docker run -p 8000:8000 -p 8001:8001 -p 8002:8002 {image}\n</code></pre> <p>To deploy a container in Kubernetes, add a ports definition for the container in YAML deployment configuration:</p> <pre><code>containers:\n  - name: pytriton\n    ...\n    ports:\n      - containerPort: 8000\n        name: http\n      - containerPort: 8001\n        name: grpc\n      - containerPort: 8002\n        name: metrics\n</code></pre>"},{"location":"guides/deploying_in_clusters/#configuring-shared-memory","title":"Configuring shared memory","text":"<p>The connection between Python callbacks and the Triton Inference Server uses shared memory to pass data between the processes. In the Docker container, the default amount of shared memory is <code>64MB</code>, which may not be enough to pass input and output data of the model. The PyTriton initialize <code>16MB</code> of shared memory for <code>Proxy Backend</code> at start to pass input/output tensors between processes. The additional memory is allocated dynamically. In case of failure, the size of available shared memory might need to be increased.</p> <p>To increase the available shared memory size, pass an additional flag to the <code>docker run</code> command. An example of increasing the shared memory size to 8GB:</p> <p><pre><code>docker run --shm-size 8GB {image}\n</code></pre> To increase the shared memory size for Kubernetes, the following configuration can be used:</p> <pre><code>spec:\n  volumes:\n    - name: shared-memory\n      emptyDir:\n        medium: Memory\n  containers:\n    - name: pytriton\n      ...\n      volumeMounts:\n        - mountPath: /dev/shm\n          name: shared-memory\n</code></pre>"},{"location":"guides/deploying_in_clusters/#specify-container-init-process","title":"Specify container init process","text":"<p>You can use the <code>--init</code> flag of the <code>docker run</code> command to indicate that an init process should be used as the PID 1 in the container. Specifying an init process ensures that reaping zombie processes are performed inside the container. The reaping zombie processes functionality is important in case of an unexpected error occurrence in scripts hosting PyTriton.</p>"},{"location":"guides/distributed_tracing/","title":"Distributed Tracing","text":""},{"location":"guides/distributed_tracing/#distributed-tracing","title":"Distributed Tracing","text":"<p>Distributed tracing enables tracking application requests as they traverse through various services. This is crucial for monitoring the health and performance of distributed applications, especially in microservices architectures. This guide will demonstrate how to configure the Triton Inference Server to send traces to an OpenTelemetry collector and later visualize them.</p>"},{"location":"guides/distributed_tracing/#setting-up-the-opentelemetry-environment","title":"Setting Up the OpenTelemetry Environment","text":"<p>The OpenTelemetry collector serves to collect, process, and export traces and metrics. As a collector, you can utilize the Jaeger tracing platform, which also includes a UI for trace visualization. To run Jaeger backend components and the UI in a single container, execute the following command:</p> <pre><code>docker run -d --name jaeger \\\n    -e COLLECTOR_OTLP_ENABLED=true \\\n    -p 4318:4318 \\\n    -p 16686:16686 \\\n    jaegertracing/all-in-one:1\n</code></pre> <p>This command will initiate a daemon mode HTTP trace collector listening on port 4318 and the Jaeger UI on port 16686. You can then access the Jaeger UI via http://localhost:16686. Further details on the parameters of this Docker image can be found in the Jaeger Getting Started document.</p>"},{"location":"guides/distributed_tracing/#pytriton-and-distributed-tracing","title":"PyTriton and Distributed Tracing","text":"<p>With the OpenTelemetry collector set up, you can now configure the Triton Inference Server tracer to send trace spans to it.</p> <p>The following example demonstrates how to configure the Triton Inference Server to send traces to the OpenTelemetry collector:</p> <pre><code>from pytriton.triton import TritonConfig\nconfig=TritonConfig(\n    trace_config=[\n        \"level=TIMESTAMPS\",\n        \"rate=1\",\n        \"mode=opentelemetry\",\n        \"opentelemetry,url=http://localhost:4318/v1/traces\",\n        \"opentelemetry,resource=service.name=test_server_with_passthrough\",\n        \"opentelemetry,resource=test.key=test.value\",\n    ],\n)\n</code></pre> <p>Each parameter in the <code>trace_config</code> list corresponds to a specific configuration option:</p> <ul> <li><code>level=TIMESTAMPS</code>: Specifies the level of detail in the trace spans.</li> <li><code>rate=1</code>: Indicates that all requests should be traced.</li> <li><code>mode=opentelemetry</code>: Specifies the tracing mode.</li> <li><code>opentelemetry,url=http://localhost:4318/v1/traces</code>: Specifies the URL of the OpenTelemetry collector.</li> <li><code>opentelemetry,resource=service.name=test_server_with_passthrough</code>: Specifies the resource name for the service.</li> <li><code>opentelemetry,resource=test.key=test.value</code>: Specifies additional resource attributes.</li> </ul> <p>All the supported Triton Inference Server trace API settings are described in the user guide on tracing.</p> <p>You can use config for the Triton Inference Server as follows:</p> <pre><code>import time\nimport numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n\ndef passthrough(requests):\n    responses = []\n    for request in requests:\n        sleep = request.data[\"sleep\"]\n        error = request.data[\"error\"]\n        raise_error = np.any(error)\n        if raise_error:\n            raise ValueError(\"Requested Error\")\n        max_sleep = np.max(sleep).item()\n\n        time.sleep(max_sleep)\n\n        responses.append({\"sleep\": sleep, \"error\": error})\n    return responses\n\n# config was defined in example above\ntriton = Triton(config=config)\n\ntriton.bind(\n    model_name=\"passthrough\",\n    infer_func=passthrough,\n    inputs=[Tensor(name=\"sleep\", dtype=np.float32, shape=(1,)), Tensor(name=\"error\", dtype=np.bool_, shape=(1,))],\n    outputs=[Tensor(name=\"sleep\", dtype=np.float32, shape=(1,)), Tensor(name=\"error\", dtype=np.bool_, shape=(1,))],\n    config=ModelConfig(max_batch_size=128),\n    strict=False,\n)\ntriton.run()\n</code></pre> <p>Now, you can send requests with curl to the Triton Inference Server:</p> <pre><code>curl http://127.0.0.1:8000/v2/models/passthrough/generate \\\n    -H \"Content-Type: application/json\" \\\n    -sS \\\n    -w \"\\n\" \\\n    -d '{\"sleep\": 0.001, \"error\": false}'\n</code></pre> <p>The Triton Inference Server will send trace spans to the OpenTelemetry collector, which will visualize the trace in the Jaeger UI. The trace included above contains several span generated by Triton and PyTriton:</p> <ul> <li><code>InferRequest</code>: The span representing the entire request processing lifecycle.</li> <li><code>passthrough</code>: The span representing the execution of the <code>passthrough</code> model.</li> <li><code>compute</code>: The span representing the computation of the model as seen in the Triton Inference Server.</li> <li><code>python_backend_execute</code>: The span representing the <code>execute</code> of the Python backend function.</li> <li><code>proxy_inference_callable</code>: The span representing the proxy inference callable execution.</li> </ul> <p></p>"},{"location":"guides/distributed_tracing/#custom-tracing-with-traced_span-method","title":"Custom tracing with <code>traced_span</code> method","text":"<p>PyTriton provides a simplified way to instrument your code with telemetry using the <code>traced_span</code> method from the <code>Request</code> class. This method allows you to easily create spans for different parts of your code without needing to directly interact with the OpenTelemetry API.</p>"},{"location":"guides/distributed_tracing/#example","title":"Example","text":"<p>Here is an example of using the <code>traced_span</code> method within a passthrough function. This function processes requests, performing actions such as getting data, sleeping, and appending responses, with each step being traced for telemetry purposes.</p> <pre><code>import time\nimport numpy as np\n\ndef passthrough(requests):\n    responses = []\n    for request in requests:\n        # Create a traced span for getting data\n        with request.traced_span(\"pass-through-get-data\"):\n            sleep = request.data[\"sleep\"]\n            error = request.data[\"error\"]\n            raise_error = np.any(error)\n            if raise_error:\n                raise ValueError(\"Requested Error\")\n            max_sleep = np.max(sleep).item()\n\n        # Create a traced span for sleeping\n        with request.traced_span(\"pass-through-sleep\"):\n            time.sleep(max_sleep)\n\n        # Create a traced span for appending responses\n        with request.traced_span(\"pass-through-append\"):\n            responses.append({\"sleep\": sleep, \"error\": error})\n\n    return responses\n</code></pre> <p>The introduction of three spans (<code>pass-through-get-data</code>, <code>pass-through-sleep</code>, <code>pass-through-append</code>) in the <code>passthrough</code> function allows you to track the time spent on each operation. These spans will be sent to the OpenTelemetry collector and visualized in the Jaeger UI.</p> <p></p>"},{"location":"guides/distributed_tracing/#explanation","title":"Explanation","text":"<ol> <li> <p>Creating Spans with <code>traced_span</code>:</p> <ul> <li>For each request, we use the <code>traced_span</code> method provided by the <code>Request</code> class to create spans for different operations. This method automatically handles the start and end of spans, simplifying the instrumentation process.</li> </ul> </li> <li> <p>Getting Data:</p> <ul> <li>We wrap the data extraction logic in a <code>traced_span</code> named <code>\"pass-through-get-data\"</code>. This span captures the process of extracting the <code>sleep</code> and <code>error</code> data from the request, checking for errors, and determining the maximum sleep time.</li> </ul> </li> <li> <p>Sleeping:</p> <ul> <li>We wrap the sleep operation in a <code>traced_span</code> named <code>\"pass-through-sleep\"</code>. This span captures the time spent sleeping.</li> </ul> </li> <li> <p>Appending Responses:</p> <ul> <li>We wrap the response appending logic in a <code>traced_span</code> named <code>\"pass-through-append\"</code>. This span captures the process of appending the response.</li> </ul> </li> </ol>"},{"location":"guides/distributed_tracing/#benefits","title":"Benefits","text":"<ul> <li>Simplicity: Using the <code>traced_span</code> method is straightforward and does not require direct interaction with the OpenTelemetry API, making it easier to instrument your code.</li> <li>Automatic Management: The <code>traced_span</code> method automatically manages the lifecycle of spans, reducing boilerplate code and potential errors.</li> <li>Seamless Integration: This approach integrates seamlessly with existing PyTriton infrastructure, ensuring consistent telemetry data collection.</li> </ul>"},{"location":"guides/distributed_tracing/#advanced-telemetry-usage-with-opentelemetry-api","title":"Advanced Telemetry Usage with OpenTelemetry API","text":"<p>In addition to the simple telemetry example provided using the PyTriton API, we can also leverage the direct usage of the OpenTelemetry API for more fine-grained control over tracing and telemetry. This advanced approach provides flexibility and a deeper integration with OpenTelemetry.</p>"},{"location":"guides/distributed_tracing/#example_1","title":"Example","text":"<p>Here is an example of using the OpenTelemetry API directly within a passthrough function. This function processes requests, performing actions such as getting data, sleeping, and appending responses, with each step being traced for telemetry purposes.</p> <pre><code>from opentelemetry import trace\nimport time\nimport numpy as np\n\n# Initialize a tracer for the current module\ntracer = trace.get_tracer(__name__)\n\ndef passthrough(requests):\n    responses = []\n    for request in requests:\n        # Use the span associated with the request, but do not end it automatically\n        with trace.use_span(request.span, end_on_exit=False):\n            # Start a new span for getting data\n            with tracer.start_as_current_span(\"pass-through-get-data\"):\n                sleep = request.data[\"sleep\"]\n                error = request.data[\"error\"]\n                raise_error = np.any(error)\n                if raise_error:\n                    raise ValueError(\"Requested Error\")\n                max_sleep = np.max(sleep).item()\n\n            # Start a new span for sleeping\n            with tracer.start_as_current_span(\"pass-through-sleep\"):\n                time.sleep(max_sleep)\n\n            # Start a new span for appending responses\n            with tracer.start_as_current_span(\"pass-through-append\"):\n                responses.append({\"sleep\": sleep, \"error\": error})\n\n    return responses\n</code></pre> <p></p>"},{"location":"guides/distributed_tracing/#explanation_1","title":"Explanation","text":"<ol> <li> <p>Initialization of Tracer:</p> <ul> <li>We initialize a tracer for the current module using <code>trace.get_tracer(__name__)</code>. This tracer will be used to create spans that represent individual operations within the <code>passthrough</code> function.</li> </ul> </li> <li> <p>Using Existing Spans:</p> <ul> <li>For each request, we use the span already associated with it by wrapping the processing logic within <code>trace.use_span(request.span, end_on_exit=False)</code>. This ensures that our custom spans are nested within the request's span, providing a hierarchical structure to the telemetry data.</li> </ul> </li> <li> <p>Creating Custom Spans:</p> <ul> <li>We create custom spans for different operations (<code>pass-through-get-data</code>, <code>pass-through-sleep</code>, <code>pass-through-append</code>) using <code>tracer.start_as_current_span</code>. Each operation is wrapped in its respective span, capturing the execution time and any additional attributes or events we might want to add.</li> </ul> </li> </ol>"},{"location":"guides/distributed_tracing/#benefits_1","title":"Benefits","text":"<ul> <li>Flexible Integration: Using the OpenTelemetry API directly allows for greater flexibility in how spans are managed and how telemetry data is collected and reported.</li> <li>Seamless Fallback: The use of <code>trace.use_span</code> ensures that if telemetry is not active, the span operations are effectively no-ops, avoiding unnecessary checks and minimizing overhead.</li> </ul>"},{"location":"guides/distributed_tracing/#opentelemetry-context-propagation","title":"OpenTelemetry Context Propagation","text":"<p>Triton Inference Server supports OpenTelemetry context propagation, enabling the tracing of requests across multiple services. This is particularly useful in microservices architectures where the Triton Inference Server is one of many services involved in processing a request.</p> <p>To test this feature, you can use the following Python client based on python requests package. This client will send a request to the Triton Inference Server and propagates its own OpenTelemetry context. First, install the required packages:</p> <pre><code>pip install \"opentelemetry-api&lt;=1.27.0\" \\\n    \"opentelemetry-sdk&lt;=1.27.0\" \\\n    \"opentelemetry-instrumentation-requests&lt;=1.27.0\" \\\n    \"opentelemetry-exporter-otlp&lt;=1.27.0\"\n</code></pre> <p>The conflict between <code>tritonclient == 2.50.0</code> and <code>opentelemetry-api == 1.28.0</code> can prevent you rom installing both. To avoid this issue, you can force telemetry to use the version <code>1.27.0</code>.</p> <p>First you need to import the required packages and configure the OpenTelemetry context and instrumet requests library:</p> <pre><code>import time\nimport requests\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\n\n# Enable instrumentation in the requests library.\nRequestsInstrumentor().instrument()\n</code></pre> <p>The next step is to configure the OpenTelemetry context:</p> <pre><code># OTLPSpanExporter can be also configured with OTEL_EXPORTER_OTLP_TRACES_ENDPOINT environment variable\ntrace.set_tracer_provider(\n    TracerProvider(\n        resource=Resource(attributes={\"service.name\": \"upstream-service\"}),\n        active_span_processor=BatchSpanProcessor(OTLPSpanExporter(endpoint=\"http://127.0.0.1:4318/v1/traces\")),\n    )\n)\n</code></pre> <p>The final step is to send a request to the Triton Inference Server and propagate the OpenTelemetry context:</p> <pre><code>tracer = trace.get_tracer(__name__)\nwith tracer.start_as_current_span(\"outgoing-request\"):\n    time.sleep(0.001)\n    response = requests.post(\n        \"http://127.0.0.1:8000/v2/models/passthrough/generate\",\n        headers={\"Content-Type\": \"application/json\"},\n        json={\"sleep\": 0.001, \"error\": False},\n    )\n    time.sleep(0.001)\n\nprint(response.json())\n</code></pre> <p>This script sends a request to the Triton Inference Server and propagates its own OpenTelemetry context. The Triton Inference Server will then forward this context to the OpenTelemetry collector, which will visualize the trace in the Jaeger UI. The trace included above contains two spans: <code>outgoing-request</code> explicitly created in your script and <code>POST</code> created by requests instrumentation.</p> <p></p>"},{"location":"guides/downloaded_input_data/","title":"Downloaded input data","text":""},{"location":"guides/downloaded_input_data/#example-with-downloaded-input-data","title":"Example with downloaded input data","text":"<p>In the following example, we will demonstrate how to effectively utilize PyTriton with downloaded input data. While the model itself does not possess any inputs, it utilize custom parameters or headers to extract a URL and download data from an external source, such as an S3 bucket.</p> <p>The corresponding function can leverage the batch decorator since it does not rely on any parameters or headers.</p>"},{"location":"guides/downloaded_input_data/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\n\n@batch\ndef model_infer_function(**inputs):\n    ...\n\ndef request_infer_function(requests):\n    for request in requests:\n        image_url = request.parameters[\"custom_url\"]\n        image_jpeg = download(image_url)\n        image_data = decompress(image_jpeg)\n        request['images_data'] = image_data\n    outputs = model_infer_function(requests)\n    return outputs\n\nwith Triton(config=TritonConfig(http_header_forward_pattern=\"custom.*\")) as triton:\n    triton.bind(\n        model_name=\"ImgModel\",\n        infer_func=request_infer_function,\n        inputs=[],\n        outputs=[Tensor(name=\"out\", dtype=np.float32, shape=(-1,))],\n        config=ModelConfig(max_batch_size=128),\n    )\n    triton.serve()\n</code></pre>"},{"location":"inference_callables/","title":"Overview","text":""},{"location":"inference_callables/#inference-callable","title":"Inference Callable","text":"<p>The inference callable is an entry point for handling inference requests. The interface of the inference callable assumes it receives a list of requests with input dictionaries, where each dictionary represents one request mapping model input names to NumPy ndarrays. Requests contain also custom HTTP/gRPC headers and parameters in parameters dictionary.</p> <p>This document provides guidelines for creating an Inference Callable for PyTriton, which serves as the entry point for handling inference requests.</p>"},{"location":"inference_callables/#overview","title":"Overview","text":"<p>The simplest Inference Callable is a function that implements the interface to handle requests and returns responses.</p> <p>The Request class contains the following fields:</p> <ul> <li><code>data</code> - for inputs stored as a mapping. It can also be accessed with the request mapping protocol of the <code>Request</code> object (e.g., request[\"input_name\"])</li> <li><code>parameters</code> - for mapping consisting of combined parameters and HTTP/gRPC headers</li> </ul> <p>For more information about parameters and headers, see here.</p> <pre><code>import numpy as np\nfrom typing import Dict, List\nfrom pytriton.proxy.types import Request\n\ndef infer_fn(requests: List[Request]) -&gt; List[Dict[str, np.ndarray]]:\n    ...\n</code></pre> <p>In many cases, it is worth implement Inference Callable as method. This is especially useful when you want to have control over pipeline instance initialization.</p> <pre><code>import numpy as np\nfrom typing import Dict, List\nfrom pytriton.proxy.types import Request\n\nclass InferCallable:\n\n    def __init__(self, *args, **kwargs):\n        ...  # model initialization\n\n    def __call__(self, requests: List[Request]) -&gt; List[Dict[str, np.ndarray]]:\n        ...\n</code></pre>"},{"location":"inference_callables/#asynchronous-interface","title":"Asynchronous Interface","text":"<p>Some models can run asynchronously, meaning they can process multiple requests at the same time without waiting for each one to finish. If your model supports this feature, you can use an asynchronous coroutine to define the Inference Callable.</p> <pre><code>import numpy as np\nfrom typing import Dict, List\nfrom pytriton.proxy.types import Request\n\nasync def infer_fn(requests: List[Request]) -&gt; List[Dict[str, np.ndarray]]:\n    ...\n</code></pre>"},{"location":"inference_callables/#streaming-partial-results","title":"Streaming Partial Results","text":"<p>Some models can send more than one response for a request, or no response at all. This is useful for models that produce intermediate results or stream data continuously. If your model supports this feature, you can use a generator function or an asynchronous generator coroutine to define the Inference Callable:</p> <pre><code>import numpy as np\nfrom typing import AsyncGenerator, Dict, Generator, List\nfrom pytriton.proxy.types import Request\n\ndef infer_fn(requests: List[Request]) -&gt; Generator[Dict[str, np.ndarray], None, None]:\n    ...\n\nasync def infer_fn(requests: List[Request]) -&gt; AsyncGenerator[Dict[str, np.ndarray], None]:\n    ...\n</code></pre> <p>This feature only works when the model is served in decoupled mode. For more information, see the Decoupled Models section.</p>"},{"location":"inference_callables/#binding-to-triton","title":"Binding to Triton","text":"<p>To use the Inference Callable with PyTriton, it must be bound to a Triton server instance using the bind method. This method takes the following arguments:</p> <ul> <li><code>model_name</code>: The name of the model that will be used by Triton clients.</li> <li><code>infer_func</code>: The Inference Callable described above.</li> <li><code>inputs</code>: A list of Tensor objects that describe the input tensors expected by the model.</li> <li><code>outputs</code>: A list of Tensor objects that describe the output tensors produced by the model.</li> <li><code>config</code>: A ModelConfig object that specifies additional configuration options for the model (e.g., maximum batch size).</li> </ul> <pre><code>import numpy as np\nfrom pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\n\ndef infer_fn(requests: List[Request]) -&gt; List[Dict[str, np.ndarray]]:\n    ...\n\nwith Triton() as triton:\n    triton.bind(\n        model_name=\"MyModel\",\n        infer_func=infer_fn,\n        inputs=[Tensor(shape=(1,), dtype=np.float32)],\n        outputs=[Tensor(shape=(1,), dtype=np.float32)],\n        config=ModelConfig(max_batch_size=8)\n    )\n</code></pre> <p>For more information on serving the Inference Callable, refer to the Loading models section on the Deploying Models page.</p>"},{"location":"inference_callables/custom_params/","title":"Custom parameters/headers","text":""},{"location":"inference_callables/custom_params/#custom-httpgrpc-headers-and-parameters","title":"Custom HTTP/gRPC headers and parameters","text":"<p>This document provides guidelines for using custom HTTP/gRPC headers and parameters with PyTriton. Original Triton documentation related to parameters can be found here. Now, undecorated inference function accepts list of Request instances. Request class contains following fields:</p> <ul> <li>data - for inputs (stored as dictionary, but can be also accessed with request dict interface e.g. request[\"input_name\"])</li> <li>parameters - for combined parameters and HTTP/gRPC headers</li> </ul> <p>Parameters/headers usage limitations</p> <p>Currently, custom parameters and headers can be only accessed in undecorated inference function (they don't work with decorators). There is separate example how to use parameters/headers in preprocessing step (see here)</p>"},{"location":"inference_callables/custom_params/#parameters","title":"Parameters","text":"<p>Parameters are passed to the inference callable as a dictionary. The dictionary is stored in HTTP/gRPC request body payload.</p>"},{"location":"inference_callables/custom_params/#httpgrpc-headers","title":"HTTP/gRPC headers","text":"<p>Custom HTTP/gRPC headers are passed to the inference callable in the same dictionary as parameters, but they are stored in HTTP/gRPC request headers instead of the request body payload. For the headers it is also necessary to specify the header prefix in Triton config, which is used to distinguish  the custom headers from standard ones (only headers with specified prefix are passed to the inference callable).</p>"},{"location":"inference_callables/custom_params/#usage","title":"Usage","text":"<ol> <li> <p>Define inference callable (that one uses one parameter and one header):</p> <pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\n\ndef _infer_with_params_and_headers(requests):\n    responses = []\n    for req in requests:\n        a_batch, b_batch = req.values()\n        scaled_add_batch = (a_batch + b_batch) / float(req.parameters[\"header_divisor\"])\n        scaled_sub_batch = (a_batch - b_batch) * float(req.parameters[\"parameter_multiplier\"])\n        responses.append({\"scaled_add\": scaled_add_batch, \"scaled_sub\": scaled_sub_batch})\n    return responses\n</code></pre> </li> <li> <p>Bind inference callable to Triton (\"header\" is the prefix for custom headers):</p> <p> <pre><code>triton = Triton(config=TritonConfig(http_header_forward_pattern=\"header.*\"))\ntriton.bind(\n    model_name=\"ParamsAndHeaders\",\n    infer_func=_infer_with_params_and_headers,\n    inputs=[\n        Tensor(dtype=np.float32, shape=(-1,)),\n        Tensor(dtype=np.float32, shape=(-1,)),\n    ],\n    outputs=[\n        Tensor(name=\"scaled_add\", dtype=np.float32, shape=(-1,)),\n        Tensor(name=\"scaled_sub\", dtype=np.float32, shape=(-1,)),\n    ],\n    config=ModelConfig(max_batch_size=128),\n)\n\ntriton.run()\n</code></pre></p> </li> <li> <p>Call the model using ModelClient:</p> <p><pre><code>import numpy as np\nfrom pytriton.client import ModelClient\n\nbatch_size = 2\na_batch = np.ones((batch_size, 1), dtype=np.float32) * 2\nb_batch = np.ones((batch_size, 1), dtype=np.float32)\n</code></pre> <pre><code>with ModelClient(\"localhost\", \"ParamsAndHeaders\") as client:\n    result_batch = client.infer_batch(a_batch, b_batch, parameters={\"parameter_multiplier\": 2}, headers={\"header_divisor\": 3})\n</code></pre></p> <p> </p> </li> </ol>"},{"location":"inference_callables/decorators/","title":"Decorators","text":""},{"location":"inference_callables/decorators/#decorators","title":"Decorators","text":"<p>The PyTriton provide decorators for operations on input requests to simplify passing the requests to the model inputs. We have prepared several useful decorators for converting generic request input into common user needs. You can create custom decorators tailored to your requirements and chain them with other decorators.</p>"},{"location":"inference_callables/decorators/#batch","title":"Batch","text":"<p>In many cases, it is more convenient to receive input already batched in the form of a NumPy array instead of a list of separate requests. For such cases, we have prepared the <code>@batch</code> decorator that adapts generic input into a batched form. It passes kwargs to the inference function where each named input contains a NumPy array with a batch of requests received by the Triton server.</p> <p>Below, we show the difference between decorated and undecorated functions bound with Triton:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.proxy.types import Request\n\n# Sample input data with 2 requests - each with 2 inputs\ninput_data = [\n     Request({'in1': np.array([[1, 1]]), 'in2': np.array([[2, 2]])}),\n     Request({'in1': np.array([[1, 2]]), 'in2': np.array([[2, 3]])})\n]\n\n\ndef undecorated_identity_fn(requests):\n    print(requests)\n    # As expected, requests = [\n    #     Request({'in1': np.array([[1, 1]]), 'in2': np.array([[2, 2]])}),\n    #     Request({'in1': np.array([[1, 2]]), 'in2': np.array([[2, 3]])}),\n    # ]\n    results = requests\n    return results\n\n\n@batch\ndef decorated_identity_fn(in1, in2):\n    print(in1, in2)\n    # in1 = np.array([[1, 1], [1, 2]])\n    # in2 = np.array([[2, 2], [2, 3]])\n    # Inputs are batched by `@batch` decorator and passed to the function as kwargs, so they can be automatically mapped\n    # with in1, in2 function parameters\n    # Of course, we could get these params explicitly with **kwargs like this:\n    # def decorated_infer_fn(**kwargs):\n    return {\"out1\": in1, \"out2\": in2}\n\n\nundecorated_identity_fn(input_data)\ndecorated_identity_fn(input_data)\n</code></pre> <p>More examples using the <code>@batch</code> decorator with different frameworks are shown below.</p> <p>Example implementation for TensorFlow model:</p> <pre><code>import numpy as np\nimport tensorflow as tf\n\nfrom pytriton.decorators import batch\n\n\n@batch\ndef infer_tf_fn(**inputs: np.ndarray):\n    (images_batch,) = inputs.values()\n    images_batch_tensor = tf.convert_to_tensor(images_batch)\n    output1_batch = model.predict(images_batch_tensor)\n    return [output1_batch]\n</code></pre> <p>Example implementation for PyTorch model:</p> <pre><code>import numpy as np\nimport torch\n\nfrom pytriton.decorators import batch\n\n\n@batch\ndef infer_pt_fn(**inputs: np.ndarray):\n    (input1_batch,) = inputs.values()\n    input1_batch_tensor = torch.from_numpy(input1_batch).to(\"cuda\")\n    output1_batch_tensor = model(input1_batch_tensor)\n    output1_batch = output1_batch_tensor.cpu().detach().numpy()\n    return [output1_batch]\n</code></pre> <p>Example implementation with named inputs and outputs:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n\n\n@batch\ndef add_subtract_fn(a: np.ndarray, b: np.ndarray):\n    return {\"add\": a + b, \"sub\": a - b}\n\n\n@batch\ndef multiply_fn(**inputs: np.ndarray):\n    a = inputs[\"a\"]\n    b = inputs[\"b\"]\n    return [a * b]\n</code></pre> <p>Example implementation with strings:</p> <pre><code>import numpy as np\nfrom transformers import pipeline\nfrom pytriton.decorators import batch\n\nCLASSIFIER = pipeline(\"zero-shot-classification\", model=\"facebook/bart-base\", device=0)\n\n\n@batch\ndef classify_text_fn(text_array: np.ndarray):\n    text = text_array[0]  # text_array contains one string at index 0\n    text = text.decode(\"utf-8\")  # string is stored in byte array encoded in utf-8\n    result = CLASSIFIER(text)\n    return [np.array(result)]  # return statistics generated by classifier\n</code></pre>"},{"location":"inference_callables/decorators/#sample","title":"Sample","text":"<p><code>@sample</code> - takes the first request and converts it into named inputs. This decorator is useful with non-batching models. Instead of a one-element list of requests, we get named inputs - <code>kwargs</code>.</p> <pre><code>from pytriton.decorators import sample\n\n\n@sample\ndef infer_fn(sequence):\n    pass\n</code></pre>"},{"location":"inference_callables/decorators/#group-by-keys","title":"Group by keys","text":"<p><code>@group_by_keys</code> - groups requests with the same set of keys and calls the wrapped function for each group separately. This decorator is convenient to use before batching because the batching decorator requires a consistent set of inputs as it stacks them into batches.</p> <pre><code>from pytriton.decorators import batch, group_by_keys\n\n\n@group_by_keys\n@batch\ndef infer_fn(mandatory_input, optional_input=None):\n    # perform inference\n    pass\n</code></pre>"},{"location":"inference_callables/decorators/#group-by-values","title":"Group by values","text":"<p><code>@group_by_values(*keys)</code> - groups requests with the same input value (for selected keys) and calls the wrapped function for each group separately. This decorator is particularly useful with models requiring dynamic parameters sent by users, such as temperature. In this case, we want to run the model only for requests with the same temperature value.</p> <pre><code>from pytriton.decorators import batch, group_by_values\n\n\n@batch\n@group_by_values('temperature')\ndef infer_fn(mandatory_input, temperature):\n    # perform inference\n    pass\n</code></pre>"},{"location":"inference_callables/decorators/#fill-optionals","title":"Fill optionals","text":"<p><code>@fill_optionals(**defaults)</code> - fills missing inputs in requests with default values provided by the user. If model owners have default values for some optional parameters, it's a good idea to provide them at the beginning, so other decorators can create larger consistent groups and send them to the inference callable.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, group_by_values\n\n\n@fill_optionals(temperature=np.array([0.7]))\n@batch\n@group_by_values('temperature')\ndef infer_fn(mandatory_input, temperature):\n    # perform inference\n    pass\n</code></pre> <p>``</p>"},{"location":"inference_callables/decorators/#pad-batch","title":"Pad batch","text":"<p><code>@pad_batch</code> - appends the last row to the input multiple times to achieve the desired batch size (preferred batch size or max batch size from the model config, whichever is closer to the current input size).</p> <pre><code>from pytriton.decorators import batch, pad_batch\n\n@batch\n@pad_batch\ndef infer_fn(mandatory_input):\n    # this model requires mandatory_input batch to be the size provided in the model config\n    pass\n</code></pre>"},{"location":"inference_callables/decorators/#first-value","title":"First value","text":"<p><code>@first_value</code> - this decorator takes the first elements from batches for selected inputs specified by the <code>keys</code> parameter. If the value is a one-element array, it is converted to a scalar value. This decorator is convenient to use with dynamic model parameters that users send in requests. You can use <code>@group_by_values</code> before to have batches with the same values in each batch.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, first_value, group_by_values\n\n@fill_optionals(temperature=np.array([0.7]))\n@batch\n@group_by_values('temperature')\n@first_value('temperature')\ndef infer_fn(mandatory_input, temperature):\n    # perform inference with scalar temperature=10\n    pass\n</code></pre>"},{"location":"inference_callables/decorators/#triton-context","title":"Triton context","text":"<p>The <code>@triton_context</code> decorator provides an additional argument called <code>triton_context</code>, from which you can read the model config.</p> <p>```python  from pytriton.decorators import triton_context</p> <p>@triton_context def infer_fn(input_list, **kwargs):     model_config = kwargs['triton_context'].model_config     # perform inference using some information from model_config     pass  ```</p>"},{"location":"inference_callables/decorators/#stacking-multiple-decorators","title":"Stacking multiple decorators","text":"<p>Here is an example of stacking multiple decorators together. We recommend starting with type 1 decorators, followed by types 2 and 3. Place the <code>@triton_context</code> decorator last in the chain.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, first_value, group_by_keys, group_by_values, triton_context\n\n\n@fill_optionals(temperature=np.array([0.7]))\n@group_by_keys\n@batch\n@group_by_values('temperature')\n@first_value('temperature')\n@triton_context\ndef infer(triton_context, mandatory_input, temperature, opt1=None, opt2=None):\n    model_config = triton_context.model_config\n    # perform inference using:\n    #   - some information from model_config\n    #   - scalar temperature value\n    #   - optional parameters opt1 and/or opt2\n</code></pre>"},{"location":"reference/clients/","title":"Model Clients","text":""},{"location":"reference/clients/#model-clients","title":"Model Clients","text":""},{"location":"reference/clients/#pytriton.client.ModelClient","title":"pytriton.client.ModelClient","text":"<pre><code>ModelClient(url: str, model_name: str, model_version: Optional[str] = None, *, lazy_init: bool = True, init_timeout_s: Optional[float] = None, inference_timeout_s: Optional[float] = None, model_config: Optional[TritonModelConfig] = None, ensure_model_is_ready: bool = True)\n</code></pre> <p>               Bases: <code>BaseModelClient</code></p> <p>Synchronous client for model deployed on the Triton Inference Server.</p> <p>Inits ModelClient for given model deployed on the Triton Inference Server.</p> <p>If <code>lazy_init</code> argument is False, model configuration will be read from inference server during initialization.</p> <p>Common usage:</p> <pre><code>client = ModelClient(\"localhost\", \"BERT\")\nresult_dict = client.infer_sample(input1_sample, input2_sample)\nclient.close()\n</code></pre> <p>Client supports also context manager protocol:</p> <pre><code>with ModelClient(\"localhost\", \"BERT\") as client:\n    result_dict = client.infer_sample(input1_sample, input2_sample)\n</code></pre> <p>The creation of client requires connection to the server and downloading model configuration. You can create client from existing client using the same class:</p> <pre><code>client = ModelClient.from_existing_client(existing_client)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The Triton Inference Server url, e.g. 'grpc://localhost:8001'. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>name of the model to interact with.</p> </li> <li> <code>model_version</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> </li> <li> <code>lazy_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if initialization should be performed just before sending first request to inference server.</p> </li> <li> <code>init_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout for maximum waiting time in loop, which sends retry requests ask if model is ready. It is applied at initialization time only when <code>lazy_init</code> argument is False. Default is to do retry loop at first inference.</p> </li> <li> <code>inference_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout in seconds for the model inference process. If non passed default 60 seconds timeout will be used. For HTTP client it is not only inference timeout but any client request timeout - get model config, is model loaded. For GRPC client it is only inference timeout.</p> </li> <li> <code>model_config</code>               (<code>Optional[TritonModelConfig]</code>, default:                   <code>None</code> )           \u2013            <p>model configuration. If not passed, it will be read from inference server during initialization.</p> </li> <li> <code>ensure_model_is_ready</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if model should be checked if it is ready before first inference request.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> </li> <li> <code>PyTritonClientUrlParseError</code>             \u2013            <p>In case of problems with parsing url.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    lazy_init: bool = True,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n    model_config: Optional[TritonModelConfig] = None,\n    ensure_model_is_ready: bool = True,\n):\n    \"\"\"Inits ModelClient for given model deployed on the Triton Inference Server.\n\n    If `lazy_init` argument is False, model configuration will be read\n    from inference server during initialization.\n\n    Common usage:\n\n    ```python\n    client = ModelClient(\"localhost\", \"BERT\")\n    result_dict = client.infer_sample(input1_sample, input2_sample)\n    client.close()\n    ```\n\n    Client supports also context manager protocol:\n\n    ```python\n    with ModelClient(\"localhost\", \"BERT\") as client:\n        result_dict = client.infer_sample(input1_sample, input2_sample)\n    ```\n\n    The creation of client requires connection to the server and downloading model configuration. You can create client from existing client using the same class:\n\n    ```python\n    client = ModelClient.from_existing_client(existing_client)\n    ```\n\n    Args:\n        url: The Triton Inference Server url, e.g. 'grpc://localhost:8001'.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout for maximum waiting time in loop, which sends retry requests ask if model is ready. It is applied at initialization time only when `lazy_init` argument is False. Default is to do retry loop at first inference.\n        inference_timeout_s: timeout in seconds for the model inference process.\n            If non passed default 60 seconds timeout will be used.\n            For HTTP client it is not only inference timeout but any client request timeout\n            - get model config, is model loaded. For GRPC client it is only inference timeout.\n        model_config: model configuration. If not passed, it will be read from inference server during initialization.\n        ensure_model_is_ready: if model should be checked if it is ready before first inference request.\n\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError:\n            if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientUrlParseError: In case of problems with parsing url.\n    \"\"\"\n    super().__init__(\n        url=url,\n        model_name=model_name,\n        model_version=model_version,\n        lazy_init=lazy_init,\n        init_timeout_s=init_timeout_s,\n        inference_timeout_s=inference_timeout_s,\n        model_config=model_config,\n        ensure_model_is_ready=ensure_model_is_ready,\n    )\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.is_batching_supported","title":"is_batching_supported  <code>property</code>","text":"<pre><code>is_batching_supported\n</code></pre> <p>Checks if model supports batching.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"reference/clients/#pytriton.client.ModelClient.model_config","title":"model_config  <code>property</code>","text":"<pre><code>model_config: TritonModelConfig\n</code></pre> <p>Obtain the configuration of the model deployed on the Triton Inference Server.</p> <p>This method waits for the server to get into readiness state before obtaining the model configuration.</p> <p>Returns:</p> <ul> <li> <code>TritonModelConfig</code> (              <code>TritonModelConfig</code> )          \u2013            <p>configuration of the model deployed on the Triton Inference Server.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If the server and model are not in readiness state before the given timeout.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If the model with the given name (and version) is unavailable.</p> </li> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If the hosting process receives SIGINT.</p> </li> <li> <code>PyTritonClientClosedError</code>             \u2013            <p>If the ModelClient is closed.</p> </li> </ul>"},{"location":"reference/clients/#pytriton.client.ModelClient.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Create context for using ModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __enter__(self):\n    \"\"\"Create context for using ModelClient as a context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.__exit__","title":"__exit__","text":"<pre><code>__exit__(*_)\n</code></pre> <p>Close resources used by ModelClient instance when exiting from the context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __exit__(self, *_):\n    \"\"\"Close resources used by ModelClient instance when exiting from the context.\"\"\"\n    self.close()\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close resources used by ModelClient.</p> <p>This method closes the resources used by the ModelClient instance, including the Triton Inference Server connections. Once this method is called, the ModelClient instance should not be used again.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def close(self):\n    \"\"\"Close resources used by ModelClient.\n\n    This method closes the resources used by the ModelClient instance,\n    including the Triton Inference Server connections.\n    Once this method is called, the ModelClient instance should not be used again.\n    \"\"\"\n    _LOGGER.debug(\"Closing ModelClient\")\n    try:\n        if self._general_client is not None:\n            self._general_client.close()\n        if self._infer_client is not None:\n            self._infer_client.close()\n        self._general_client = None\n        self._infer_client = None\n    except Exception as e:\n        _LOGGER.error(f\"Error while closing ModelClient resources: {e}\")\n        raise e\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.create_client_from_url","title":"create_client_from_url","text":"<pre><code>create_client_from_url(url: str, network_timeout_s: Optional[float] = None)\n</code></pre> <p>Create Triton Inference Server client.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>url of the server to connect to. If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added. If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.</p> </li> <li> <code>network_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout for client commands. Default value is 60.0 s.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Triton Inference Server client.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientInvalidUrlError</code>             \u2013            <p>If provided Triton Inference Server url is invalid.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def create_client_from_url(self, url: str, network_timeout_s: Optional[float] = None):\n    \"\"\"Create Triton Inference Server client.\n\n    Args:\n        url: url of the server to connect to.\n            If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added.\n            If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.\n        network_timeout_s: timeout for client commands. Default value is 60.0 s.\n\n    Returns:\n        Triton Inference Server client.\n\n    Raises:\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\n    self._triton_url = TritonUrl.from_url(url)\n    self._url = self._triton_url.without_scheme\n    self._triton_client_lib = self.get_lib()\n    self._monkey_patch_client()\n\n    if self._triton_url.scheme == \"grpc\":\n        # by default grpc client has very large number of timeout, thus we want to make it equal to http client timeout\n        network_timeout_s = _DEFAULT_NETWORK_TIMEOUT_S if network_timeout_s is None else network_timeout_s\n        warnings.warn(\n            f\"tritonclient.grpc doesn't support timeout for other commands than infer. Ignoring network_timeout: {network_timeout_s}.\",\n            NotSupportedTimeoutWarning,\n            stacklevel=1,\n        )\n\n    triton_client_init_kwargs = self._get_init_extra_args()\n\n    _LOGGER.debug(\n        f\"Creating InferenceServerClient for {self._triton_url.with_scheme} with {triton_client_init_kwargs}\"\n    )\n    return self._triton_client_lib.InferenceServerClient(self._url, **triton_client_init_kwargs)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.from_existing_client","title":"from_existing_client  <code>classmethod</code>","text":"<pre><code>from_existing_client(existing_client: BaseModelClient)\n</code></pre> <p>Create a new instance from an existing client using the same class.</p> <p>Common usage: <pre><code>client = BaseModelClient.from_existing_client(existing_client)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>existing_client</code>               (<code>BaseModelClient</code>)           \u2013            <p>An instance of an already initialized subclass.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A new instance of the same subclass with shared configuration and readiness state.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>@classmethod\ndef from_existing_client(cls, existing_client: \"BaseModelClient\"):\n    \"\"\"Create a new instance from an existing client using the same class.\n\n    Common usage:\n    ```python\n    client = BaseModelClient.from_existing_client(existing_client)\n    ```\n\n    Args:\n        existing_client: An instance of an already initialized subclass.\n\n    Returns:\n        A new instance of the same subclass with shared configuration and readiness state.\n    \"\"\"\n    kwargs = {}\n    # Copy model configuration and readiness state if present\n    if hasattr(existing_client, \"_model_config\"):\n        kwargs[\"model_config\"] = existing_client._model_config\n        kwargs[\"ensure_model_is_ready\"] = False\n\n    new_client = cls(\n        url=existing_client._url,\n        model_name=existing_client._model_name,\n        model_version=existing_client._model_version,\n        init_timeout_s=existing_client._init_timeout_s,\n        inference_timeout_s=existing_client._inference_timeout_s,\n        **kwargs,\n    )\n\n    return new_client\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.get_lib","title":"get_lib","text":"<pre><code>get_lib()\n</code></pre> <p>Returns tritonclient library for given scheme.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n    \"\"\"Returns tritonclient library for given scheme.\"\"\"\n    return {\"grpc\": tritonclient.grpc, \"http\": tritonclient.http}[self._triton_url.scheme.lower()]\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.infer_batch","title":"infer_batch","text":"<pre><code>infer_batch(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs) -&gt; Dict[str, ndarray]\n</code></pre> <p>Run synchronous inference on batched data.</p> <p>Typical usage:</p> <pre><code>client = ModelClient(\"localhost\", \"MyModel\")\nresult_dict = client.infer_batch(input1, input2)\nclient.close()\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = client.infer_batch(input1, input2)\nresult_dict = client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>Inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom inference headers.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>Inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, ndarray]</code>           \u2013            <p>Dictionary with inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientValueError</code>             \u2013            <p>If mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If the wait time for the server and model being ready exceeds <code>init_timeout_s</code> or inference request time exceeds <code>inference_timeout_s</code>.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If the model with the given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If an error occurred on the inference callable or Triton Inference Server side.</p> </li> <li> <code>PyTritonClientModelDoesntSupportBatchingError</code>             \u2013            <p>If the model doesn't support batching.</p> </li> <li> <code>PyTritonClientValueError</code>             \u2013            <p>if mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>inference_timeout_s</code> passed to <code>__init__</code>.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If error occurred on inference callable or Triton Inference Server side,</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_batch(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run synchronous inference on batched data.\n\n    Typical usage:\n\n    ```python\n    client = ModelClient(\"localhost\", \"MyModel\")\n    result_dict = client.infer_batch(input1, input2)\n    client.close()\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    result_dict = client.infer_batch(input1, input2)\n    result_dict = client.infer_batch(a=input1, b=input2)\n    ```\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Custom inference parameters.\n        headers: Custom inference headers.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        Dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: If mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError: If the wait time for the server and model being ready exceeds `init_timeout_s` or\n            inference request time exceeds `inference_timeout_s`.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If an error occurred on the inference callable or Triton Inference Server side.\n        PyTritonClientModelDoesntSupportBatchingError: If the model doesn't support batching.\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s` or\n            inference time exceeds `inference_timeout_s` passed to `__init__`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side,\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    if not self.is_batching_supported:\n        raise PyTritonClientModelDoesntSupportBatchingError(\n            f\"Model {self.model_config.model_name} doesn't support batching - use infer_sample method instead\"\n        )\n\n    return self._infer(inputs or named_inputs, parameters, headers)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.infer_sample","title":"infer_sample","text":"<pre><code>infer_sample(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs) -&gt; Dict[str, ndarray]\n</code></pre> <p>Run synchronous inference on a single data sample.</p> <p>Typical usage:</p> <pre><code>client = ModelClient(\"localhost\", \"MyModel\")\nresult_dict = client.infer_sample(input1, input2)\nclient.close()\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = client.infer_sample(input1, input2)\nresult_dict = client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>Inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom inference headers.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>Inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, ndarray]</code>           \u2013            <p>Dictionary with inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientValueError</code>             \u2013            <p>If mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If the wait time for the server and model being ready exceeds <code>init_timeout_s</code> or inference request time exceeds <code>inference_timeout_s</code>.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If the model with the given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If an error occurred on the inference callable or Triton Inference Server side.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_sample(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run synchronous inference on a single data sample.\n\n    Typical usage:\n\n    ```python\n    client = ModelClient(\"localhost\", \"MyModel\")\n    result_dict = client.infer_sample(input1, input2)\n    client.close()\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    result_dict = client.infer_sample(input1, input2)\n    result_dict = client.infer_sample(a=input1, b=input2)\n    ```\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Custom inference parameters.\n        headers: Custom inference headers.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        Dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: If mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError: If the wait time for the server and model being ready exceeds `init_timeout_s` or\n            inference request time exceeds `inference_timeout_s`.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If an error occurred on the inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    if self.is_batching_supported:\n        if inputs:\n            inputs = tuple(data[np.newaxis, ...] for data in inputs)\n        elif named_inputs:\n            named_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\n\n    result = self._infer(inputs or named_inputs, parameters, headers)\n\n    return self._debatch_result(result)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.load_model","title":"load_model","text":"<pre><code>load_model(config: Optional[str] = None, files: Optional[dict] = None)\n</code></pre> <p>Load model on the Triton Inference Server.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>str - Optional JSON representation of a model config provided for the load request, if provided, this config will be used for loading the model.</p> </li> <li> <code>files</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>dict - Optional dictionary specifying file path (with \"file:\" prefix) in the override model directory to the file content as bytes. The files will form the model directory that the model will be loaded from. If specified, 'config' must be provided to be the model configuration of the override model directory.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def load_model(self, config: Optional[str] = None, files: Optional[dict] = None):\n    \"\"\"Load model on the Triton Inference Server.\n\n    Args:\n        config: str - Optional JSON representation of a model config provided for\n            the load request, if provided, this config will be used for\n            loading the model.\n        files: dict - Optional dictionary specifying file path (with \"file:\" prefix) in\n            the override model directory to the file content as bytes.\n            The files will form the model directory that the model will be\n            loaded from. If specified, 'config' must be provided to be\n            the model configuration of the override model directory.\n    \"\"\"\n    self._general_client.load_model(self._model_name, config=config, files=files)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.unload_model","title":"unload_model","text":"<pre><code>unload_model()\n</code></pre> <p>Unload model from the Triton Inference Server.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def unload_model(self):\n    \"\"\"Unload model from the Triton Inference Server.\"\"\"\n    self._general_client.unload_model(self._model_name)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.wait_for_model","title":"wait_for_model","text":"<pre><code>wait_for_model(timeout_s: float)\n</code></pre> <p>Wait for the Triton Inference Server and the deployed model to be ready.</p> <p>Parameters:</p> <ul> <li> <code>timeout_s</code>               (<code>float</code>)           \u2013            <p>timeout in seconds to wait for the server and model to be ready.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If the server and model are not ready before the given timeout.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If the model with the given name (and version) is unavailable.</p> </li> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If the hosting process receives SIGINT.</p> </li> <li> <code>PyTritonClientClosedError</code>             \u2013            <p>If the ModelClient is closed.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_model(self, timeout_s: float):\n    \"\"\"Wait for the Triton Inference Server and the deployed model to be ready.\n\n    Args:\n        timeout_s: timeout in seconds to wait for the server and model to be ready.\n\n    Raises:\n        PyTritonClientTimeoutError: If the server and model are not ready before the given timeout.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        KeyboardInterrupt: If the hosting process receives SIGINT.\n        PyTritonClientClosedError: If the ModelClient is closed.\n    \"\"\"\n    if self._general_client is None:\n        raise PyTritonClientClosedError(\"ModelClient is closed\")\n    wait_for_model_ready(self._general_client, self._model_name, self._model_version, timeout_s=timeout_s)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.ModelClient.wait_for_server","title":"wait_for_server","text":"<pre><code>wait_for_server(timeout_s: float)\n</code></pre> <p>Wait for Triton Inference Server readiness.</p> <p>Parameters:</p> <ul> <li> <code>timeout_s</code>               (<code>float</code>)           \u2013            <p>timeout to server get into readiness state.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If server is not in readiness state before given timeout.</p> </li> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If hosting process receives SIGINT</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_server(self, timeout_s: float):\n    \"\"\"Wait for Triton Inference Server readiness.\n\n    Args:\n        timeout_s: timeout to server get into readiness state.\n\n    Raises:\n        PyTritonClientTimeoutError: If server is not in readiness state before given timeout.\n        KeyboardInterrupt: If hosting process receives SIGINT\n    \"\"\"\n    wait_for_server_ready(self._general_client, timeout_s=timeout_s)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient","title":"pytriton.client.AsyncioModelClient","text":"<pre><code>AsyncioModelClient(url: str, model_name: str, model_version: Optional[str] = None, *, lazy_init: bool = True, init_timeout_s: Optional[float] = None, inference_timeout_s: Optional[float] = None, model_config: Optional[TritonModelConfig] = None, ensure_model_is_ready: bool = True)\n</code></pre> <p>               Bases: <code>BaseModelClient</code></p> <p>Asyncio client for model deployed on the Triton Inference Server.</p> This client is based on Triton Inference Server Python clients and GRPC library <ul> <li><code>tritonclient.http.aio.InferenceServerClient</code></li> <li><code>tritonclient.grpc.aio.InferenceServerClient</code></li> </ul> <p>It can wait for server to be ready with model loaded and then perform inference on it. <code>AsyncioModelClient</code> supports asyncio context manager protocol.</p> <p>Typical usage:</p> <pre><code>from pytriton.client import AsyncioModelClient\nimport numpy as np\n\ninput1_sample = np.random.rand(1, 3, 224, 224).astype(np.float32)\ninput2_sample = np.random.rand(1, 3, 224, 224).astype(np.float32)\n\nclient = AsyncioModelClient(\"localhost\", \"MyModel\")\nresult_dict = await client.infer_sample(input1_sample, input2_sample)\nprint(result_dict[\"output_name\"])\nawait client.close()\n</code></pre> <p>Inits ModelClient for given model deployed on the Triton Inference Server.</p> <p>If <code>lazy_init</code> argument is False, model configuration will be read from inference server during initialization.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The Triton Inference Server url, e.g. 'grpc://localhost:8001'. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>name of the model to interact with.</p> </li> <li> <code>model_version</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> </li> <li> <code>lazy_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if initialization should be performed just before sending first request to inference server.</p> </li> <li> <code>init_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout for server and model being ready.</p> </li> <li> <code>inference_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.</p> </li> <li> <code>model_config</code>               (<code>Optional[TritonModelConfig]</code>, default:                   <code>None</code> )           \u2013            <p>model configuration. If not passed, it will be read from inference server during initialization.</p> </li> <li> <code>ensure_model_is_ready</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if model should be checked if it is ready before first inference request.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> </li> <li> <code>PyTritonClientUrlParseError</code>             \u2013            <p>In case of problems with parsing url.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    lazy_init: bool = True,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n    model_config: Optional[TritonModelConfig] = None,\n    ensure_model_is_ready: bool = True,\n):\n    \"\"\"Inits ModelClient for given model deployed on the Triton Inference Server.\n\n    If `lazy_init` argument is False, model configuration will be read\n    from inference server during initialization.\n\n    Args:\n        url: The Triton Inference Server url, e.g. 'grpc://localhost:8001'.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout for server and model being ready.\n        inference_timeout_s: timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.\n        model_config: model configuration. If not passed, it will be read from inference server during initialization.\n        ensure_model_is_ready: if model should be checked if it is ready before first inference request.\n\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError: if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientUrlParseError: In case of problems with parsing url.\n    \"\"\"\n    super().__init__(\n        url=url,\n        model_name=model_name,\n        model_version=model_version,\n        lazy_init=lazy_init,\n        init_timeout_s=init_timeout_s,\n        inference_timeout_s=inference_timeout_s,\n        model_config=model_config,\n        ensure_model_is_ready=ensure_model_is_ready,\n    )\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.model_config","title":"model_config  <code>async</code> <code>property</code>","text":"<pre><code>model_config\n</code></pre> <p>Obtain configuration of model deployed on the Triton Inference Server.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__()\n</code></pre> <p>Create context for use AsyncioModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Create context for use AsyncioModelClient as a context manager.\"\"\"\n    _LOGGER.debug(\"Entering AsyncioModelClient context\")\n    try:\n        if not self._lazy_init:\n            _LOGGER.debug(\"Waiting in AsyncioModelClient context for model to be ready\")\n            await self._wait_and_init_model_config(self._init_timeout_s)\n            _LOGGER.debug(\"Model is ready in AsyncioModelClient context\")\n        return self\n    except Exception as e:\n        _LOGGER.error(\"Error occurred during AsyncioModelClient context initialization\")\n        await self.close()\n        raise e\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(*_)\n</code></pre> <p>Close resources used by AsyncioModelClient when exiting from context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def __aexit__(self, *_):\n    \"\"\"Close resources used by AsyncioModelClient when exiting from context.\"\"\"\n    await self.close()\n    _LOGGER.debug(\"Exiting AsyncioModelClient context\")\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> <p>Close resources used by _ModelClientBase.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def close(self):\n    \"\"\"Close resources used by _ModelClientBase.\"\"\"\n    _LOGGER.debug(\"Closing InferenceServerClient\")\n    await self._general_client.close()\n    await self._infer_client.close()\n    _LOGGER.debug(\"InferenceServerClient closed\")\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.create_client_from_url","title":"create_client_from_url","text":"<pre><code>create_client_from_url(url: str, network_timeout_s: Optional[float] = None)\n</code></pre> <p>Create Triton Inference Server client.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>url of the server to connect to. If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added. If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.</p> </li> <li> <code>network_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout for client commands. Default value is 60.0 s.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Triton Inference Server client.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientInvalidUrlError</code>             \u2013            <p>If provided Triton Inference Server url is invalid.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def create_client_from_url(self, url: str, network_timeout_s: Optional[float] = None):\n    \"\"\"Create Triton Inference Server client.\n\n    Args:\n        url: url of the server to connect to.\n            If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added.\n            If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.\n        network_timeout_s: timeout for client commands. Default value is 60.0 s.\n\n    Returns:\n        Triton Inference Server client.\n\n    Raises:\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\n    self._triton_url = TritonUrl.from_url(url)\n    self._url = self._triton_url.without_scheme\n    self._triton_client_lib = self.get_lib()\n    self._monkey_patch_client()\n\n    if self._triton_url.scheme == \"grpc\":\n        # by default grpc client has very large number of timeout, thus we want to make it equal to http client timeout\n        network_timeout_s = _DEFAULT_NETWORK_TIMEOUT_S if network_timeout_s is None else network_timeout_s\n        warnings.warn(\n            f\"tritonclient.grpc doesn't support timeout for other commands than infer. Ignoring network_timeout: {network_timeout_s}.\",\n            NotSupportedTimeoutWarning,\n            stacklevel=1,\n        )\n\n    triton_client_init_kwargs = self._get_init_extra_args()\n\n    _LOGGER.debug(\n        f\"Creating InferenceServerClient for {self._triton_url.with_scheme} with {triton_client_init_kwargs}\"\n    )\n    return self._triton_client_lib.InferenceServerClient(self._url, **triton_client_init_kwargs)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.from_existing_client","title":"from_existing_client  <code>classmethod</code>","text":"<pre><code>from_existing_client(existing_client: BaseModelClient)\n</code></pre> <p>Create a new instance from an existing client using the same class.</p> <p>Common usage: <pre><code>client = BaseModelClient.from_existing_client(existing_client)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>existing_client</code>               (<code>BaseModelClient</code>)           \u2013            <p>An instance of an already initialized subclass.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A new instance of the same subclass with shared configuration and readiness state.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>@classmethod\ndef from_existing_client(cls, existing_client: \"BaseModelClient\"):\n    \"\"\"Create a new instance from an existing client using the same class.\n\n    Common usage:\n    ```python\n    client = BaseModelClient.from_existing_client(existing_client)\n    ```\n\n    Args:\n        existing_client: An instance of an already initialized subclass.\n\n    Returns:\n        A new instance of the same subclass with shared configuration and readiness state.\n    \"\"\"\n    kwargs = {}\n    # Copy model configuration and readiness state if present\n    if hasattr(existing_client, \"_model_config\"):\n        kwargs[\"model_config\"] = existing_client._model_config\n        kwargs[\"ensure_model_is_ready\"] = False\n\n    new_client = cls(\n        url=existing_client._url,\n        model_name=existing_client._model_name,\n        model_version=existing_client._model_version,\n        init_timeout_s=existing_client._init_timeout_s,\n        inference_timeout_s=existing_client._inference_timeout_s,\n        **kwargs,\n    )\n\n    return new_client\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.get_lib","title":"get_lib","text":"<pre><code>get_lib()\n</code></pre> <p>Get Triton Inference Server Python client library.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n    \"\"\"Get Triton Inference Server Python client library.\"\"\"\n    return {\"grpc\": tritonclient.grpc.aio, \"http\": tritonclient.http.aio}[self._triton_url.scheme.lower()]\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.infer_batch","title":"infer_batch  <code>async</code>","text":"<pre><code>infer_batch(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs)\n</code></pre> <p>Run asynchronous inference on batched data.</p> <p>Typical usage:</p> <pre><code>client = AsyncioModelClient(\"localhost\", \"MyModel\")\nresult_dict = await client.infer_batch(input1, input2)\nawait client.close()\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = await client.infer_batch(input1, input2)\nresult_dict = await client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>custom inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>custom inference headers.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>dictionary with inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientValueError</code>             \u2013            <p>if mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>timeout_s</code>.</p> </li> <li> <code>PyTritonClientModelDoesntSupportBatchingError</code>             \u2013            <p>if model doesn't support batching.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If error occurred on inference callable or Triton Inference Server side.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>async def infer_batch(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n):\n    \"\"\"Run asynchronous inference on batched data.\n\n    Typical usage:\n\n    ```python\n    client = AsyncioModelClient(\"localhost\", \"MyModel\")\n    result_dict = await client.infer_batch(input1, input2)\n    await client.close()\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    result_dict = await client.infer_batch(input1, input2)\n    result_dict = await client.infer_batch(a=input1, b=input2)\n    ```\n\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.\n\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        parameters: custom inference parameters.\n        headers: custom inference headers.\n        **named_inputs: inference inputs provided as named arguments.\n\n    Returns:\n        dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelDoesntSupportBatchingError: if model doesn't support batching.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    _LOGGER.debug(f\"Running inference for {self._model_name}\")\n    model_config = await self.model_config\n    _LOGGER.debug(f\"Model config for {self._model_name} obtained\")\n\n    model_supports_batching = model_config.max_batch_size &gt; 0\n    if not model_supports_batching:\n        _LOGGER.error(f\"Model {model_config.model_name} doesn't support batching\")\n        raise PyTritonClientModelDoesntSupportBatchingError(\n            f\"Model {model_config.model_name} doesn't support batching - use infer_sample method instead\"\n        )\n\n    _LOGGER.debug(f\"Running _infer for {self._model_name}\")\n    result = await self._infer(inputs or named_inputs, parameters, headers)\n    _LOGGER.debug(f\"_infer for {self._model_name} finished\")\n    return result\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.infer_sample","title":"infer_sample  <code>async</code>","text":"<pre><code>infer_sample(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs)\n</code></pre> <p>Run asynchronous inference on single data sample.</p> <p>Typical usage:</p> <pre><code>client = AsyncioModelClient(\"localhost\", \"MyModel\")\nresult_dict = await client.infer_sample(input1, input2)\nawait client.close()\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = await client.infer_sample(input1, input2)\nresult_dict = await client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>custom inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>custom inference headers.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>dictionary with inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientValueError</code>             \u2013            <p>if mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>timeout_s</code>.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If error occurred on inference callable or Triton Inference Server side.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>async def infer_sample(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n):\n    \"\"\"Run asynchronous inference on single data sample.\n\n    Typical usage:\n\n    ```python\n    client = AsyncioModelClient(\"localhost\", \"MyModel\")\n    result_dict = await client.infer_sample(input1, input2)\n    await client.close()\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    result_dict = await client.infer_sample(input1, input2)\n    result_dict = await client.infer_sample(a=input1, b=input2)\n    ```\n\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.\n\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        parameters: custom inference parameters.\n        headers: custom inference headers.\n        **named_inputs: inference inputs provided as named arguments.\n\n    Returns:\n        dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    _LOGGER.debug(f\"Running inference for {self._model_name}\")\n    model_config = await self.model_config\n    _LOGGER.debug(f\"Model config for {self._model_name} obtained\")\n\n    model_supports_batching = model_config.max_batch_size &gt; 0\n    if model_supports_batching:\n        if inputs:\n            inputs = tuple(data[np.newaxis, ...] for data in inputs)\n        elif named_inputs:\n            named_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\n\n    _LOGGER.debug(f\"Running _infer for {self._model_name}\")\n    result = await self._infer(inputs or named_inputs, parameters, headers)\n    _LOGGER.debug(f\"_infer for {self._model_name} finished\")\n    if model_supports_batching:\n        result = {name: data[0] for name, data in result.items()}\n\n    return result\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioModelClient.wait_for_model","title":"wait_for_model  <code>async</code>","text":"<pre><code>wait_for_model(timeout_s: float)\n</code></pre> <p>Asynchronous wait for Triton Inference Server and deployed on it model readiness.</p> <p>Parameters:</p> <ul> <li> <code>timeout_s</code>               (<code>float</code>)           \u2013            <p>timeout to server and model get into readiness state.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If server and model are not in readiness state before given timeout.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If hosting process receives SIGINT</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>async def wait_for_model(self, timeout_s: float):\n    \"\"\"Asynchronous wait for Triton Inference Server and deployed on it model readiness.\n\n    Args:\n        timeout_s: timeout to server and model get into readiness state.\n\n    Raises:\n        PyTritonClientTimeoutError: If server and model are not in readiness state before given timeout.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        KeyboardInterrupt: If hosting process receives SIGINT\n    \"\"\"\n    _LOGGER.debug(f\"Waiting for model {self._model_name} to be ready\")\n    try:\n        await asyncio.wait_for(\n            asyncio_wait_for_model_ready(\n                self._general_client, self._model_name, self._model_version, timeout_s=timeout_s\n            ),\n            self._init_timeout_s,\n        )\n    except asyncio.TimeoutError as e:\n        message = f\"Timeout while waiting for model {self._model_name} to be ready for {self._init_timeout_s}s\"\n        _LOGGER.error(message)\n        raise PyTritonClientTimeoutError(message) from e\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient","title":"pytriton.client.DecoupledModelClient","text":"<pre><code>DecoupledModelClient(url: str, model_name: str, model_version: Optional[str] = None, *, lazy_init: bool = True, init_timeout_s: Optional[float] = None, inference_timeout_s: Optional[float] = None, model_config: Optional[TritonModelConfig] = None, ensure_model_is_ready: bool = True)\n</code></pre> <p>               Bases: <code>ModelClient</code></p> <p>Synchronous client for decoupled model deployed on the Triton Inference Server.</p> <p>Inits DecoupledModelClient for given decoupled model deployed on the Triton Inference Server.</p> <p>Common usage:</p> <pre><code>client = DecoupledModelClient(\"localhost\", \"BERT\")\nfor response in client.infer_sample(input1_sample, input2_sample):\n    print(response)\nclient.close()\n</code></pre> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The Triton Inference Server url, e.g. <code>grpc://localhost:8001</code>. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>name of the model to interact with.</p> </li> <li> <code>model_version</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> </li> <li> <code>lazy_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if initialization should be performed just before sending first request to inference server.</p> </li> <li> <code>init_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout in seconds for the server and model to be ready. If not passed, the default timeout of 300 seconds will be used.</p> </li> <li> <code>inference_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.</p> </li> <li> <code>model_config</code>               (<code>Optional[TritonModelConfig]</code>, default:                   <code>None</code> )           \u2013            <p>model configuration. If not passed, it will be read from inference server during initialization.</p> </li> <li> <code>ensure_model_is_ready</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if model should be checked if it is ready before first inference request.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> </li> <li> <code>PyTritonClientInvalidUrlError</code>             \u2013            <p>If provided Triton Inference Server url is invalid.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    lazy_init: bool = True,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n    model_config: Optional[TritonModelConfig] = None,\n    ensure_model_is_ready: bool = True,\n):\n    \"\"\"Inits DecoupledModelClient for given decoupled model deployed on the Triton Inference Server.\n\n    Common usage:\n\n    ```python\n    client = DecoupledModelClient(\"localhost\", \"BERT\")\n    for response in client.infer_sample(input1_sample, input2_sample):\n        print(response)\n    client.close()\n    ```\n\n    Args:\n        url: The Triton Inference Server url, e.g. `grpc://localhost:8001`.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout in seconds for the server and model to be ready. If not passed, the default timeout of 300 seconds will be used.\n        inference_timeout_s: timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.\n        model_config: model configuration. If not passed, it will be read from inference server during initialization.\n        ensure_model_is_ready: if model should be checked if it is ready before first inference request.\n\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError:\n            if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\n    super().__init__(\n        url,\n        model_name,\n        model_version,\n        lazy_init=lazy_init,\n        init_timeout_s=init_timeout_s,\n        inference_timeout_s=inference_timeout_s,\n        model_config=model_config,\n        ensure_model_is_ready=ensure_model_is_ready,\n    )\n    if self._triton_url.scheme == \"http\":\n        raise PyTritonClientValueError(\"DecoupledModelClient is only supported for grpc protocol\")\n    self._queue = Queue()\n    self._lock = Lock()\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.is_batching_supported","title":"is_batching_supported  <code>property</code>","text":"<pre><code>is_batching_supported\n</code></pre> <p>Checks if model supports batching.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.model_config","title":"model_config  <code>property</code>","text":"<pre><code>model_config: TritonModelConfig\n</code></pre> <p>Obtain the configuration of the model deployed on the Triton Inference Server.</p> <p>This method waits for the server to get into readiness state before obtaining the model configuration.</p> <p>Returns:</p> <ul> <li> <code>TritonModelConfig</code> (              <code>TritonModelConfig</code> )          \u2013            <p>configuration of the model deployed on the Triton Inference Server.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If the server and model are not in readiness state before the given timeout.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If the model with the given name (and version) is unavailable.</p> </li> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If the hosting process receives SIGINT.</p> </li> <li> <code>PyTritonClientClosedError</code>             \u2013            <p>If the ModelClient is closed.</p> </li> </ul>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Create context for using ModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __enter__(self):\n    \"\"\"Create context for using ModelClient as a context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.__exit__","title":"__exit__","text":"<pre><code>__exit__(*_)\n</code></pre> <p>Close resources used by ModelClient instance when exiting from the context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __exit__(self, *_):\n    \"\"\"Close resources used by ModelClient instance when exiting from the context.\"\"\"\n    self.close()\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close resources used by DecoupledModelClient.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def close(self):\n    \"\"\"Close resources used by DecoupledModelClient.\"\"\"\n    _LOGGER.debug(\"Closing DecoupledModelClient\")\n    if self._lock.acquire(blocking=False):\n        try:\n            super().close()\n        finally:\n            self._lock.release()\n    else:\n        _LOGGER.warning(\"DecoupledModelClient is stil streaming answers\")\n        self._infer_client.stop_stream(False)\n        super().close()\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.create_client_from_url","title":"create_client_from_url","text":"<pre><code>create_client_from_url(url: str, network_timeout_s: Optional[float] = None)\n</code></pre> <p>Create Triton Inference Server client.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>url of the server to connect to. If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added. If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.</p> </li> <li> <code>network_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout for client commands. Default value is 60.0 s.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Triton Inference Server client.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientInvalidUrlError</code>             \u2013            <p>If provided Triton Inference Server url is invalid.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def create_client_from_url(self, url: str, network_timeout_s: Optional[float] = None):\n    \"\"\"Create Triton Inference Server client.\n\n    Args:\n        url: url of the server to connect to.\n            If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added.\n            If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.\n        network_timeout_s: timeout for client commands. Default value is 60.0 s.\n\n    Returns:\n        Triton Inference Server client.\n\n    Raises:\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\n    self._triton_url = TritonUrl.from_url(url)\n    self._url = self._triton_url.without_scheme\n    self._triton_client_lib = self.get_lib()\n    self._monkey_patch_client()\n\n    if self._triton_url.scheme == \"grpc\":\n        # by default grpc client has very large number of timeout, thus we want to make it equal to http client timeout\n        network_timeout_s = _DEFAULT_NETWORK_TIMEOUT_S if network_timeout_s is None else network_timeout_s\n        warnings.warn(\n            f\"tritonclient.grpc doesn't support timeout for other commands than infer. Ignoring network_timeout: {network_timeout_s}.\",\n            NotSupportedTimeoutWarning,\n            stacklevel=1,\n        )\n\n    triton_client_init_kwargs = self._get_init_extra_args()\n\n    _LOGGER.debug(\n        f\"Creating InferenceServerClient for {self._triton_url.with_scheme} with {triton_client_init_kwargs}\"\n    )\n    return self._triton_client_lib.InferenceServerClient(self._url, **triton_client_init_kwargs)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.from_existing_client","title":"from_existing_client  <code>classmethod</code>","text":"<pre><code>from_existing_client(existing_client: BaseModelClient)\n</code></pre> <p>Create a new instance from an existing client using the same class.</p> <p>Common usage: <pre><code>client = BaseModelClient.from_existing_client(existing_client)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>existing_client</code>               (<code>BaseModelClient</code>)           \u2013            <p>An instance of an already initialized subclass.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A new instance of the same subclass with shared configuration and readiness state.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>@classmethod\ndef from_existing_client(cls, existing_client: \"BaseModelClient\"):\n    \"\"\"Create a new instance from an existing client using the same class.\n\n    Common usage:\n    ```python\n    client = BaseModelClient.from_existing_client(existing_client)\n    ```\n\n    Args:\n        existing_client: An instance of an already initialized subclass.\n\n    Returns:\n        A new instance of the same subclass with shared configuration and readiness state.\n    \"\"\"\n    kwargs = {}\n    # Copy model configuration and readiness state if present\n    if hasattr(existing_client, \"_model_config\"):\n        kwargs[\"model_config\"] = existing_client._model_config\n        kwargs[\"ensure_model_is_ready\"] = False\n\n    new_client = cls(\n        url=existing_client._url,\n        model_name=existing_client._model_name,\n        model_version=existing_client._model_version,\n        init_timeout_s=existing_client._init_timeout_s,\n        inference_timeout_s=existing_client._inference_timeout_s,\n        **kwargs,\n    )\n\n    return new_client\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.get_lib","title":"get_lib","text":"<pre><code>get_lib()\n</code></pre> <p>Returns tritonclient library for given scheme.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n    \"\"\"Returns tritonclient library for given scheme.\"\"\"\n    return {\"grpc\": tritonclient.grpc, \"http\": tritonclient.http}[self._triton_url.scheme.lower()]\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.infer_batch","title":"infer_batch","text":"<pre><code>infer_batch(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs) -&gt; Dict[str, ndarray]\n</code></pre> <p>Run synchronous inference on batched data.</p> <p>Typical usage:</p> <pre><code>client = ModelClient(\"localhost\", \"MyModel\")\nresult_dict = client.infer_batch(input1, input2)\nclient.close()\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = client.infer_batch(input1, input2)\nresult_dict = client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>Inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom inference headers.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>Inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, ndarray]</code>           \u2013            <p>Dictionary with inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientValueError</code>             \u2013            <p>If mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If the wait time for the server and model being ready exceeds <code>init_timeout_s</code> or inference request time exceeds <code>inference_timeout_s</code>.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If the model with the given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If an error occurred on the inference callable or Triton Inference Server side.</p> </li> <li> <code>PyTritonClientModelDoesntSupportBatchingError</code>             \u2013            <p>If the model doesn't support batching.</p> </li> <li> <code>PyTritonClientValueError</code>             \u2013            <p>if mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>inference_timeout_s</code> passed to <code>__init__</code>.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If error occurred on inference callable or Triton Inference Server side,</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_batch(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run synchronous inference on batched data.\n\n    Typical usage:\n\n    ```python\n    client = ModelClient(\"localhost\", \"MyModel\")\n    result_dict = client.infer_batch(input1, input2)\n    client.close()\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    result_dict = client.infer_batch(input1, input2)\n    result_dict = client.infer_batch(a=input1, b=input2)\n    ```\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Custom inference parameters.\n        headers: Custom inference headers.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        Dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: If mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError: If the wait time for the server and model being ready exceeds `init_timeout_s` or\n            inference request time exceeds `inference_timeout_s`.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If an error occurred on the inference callable or Triton Inference Server side.\n        PyTritonClientModelDoesntSupportBatchingError: If the model doesn't support batching.\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s` or\n            inference time exceeds `inference_timeout_s` passed to `__init__`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side,\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    if not self.is_batching_supported:\n        raise PyTritonClientModelDoesntSupportBatchingError(\n            f\"Model {self.model_config.model_name} doesn't support batching - use infer_sample method instead\"\n        )\n\n    return self._infer(inputs or named_inputs, parameters, headers)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.infer_sample","title":"infer_sample","text":"<pre><code>infer_sample(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs) -&gt; Dict[str, ndarray]\n</code></pre> <p>Run synchronous inference on a single data sample.</p> <p>Typical usage:</p> <pre><code>client = ModelClient(\"localhost\", \"MyModel\")\nresult_dict = client.infer_sample(input1, input2)\nclient.close()\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = client.infer_sample(input1, input2)\nresult_dict = client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>Inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom inference headers.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>Inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, ndarray]</code>           \u2013            <p>Dictionary with inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientValueError</code>             \u2013            <p>If mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If the wait time for the server and model being ready exceeds <code>init_timeout_s</code> or inference request time exceeds <code>inference_timeout_s</code>.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If the model with the given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If an error occurred on the inference callable or Triton Inference Server side.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_sample(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run synchronous inference on a single data sample.\n\n    Typical usage:\n\n    ```python\n    client = ModelClient(\"localhost\", \"MyModel\")\n    result_dict = client.infer_sample(input1, input2)\n    client.close()\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    result_dict = client.infer_sample(input1, input2)\n    result_dict = client.infer_sample(a=input1, b=input2)\n    ```\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Custom inference parameters.\n        headers: Custom inference headers.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        Dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: If mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError: If the wait time for the server and model being ready exceeds `init_timeout_s` or\n            inference request time exceeds `inference_timeout_s`.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If an error occurred on the inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    if self.is_batching_supported:\n        if inputs:\n            inputs = tuple(data[np.newaxis, ...] for data in inputs)\n        elif named_inputs:\n            named_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\n\n    result = self._infer(inputs or named_inputs, parameters, headers)\n\n    return self._debatch_result(result)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.load_model","title":"load_model","text":"<pre><code>load_model(config: Optional[str] = None, files: Optional[dict] = None)\n</code></pre> <p>Load model on the Triton Inference Server.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>str - Optional JSON representation of a model config provided for the load request, if provided, this config will be used for loading the model.</p> </li> <li> <code>files</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>dict - Optional dictionary specifying file path (with \"file:\" prefix) in the override model directory to the file content as bytes. The files will form the model directory that the model will be loaded from. If specified, 'config' must be provided to be the model configuration of the override model directory.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def load_model(self, config: Optional[str] = None, files: Optional[dict] = None):\n    \"\"\"Load model on the Triton Inference Server.\n\n    Args:\n        config: str - Optional JSON representation of a model config provided for\n            the load request, if provided, this config will be used for\n            loading the model.\n        files: dict - Optional dictionary specifying file path (with \"file:\" prefix) in\n            the override model directory to the file content as bytes.\n            The files will form the model directory that the model will be\n            loaded from. If specified, 'config' must be provided to be\n            the model configuration of the override model directory.\n    \"\"\"\n    self._general_client.load_model(self._model_name, config=config, files=files)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.unload_model","title":"unload_model","text":"<pre><code>unload_model()\n</code></pre> <p>Unload model from the Triton Inference Server.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def unload_model(self):\n    \"\"\"Unload model from the Triton Inference Server.\"\"\"\n    self._general_client.unload_model(self._model_name)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.wait_for_model","title":"wait_for_model","text":"<pre><code>wait_for_model(timeout_s: float)\n</code></pre> <p>Wait for the Triton Inference Server and the deployed model to be ready.</p> <p>Parameters:</p> <ul> <li> <code>timeout_s</code>               (<code>float</code>)           \u2013            <p>timeout in seconds to wait for the server and model to be ready.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If the server and model are not ready before the given timeout.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If the model with the given name (and version) is unavailable.</p> </li> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If the hosting process receives SIGINT.</p> </li> <li> <code>PyTritonClientClosedError</code>             \u2013            <p>If the ModelClient is closed.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_model(self, timeout_s: float):\n    \"\"\"Wait for the Triton Inference Server and the deployed model to be ready.\n\n    Args:\n        timeout_s: timeout in seconds to wait for the server and model to be ready.\n\n    Raises:\n        PyTritonClientTimeoutError: If the server and model are not ready before the given timeout.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        KeyboardInterrupt: If the hosting process receives SIGINT.\n        PyTritonClientClosedError: If the ModelClient is closed.\n    \"\"\"\n    if self._general_client is None:\n        raise PyTritonClientClosedError(\"ModelClient is closed\")\n    wait_for_model_ready(self._general_client, self._model_name, self._model_version, timeout_s=timeout_s)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.DecoupledModelClient.wait_for_server","title":"wait_for_server","text":"<pre><code>wait_for_server(timeout_s: float)\n</code></pre> <p>Wait for Triton Inference Server readiness.</p> <p>Parameters:</p> <ul> <li> <code>timeout_s</code>               (<code>float</code>)           \u2013            <p>timeout to server get into readiness state.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If server is not in readiness state before given timeout.</p> </li> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If hosting process receives SIGINT</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_server(self, timeout_s: float):\n    \"\"\"Wait for Triton Inference Server readiness.\n\n    Args:\n        timeout_s: timeout to server get into readiness state.\n\n    Raises:\n        PyTritonClientTimeoutError: If server is not in readiness state before given timeout.\n        KeyboardInterrupt: If hosting process receives SIGINT\n    \"\"\"\n    wait_for_server_ready(self._general_client, timeout_s=timeout_s)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient","title":"pytriton.client.AsyncioDecoupledModelClient","text":"<pre><code>AsyncioDecoupledModelClient(url: str, model_name: str, model_version: Optional[str] = None, *, lazy_init: bool = True, init_timeout_s: Optional[float] = None, inference_timeout_s: Optional[float] = None, model_config: Optional[TritonModelConfig] = None, ensure_model_is_ready: bool = True)\n</code></pre> <p>               Bases: <code>AsyncioModelClient</code></p> <p>Asyncio client for model deployed on the Triton Inference Server.</p> <p>This client is based on Triton Inference Server Python clients and GRPC library: * <code>tritonclient.grpc.aio.InferenceServerClient</code></p> <p>It can wait for server to be ready with model loaded and then perform inference on it. <code>AsyncioDecoupledModelClient</code> supports asyncio context manager protocol.</p> <p>The client is intended to be used with decoupled models and will raise an error if model is coupled.</p> <p>Typical usage: <pre><code>from pytriton.client import AsyncioDecoupledModelClient\nimport numpy as np\n\ninput1_sample = np.random.rand(1, 3, 224, 224).astype(np.float32)\ninput2_sample = np.random.rand(1, 3, 224, 224).astype(np.float32)\n\nasync with AsyncioDecoupledModelClient(\"grpc://localhost\", \"MyModel\") as client:\n    async for result_dict in client.infer_sample(input1_sample, input2_sample):\n        print(result_dict[\"output_name\"])\n</code></pre></p> <p>Inits ModelClient for given model deployed on the Triton Inference Server.</p> <p>If <code>lazy_init</code> argument is False, model configuration will be read from inference server during initialization.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The Triton Inference Server url, e.g. 'grpc://localhost:8001'. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>name of the model to interact with.</p> </li> <li> <code>model_version</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> </li> <li> <code>lazy_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if initialization should be performed just before sending first request to inference server.</p> </li> <li> <code>init_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout for server and model being ready.</p> </li> <li> <code>inference_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.</p> </li> <li> <code>model_config</code>               (<code>Optional[TritonModelConfig]</code>, default:                   <code>None</code> )           \u2013            <p>model configuration. If not passed, it will be read from inference server during initialization.</p> </li> <li> <code>ensure_model_is_ready</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if model should be checked if it is ready before first inference request.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> </li> <li> <code>PyTritonClientUrlParseError</code>             \u2013            <p>In case of problems with parsing url.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    lazy_init: bool = True,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n    model_config: Optional[TritonModelConfig] = None,\n    ensure_model_is_ready: bool = True,\n):\n    \"\"\"Inits ModelClient for given model deployed on the Triton Inference Server.\n\n    If `lazy_init` argument is False, model configuration will be read\n    from inference server during initialization.\n\n    Args:\n        url: The Triton Inference Server url, e.g. 'grpc://localhost:8001'.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout for server and model being ready.\n        inference_timeout_s: timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.\n        model_config: model configuration. If not passed, it will be read from inference server during initialization.\n        ensure_model_is_ready: if model should be checked if it is ready before first inference request.\n\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError: if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientUrlParseError: In case of problems with parsing url.\n    \"\"\"\n    super().__init__(\n        url=url,\n        model_name=model_name,\n        model_version=model_version,\n        lazy_init=lazy_init,\n        init_timeout_s=init_timeout_s,\n        inference_timeout_s=inference_timeout_s,\n        model_config=model_config,\n        ensure_model_is_ready=ensure_model_is_ready,\n    )\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.model_config","title":"model_config  <code>async</code> <code>property</code>","text":"<pre><code>model_config\n</code></pre> <p>Obtain configuration of model deployed on the Triton Inference Server.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__()\n</code></pre> <p>Create context for use AsyncioModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Create context for use AsyncioModelClient as a context manager.\"\"\"\n    _LOGGER.debug(\"Entering AsyncioModelClient context\")\n    try:\n        if not self._lazy_init:\n            _LOGGER.debug(\"Waiting in AsyncioModelClient context for model to be ready\")\n            await self._wait_and_init_model_config(self._init_timeout_s)\n            _LOGGER.debug(\"Model is ready in AsyncioModelClient context\")\n        return self\n    except Exception as e:\n        _LOGGER.error(\"Error occurred during AsyncioModelClient context initialization\")\n        await self.close()\n        raise e\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(*_)\n</code></pre> <p>Close resources used by AsyncioModelClient when exiting from context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def __aexit__(self, *_):\n    \"\"\"Close resources used by AsyncioModelClient when exiting from context.\"\"\"\n    await self.close()\n    _LOGGER.debug(\"Exiting AsyncioModelClient context\")\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> <p>Close resources used by _ModelClientBase.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def close(self):\n    \"\"\"Close resources used by _ModelClientBase.\"\"\"\n    _LOGGER.debug(\"Closing InferenceServerClient\")\n    await self._general_client.close()\n    await self._infer_client.close()\n    _LOGGER.debug(\"InferenceServerClient closed\")\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.create_client_from_url","title":"create_client_from_url","text":"<pre><code>create_client_from_url(url: str, network_timeout_s: Optional[float] = None)\n</code></pre> <p>Create Triton Inference Server client.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>url of the server to connect to. If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added. If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.</p> </li> <li> <code>network_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>timeout for client commands. Default value is 60.0 s.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Triton Inference Server client.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientInvalidUrlError</code>             \u2013            <p>If provided Triton Inference Server url is invalid.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def create_client_from_url(self, url: str, network_timeout_s: Optional[float] = None):\n    \"\"\"Create Triton Inference Server client.\n\n    Args:\n        url: url of the server to connect to.\n            If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added.\n            If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.\n        network_timeout_s: timeout for client commands. Default value is 60.0 s.\n\n    Returns:\n        Triton Inference Server client.\n\n    Raises:\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\n    self._triton_url = TritonUrl.from_url(url)\n    self._url = self._triton_url.without_scheme\n    self._triton_client_lib = self.get_lib()\n    self._monkey_patch_client()\n\n    if self._triton_url.scheme == \"grpc\":\n        # by default grpc client has very large number of timeout, thus we want to make it equal to http client timeout\n        network_timeout_s = _DEFAULT_NETWORK_TIMEOUT_S if network_timeout_s is None else network_timeout_s\n        warnings.warn(\n            f\"tritonclient.grpc doesn't support timeout for other commands than infer. Ignoring network_timeout: {network_timeout_s}.\",\n            NotSupportedTimeoutWarning,\n            stacklevel=1,\n        )\n\n    triton_client_init_kwargs = self._get_init_extra_args()\n\n    _LOGGER.debug(\n        f\"Creating InferenceServerClient for {self._triton_url.with_scheme} with {triton_client_init_kwargs}\"\n    )\n    return self._triton_client_lib.InferenceServerClient(self._url, **triton_client_init_kwargs)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.from_existing_client","title":"from_existing_client  <code>classmethod</code>","text":"<pre><code>from_existing_client(existing_client: BaseModelClient)\n</code></pre> <p>Create a new instance from an existing client using the same class.</p> <p>Common usage: <pre><code>client = BaseModelClient.from_existing_client(existing_client)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>existing_client</code>               (<code>BaseModelClient</code>)           \u2013            <p>An instance of an already initialized subclass.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A new instance of the same subclass with shared configuration and readiness state.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>@classmethod\ndef from_existing_client(cls, existing_client: \"BaseModelClient\"):\n    \"\"\"Create a new instance from an existing client using the same class.\n\n    Common usage:\n    ```python\n    client = BaseModelClient.from_existing_client(existing_client)\n    ```\n\n    Args:\n        existing_client: An instance of an already initialized subclass.\n\n    Returns:\n        A new instance of the same subclass with shared configuration and readiness state.\n    \"\"\"\n    kwargs = {}\n    # Copy model configuration and readiness state if present\n    if hasattr(existing_client, \"_model_config\"):\n        kwargs[\"model_config\"] = existing_client._model_config\n        kwargs[\"ensure_model_is_ready\"] = False\n\n    new_client = cls(\n        url=existing_client._url,\n        model_name=existing_client._model_name,\n        model_version=existing_client._model_version,\n        init_timeout_s=existing_client._init_timeout_s,\n        inference_timeout_s=existing_client._inference_timeout_s,\n        **kwargs,\n    )\n\n    return new_client\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.get_lib","title":"get_lib","text":"<pre><code>get_lib()\n</code></pre> <p>Get Triton Inference Server Python client library.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n    \"\"\"Get Triton Inference Server Python client library.\"\"\"\n    return {\"grpc\": tritonclient.grpc.aio, \"http\": tritonclient.http.aio}[self._triton_url.scheme.lower()]\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.infer_batch","title":"infer_batch  <code>async</code>","text":"<pre><code>infer_batch(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs)\n</code></pre> <p>Run asynchronous inference on batched data.</p> <p>Typical usage:</p> <pre><code>async with AsyncioDecoupledModelClient(\"grpc://localhost\", \"MyModel\") as client:\n    async for result_dict in client.infer_batch(input1_sample, input2_sample):\n        print(result_dict[\"output_name\"])\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>results_iterator = client.infer_batch(input1, input2)\nresults_iterator = client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>custom inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>custom inference headers.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Asynchronous generator, which generates dictionaries with partial inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientValueError</code>             \u2013            <p>if mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>timeout_s</code>.</p> </li> <li> <code>PyTritonClientModelDoesntSupportBatchingError</code>             \u2013            <p>if model doesn't support batching.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If error occurred on inference callable or Triton Inference Server side.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>async def infer_batch(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n):\n    \"\"\"Run asynchronous inference on batched data.\n\n    Typical usage:\n\n    ```python\n    async with AsyncioDecoupledModelClient(\"grpc://localhost\", \"MyModel\") as client:\n        async for result_dict in client.infer_batch(input1_sample, input2_sample):\n            print(result_dict[\"output_name\"])\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    results_iterator = client.infer_batch(input1, input2)\n    results_iterator = client.infer_batch(a=input1, b=input2)\n    ```\n\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.\n\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        parameters: custom inference parameters.\n        headers: custom inference headers.\n        **named_inputs: inference inputs provided as named arguments.\n\n    Returns:\n        Asynchronous generator, which generates dictionaries with partial inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelDoesntSupportBatchingError: if model doesn't support batching.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    _LOGGER.debug(f\"Running inference for {self._model_name}\")\n    model_config = await self.model_config\n    _LOGGER.debug(f\"Model config for {self._model_name} obtained\")\n\n    model_supports_batching = model_config.max_batch_size &gt; 0\n    if not model_supports_batching:\n        _LOGGER.error(f\"Model {model_config.model_name} doesn't support batching\")\n        raise PyTritonClientModelDoesntSupportBatchingError(\n            f\"Model {model_config.model_name} doesn't support batching - use infer_sample method instead\"\n        )\n\n    _LOGGER.debug(f\"Running _infer for {self._model_name}\")\n    result = self._infer(inputs or named_inputs, parameters, headers)\n    _LOGGER.debug(f\"_infer for {self._model_name} finished\")\n    async for item in result:\n        yield item\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.infer_sample","title":"infer_sample  <code>async</code>","text":"<pre><code>infer_sample(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs)\n</code></pre> <p>Run asynchronous inference on single data sample.</p> <p>Typical usage:</p> <pre><code>async with AsyncioDecoupledModelClient(\"grpc://localhost\", \"MyModel\") as client:\n    async for result_dict in client.infer_sample(input1_sample, input2_sample):\n        print(result_dict[\"output_name\"])\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>results_iterator = client.infer_sample(input1, input2)\nresults_iterator = client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>custom inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>custom inference headers.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Asynchronous generator, which generates dictionaries with partial inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientValueError</code>             \u2013            <p>if mixing of positional and named arguments passing detected.</p> </li> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>timeout_s</code>.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>PyTritonClientInferenceServerError</code>             \u2013            <p>If error occurred on inference callable or Triton Inference Server side.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>async def infer_sample(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n):\n    \"\"\"Run asynchronous inference on single data sample.\n\n    Typical usage:\n\n    ```python\n    async with AsyncioDecoupledModelClient(\"grpc://localhost\", \"MyModel\") as client:\n        async for result_dict in client.infer_sample(input1_sample, input2_sample):\n            print(result_dict[\"output_name\"])\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    results_iterator = client.infer_sample(input1, input2)\n    results_iterator = client.infer_sample(a=input1, b=input2)\n    ```\n\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.\n\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        parameters: custom inference parameters.\n        headers: custom inference headers.\n        **named_inputs: inference inputs provided as named arguments.\n\n    Returns:\n        Asynchronous generator, which generates dictionaries with partial inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    _LOGGER.debug(f\"Running inference for {self._model_name}\")\n    model_config = await self.model_config\n    _LOGGER.debug(f\"Model config for {self._model_name} obtained\")\n\n    model_supports_batching = model_config.max_batch_size &gt; 0\n    if model_supports_batching:\n        if inputs:\n            inputs = tuple(data[np.newaxis, ...] for data in inputs)\n        elif named_inputs:\n            named_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\n\n    _LOGGER.debug(f\"Running _infer for {self._model_name}\")\n    result = self._infer(inputs or named_inputs, parameters, headers)\n    _LOGGER.debug(f\"_infer for {self._model_name} finished\")\n\n    async for item in result:\n        if model_supports_batching:\n            debatched_item = {name: data[0] for name, data in item.items()}\n            yield debatched_item\n        else:\n            yield item\n</code></pre>"},{"location":"reference/clients/#pytriton.client.AsyncioDecoupledModelClient.wait_for_model","title":"wait_for_model  <code>async</code>","text":"<pre><code>wait_for_model(timeout_s: float)\n</code></pre> <p>Asynchronous wait for Triton Inference Server and deployed on it model readiness.</p> <p>Parameters:</p> <ul> <li> <code>timeout_s</code>               (<code>float</code>)           \u2013            <p>timeout to server and model get into readiness state.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientTimeoutError</code>             \u2013            <p>If server and model are not in readiness state before given timeout.</p> </li> <li> <code>PyTritonClientModelUnavailableError</code>             \u2013            <p>If model with given name (and version) is unavailable.</p> </li> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If hosting process receives SIGINT</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>async def wait_for_model(self, timeout_s: float):\n    \"\"\"Asynchronous wait for Triton Inference Server and deployed on it model readiness.\n\n    Args:\n        timeout_s: timeout to server and model get into readiness state.\n\n    Raises:\n        PyTritonClientTimeoutError: If server and model are not in readiness state before given timeout.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        KeyboardInterrupt: If hosting process receives SIGINT\n    \"\"\"\n    _LOGGER.debug(f\"Waiting for model {self._model_name} to be ready\")\n    try:\n        await asyncio.wait_for(\n            asyncio_wait_for_model_ready(\n                self._general_client, self._model_name, self._model_version, timeout_s=timeout_s\n            ),\n            self._init_timeout_s,\n        )\n    except asyncio.TimeoutError as e:\n        message = f\"Timeout while waiting for model {self._model_name} to be ready for {self._init_timeout_s}s\"\n        _LOGGER.error(message)\n        raise PyTritonClientTimeoutError(message) from e\n</code></pre>"},{"location":"reference/clients/#pytriton.client.FuturesModelClient","title":"pytriton.client.FuturesModelClient","text":"<pre><code>FuturesModelClient(url: str, model_name: str, model_version: Optional[str] = None, *, max_workers: int = 128, max_queue_size: int = 128, non_blocking: bool = False, init_timeout_s: Optional[float] = None, inference_timeout_s: Optional[float] = None)\n</code></pre> <p>A client for interacting with a model deployed on the Triton Inference Server using concurrent.futures.</p> <p>This client allows asynchronous inference requests using a thread pool executor. It can be used to perform inference on a model by providing input data and receiving the corresponding output data. The client can be used in a <code>with</code> statement to ensure proper resource management.</p> <p>Example usage with context manager:</p> <pre><code>with FuturesModelClient(\"localhost\", \"MyModel\") as client:\n    result_future = client.infer_sample(input1=input1_data, input2=input2_data)\n    # do something else\n    print(result_future.result())\n</code></pre> <p>Usage without context manager:</p> <pre><code>client = FuturesModelClient(\"localhost\", \"MyModel\")\nresult_future = client.infer_sample(input1=input1_data, input2=input2_data)\n# do something else\nprint(result_future.result())\nclient.close()\n</code></pre> <p>Initializes the FuturesModelClient for a given model.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The Triton Inference Server url, e.g. <code>grpc://localhost:8001</code>.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model to interact with.</p> </li> <li> <code>model_version</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The version of the model to interact with. If None, the latest version will be used.</p> </li> <li> <code>max_workers</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>The maximum number of threads that can be used to execute the given calls. If None, there is not limit on the number of threads.</p> </li> <li> <code>max_queue_size</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>The maximum number of requests that can be queued. If None, there is not limit on the number of requests.</p> </li> <li> <code>non_blocking</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the client will raise a PyTritonClientQueueFullError if the queue is full. If False, the client will block until the queue is not full.</p> </li> <li> <code>init_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Timeout in seconds for server and model being ready. If non passed default 60 seconds timeout will be used.</p> </li> <li> <code>inference_timeout_s</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Timeout in seconds for the single model inference request. If non passed default 60 seconds timeout will be used.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    max_workers: int = 128,\n    max_queue_size: int = 128,\n    non_blocking: bool = False,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n):\n    \"\"\"Initializes the FuturesModelClient for a given model.\n\n    Args:\n        url: The Triton Inference Server url, e.g. `grpc://localhost:8001`.\n        model_name: The name of the model to interact with.\n        model_version: The version of the model to interact with. If None, the latest version will be used.\n        max_workers: The maximum number of threads that can be used to execute the given calls. If None, there is not limit on the number of threads.\n        max_queue_size: The maximum number of requests that can be queued. If None, there is not limit on the number of requests.\n        non_blocking: If True, the client will raise a PyTritonClientQueueFullError if the queue is full. If False, the client will block until the queue is not full.\n        init_timeout_s: Timeout in seconds for server and model being ready. If non passed default 60 seconds timeout will be used.\n        inference_timeout_s: Timeout in seconds for the single model inference request. If non passed default 60 seconds timeout will be used.\n    \"\"\"\n    self._url = url\n    self._model_name = model_name\n    self._model_version = model_version\n    self._threads = []\n    self._max_workers = max_workers\n    self._max_queue_size = max_queue_size\n    self._non_blocking = non_blocking\n\n    if self._max_workers is not None and self._max_workers &lt;= 0:\n        raise ValueError(\"max_workers must be greater than 0\")\n    if self._max_queue_size is not None and self._max_queue_size &lt;= 0:\n        raise ValueError(\"max_queue_size must be greater than 0\")\n\n    kwargs = {}\n    if self._max_queue_size is not None:\n        kwargs[\"maxsize\"] = self._max_queue_size\n    self._queue = Queue(**kwargs)\n    self._queue.put((_INIT, None, None))\n    self._init_timeout_s = _DEFAULT_FUTURES_INIT_TIMEOUT_S if init_timeout_s is None else init_timeout_s\n    self._inference_timeout_s = inference_timeout_s\n    self._closed = False\n    self._lock = Lock()\n    self._existing_client = None\n</code></pre>"},{"location":"reference/clients/#pytriton.client.FuturesModelClient.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Create context for using FuturesModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __enter__(self):\n    \"\"\"Create context for using FuturesModelClient as a context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/clients/#pytriton.client.FuturesModelClient.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_value, traceback)\n</code></pre> <p>Close resources used by FuturesModelClient instance when exiting from the context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    \"\"\"Close resources used by FuturesModelClient instance when exiting from the context.\"\"\"\n    self.close()\n</code></pre>"},{"location":"reference/clients/#pytriton.client.FuturesModelClient.close","title":"close","text":"<pre><code>close(wait=True)\n</code></pre> <p>Close resources used by FuturesModelClient.</p> <p>This method closes the resources used by the FuturesModelClient instance, including the Triton Inference Server connections. Once this method is called, the FuturesModelClient instance should not be used again.</p> <p>Parameters:</p> <ul> <li> <code>wait</code>           \u2013            <p>If True, then shutdown will not return until all running futures have finished executing.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def close(self, wait=True):\n    \"\"\"Close resources used by FuturesModelClient.\n\n    This method closes the resources used by the FuturesModelClient instance, including the Triton Inference Server connections.\n    Once this method is called, the FuturesModelClient instance should not be used again.\n\n    Args:\n        wait: If True, then shutdown will not return until all running futures have finished executing.\n    \"\"\"\n    if self._closed:\n        return\n    _LOGGER.debug(\"Closing FuturesModelClient.\")\n\n    self._closed = True\n    for _ in range(len(self._threads)):\n        self._queue.put((_CLOSE, None, None))\n\n    if wait:\n        _LOGGER.debug(\"Waiting for futures to finish.\")\n        for thread in self._threads:\n            thread.join()\n</code></pre>"},{"location":"reference/clients/#pytriton.client.FuturesModelClient.infer_batch","title":"infer_batch","text":"<pre><code>infer_batch(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs) -&gt; Future\n</code></pre> <p>Run asynchronous inference on batched data and return a Future object.</p> <p>This method allows the user to perform inference on batched data by providing input data and receiving the corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.</p> <p>Example usage:</p> <pre><code>with FuturesModelClient(\"localhost\", \"BERT\") as client:\n    future = client.infer_batch(input1_sample, input2_sample)\n    # do something else\n    print(future.result())\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>future = client.infer_batch(input1, input2)\nfuture = client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>Inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary of inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary of HTTP headers for the inference request.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>Inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Future</code>           \u2013            <p>A Future object wrapping a dictionary of inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientClosedError</code>             \u2013            <p>If the FuturesModelClient is closed.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_batch(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Future:\n    \"\"\"Run asynchronous inference on batched data and return a Future object.\n\n    This method allows the user to perform inference on batched data by providing input data and receiving the corresponding output data.\n    The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.\n\n    Example usage:\n\n    ```python\n    with FuturesModelClient(\"localhost\", \"BERT\") as client:\n        future = client.infer_batch(input1_sample, input2_sample)\n        # do something else\n        print(future.result())\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    future = client.infer_batch(input1, input2)\n    future = client.infer_batch(a=input1, b=input2)\n    ```\n\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Optional dictionary of inference parameters.\n        headers: Optional dictionary of HTTP headers for the inference request.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        A Future object wrapping a dictionary of inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\n    return self._execute(name=_INFER_BATCH, request=(inputs, parameters, headers, named_inputs))\n</code></pre>"},{"location":"reference/clients/#pytriton.client.FuturesModelClient.infer_sample","title":"infer_sample","text":"<pre><code>infer_sample(*inputs, parameters: Optional[Dict[str, Union[str, int, bool]]] = None, headers: Optional[Dict[str, Union[str, int, bool]]] = None, **named_inputs) -&gt; Future\n</code></pre> <p>Run asynchronous inference on a single data sample and return a Future object.</p> <p>This method allows the user to perform inference on a single data sample by providing input data and receiving the corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.</p> <p>Example usage:</p> <pre><code>with FuturesModelClient(\"localhost\", \"BERT\") as client:\n    result_future = client.infer_sample(input1=input1_data, input2=input2_data)\n    # do something else\n    print(result_future.result())\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>future = client.infer_sample(input1, input2)\nfuture = client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>*inputs</code>           \u2013            <p>Inference inputs provided as positional arguments.</p> </li> <li> <code>parameters</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary of inference parameters.</p> </li> <li> <code>headers</code>               (<code>Optional[Dict[str, Union[str, int, bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary of HTTP headers for the inference request.</p> </li> <li> <code>**named_inputs</code>           \u2013            <p>Inference inputs provided as named arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Future</code>           \u2013            <p>A Future object wrapping a dictionary of inference results, where dictionary keys are output names.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientClosedError</code>             \u2013            <p>If the FuturesModelClient is closed.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_sample(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Future:\n    \"\"\"Run asynchronous inference on a single data sample and return a Future object.\n\n    This method allows the user to perform inference on a single data sample by providing input data and receiving the\n    corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.\n\n    Example usage:\n\n    ```python\n    with FuturesModelClient(\"localhost\", \"BERT\") as client:\n        result_future = client.infer_sample(input1=input1_data, input2=input2_data)\n        # do something else\n        print(result_future.result())\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    future = client.infer_sample(input1, input2)\n    future = client.infer_sample(a=input1, b=input2)\n    ```\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Optional dictionary of inference parameters.\n        headers: Optional dictionary of HTTP headers for the inference request.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        A Future object wrapping a dictionary of inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\n    return self._execute(\n        name=_INFER_SAMPLE,\n        request=(inputs, parameters, headers, named_inputs),\n    )\n</code></pre>"},{"location":"reference/clients/#pytriton.client.FuturesModelClient.model_config","title":"model_config","text":"<pre><code>model_config() -&gt; Future\n</code></pre> <p>Obtain the configuration of the model deployed on the Triton Inference Server.</p> <p>This method returns a Future object that will contain the TritonModelConfig object when it is ready. Client will wait init_timeout_s for the server to get into readiness state before obtaining the model configuration.</p> <p>Returns:</p> <ul> <li> <code>Future</code>           \u2013            <p>A Future object that will contain the TritonModelConfig object when it is ready.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonClientClosedError</code>             \u2013            <p>If the FuturesModelClient is closed.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def model_config(self) -&gt; Future:\n    \"\"\"Obtain the configuration of the model deployed on the Triton Inference Server.\n\n    This method returns a Future object that will contain the TritonModelConfig object when it is ready.\n    Client will wait init_timeout_s for the server to get into readiness state before obtaining the model configuration.\n\n    Returns:\n        A Future object that will contain the TritonModelConfig object when it is ready.\n\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\n    return self._execute(name=_MODEL_CONFIG)\n</code></pre>"},{"location":"reference/clients/#pytriton.client.FuturesModelClient.wait_for_model","title":"wait_for_model","text":"<pre><code>wait_for_model(timeout_s: float) -&gt; Future\n</code></pre> <p>Returns a Future object which result will be None when the model is ready.</p> <p>Typical usage:</p> <pre><code>with FuturesModelClient(\"localhost\", \"BERT\") as client\n    future = client.wait_for_model(300.)\n    # do something else\n    future.result()   # wait rest of timeout_s time\n                        # till return None if model is ready\n                        # or raise PyTritonClientTimeutError\n</code></pre> <p>Parameters:</p> <ul> <li> <code>timeout_s</code>               (<code>float</code>)           \u2013            <p>The maximum amount of time to wait for the model to be ready, in seconds.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Future</code>           \u2013            <p>A Future object which result is None when the model is ready.</p> </li> </ul> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_model(self, timeout_s: float) -&gt; Future:\n    \"\"\"Returns a Future object which result will be None when the model is ready.\n\n    Typical usage:\n\n    ```python\n    with FuturesModelClient(\"localhost\", \"BERT\") as client\n        future = client.wait_for_model(300.)\n        # do something else\n        future.result()   # wait rest of timeout_s time\n                            # till return None if model is ready\n                            # or raise PyTritonClientTimeutError\n    ```\n\n    Args:\n        timeout_s: The maximum amount of time to wait for the model to be ready, in seconds.\n\n    Returns:\n        A Future object which result is None when the model is ready.\n    \"\"\"\n    return self._execute(\n        name=_WAIT_FOR_MODEL,\n        request=timeout_s,\n    )\n</code></pre>"},{"location":"reference/decorators/","title":"Decorators","text":""},{"location":"reference/decorators/#decorators","title":"Decorators","text":""},{"location":"reference/decorators/#pytriton.decorators","title":"pytriton.decorators","text":"<p>Inference callable decorators.</p>"},{"location":"reference/decorators/#pytriton.decorators.ConstantPadder","title":"ConstantPadder","text":"<pre><code>ConstantPadder(pad_value=0)\n</code></pre> <p>Padder that pads the given batches with a constant value.</p> <p>Initialize the padder.</p> <p>Parameters:</p> <ul> <li> <code>pad_value</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Padding value. Defaults to 0.</p> </li> </ul> Source code in <code>pytriton/decorators.py</code> <pre><code>def __init__(self, pad_value=0):\n    \"\"\"Initialize the padder.\n\n    Args:\n        pad_value (int, optional): Padding value. Defaults to 0.\n    \"\"\"\n    self.pad_value = pad_value\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.ConstantPadder.__call__","title":"__call__","text":"<pre><code>__call__(batches_list: InferenceResults) -&gt; InferenceResults\n</code></pre> <p>Pad the given batches with the specified value to pad size enabling further batching to single arrays.</p> <p>Parameters:</p> <ul> <li> <code>batches_list</code>               (<code>List[Dict[str, ndarray]]</code>)           \u2013            <p>List of batches to pad.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InferenceResults</code>           \u2013            <p>List[Dict[str, np.ndarray]]: List of padded batches.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonRuntimeError</code>             \u2013            <p>If the input arrays for a given input name have different dtypes.</p> </li> </ul> Source code in <code>pytriton/decorators.py</code> <pre><code>def __call__(self, batches_list: InferenceResults) -&gt; InferenceResults:\n    \"\"\"Pad the given batches with the specified value to pad size enabling further batching to single arrays.\n\n    Args:\n        batches_list (List[Dict[str, np.ndarray]]): List of batches to pad.\n\n    Returns:\n        List[Dict[str, np.ndarray]]: List of padded batches.\n\n    Raises:\n        PyTritonRuntimeError: If the input arrays for a given input name have different dtypes.\n    \"\"\"\n\n    def _get_padded_shape(_batches: List[np.ndarray]) -&gt; Tuple[int, ...]:\n        \"\"\"Get the shape of the padded array without batch axis.\"\"\"\n        return tuple(np.max([batch.shape[1:] for batch in _batches if batch is not None], axis=0))\n\n    def _get_padded_dtype(_batches: List[np.ndarray]) -&gt; np.dtype:\n        dtypes = [batch.dtype for batch in _batches if batch is not None]\n        result_dtype = dtypes[0]\n\n        if not all(dtype.kind == result_dtype.kind for dtype in dtypes):\n            raise PyTritonRuntimeError(\"All input arrays for given input name must have the same dtype.\")\n\n        # for bytes (encoded string) or unicode string need to obtain the max length\n        if result_dtype.kind in \"SU\":\n            order_and_kind = result_dtype.str[:2]\n            max_len = max([int(dtype.str[2:]) for dtype in dtypes])\n            result_dtype = f\"{order_and_kind}{max_len}\"\n        else:\n            if not all(dtype == result_dtype for dtype in dtypes):\n                raise PyTritonRuntimeError(\"All input arrays for given input name must have the same dtype.\")\n\n        return np.dtype(result_dtype)\n\n    input_names = list(\n        collections.OrderedDict.fromkeys(input_name for batch in batches_list for input_name in batch.keys())\n    )\n    batches_by_name = {input_name: [batch.get(input_name) for batch in batches_list] for input_name in input_names}\n    for input_batches in batches_by_name.values():\n        result_shape, result_dtype = _get_padded_shape(input_batches), _get_padded_dtype(input_batches)\n        for batch_idx, batch in enumerate(input_batches):\n            if batch is not None:\n                input_batches[batch_idx] = np.pad(\n                    batch,\n                    [(0, 0)] + [(0, b - a) for a, b in zip(batch.shape[1:], result_shape)],\n                    mode=\"constant\",\n                    constant_values=self.pad_value if result_dtype.kind not in [\"S\", \"U\", \"O\"] else b\"\",\n                ).astype(result_dtype)\n\n    return [\n        {name: batches[batch_idx] for name, batches in batches_by_name.items() if batches[batch_idx] is not None}\n        for batch_idx in range(len(batches_list))\n    ]\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.ModelConfigDict","title":"ModelConfigDict","text":"<pre><code>ModelConfigDict()\n</code></pre> <p>               Bases: <code>MutableMapping</code></p> <p>Dictionary for storing model configs for inference callable.</p> <p>Create ModelConfigDict object.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __init__(self):\n    \"\"\"Create ModelConfigDict object.\"\"\"\n    self._data: Dict[str, TritonModelConfig] = {}\n    self._keys: List[Callable] = []\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.ModelConfigDict.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(infer_callable: Callable)\n</code></pre> <p>Delete model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __delitem__(self, infer_callable: Callable):\n    \"\"\"Delete model config for inference callable.\"\"\"\n    key = self._get_model_config_key(infer_callable)\n    del self._data[key]\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.ModelConfigDict.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(infer_callable: Callable) -&gt; TritonModelConfig\n</code></pre> <p>Get model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __getitem__(self, infer_callable: Callable) -&gt; TritonModelConfig:\n    \"\"\"Get model config for inference callable.\"\"\"\n    key = self._get_model_config_key(infer_callable)\n    return self._data[key]\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.ModelConfigDict.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate over inference callable keys.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over inference callable keys.\"\"\"\n    return iter(self._keys)\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.ModelConfigDict.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Get number of inference callable keys.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __len__(self):\n    \"\"\"Get number of inference callable keys.\"\"\"\n    return len(self._data)\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.ModelConfigDict.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(infer_callable: Callable, item: TritonModelConfig)\n</code></pre> <p>Set model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __setitem__(self, infer_callable: Callable, item: TritonModelConfig):\n    \"\"\"Set model config for inference callable.\"\"\"\n    self._keys.append(infer_callable)\n    key = self._get_model_config_key(infer_callable)\n    self._data[key] = item\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.TritonContext","title":"TritonContext  <code>dataclass</code>","text":"<pre><code>TritonContext(model_configs: ModelConfigDict = ModelConfigDict())\n</code></pre> <p>Triton context definition class.</p>"},{"location":"reference/decorators/#pytriton.decorators.batch","title":"batch","text":"<pre><code>batch(wrapped, instance, args, kwargs)\n</code></pre> <p>Decorator for converting list of request dicts to dict of input batches.</p> <p>Converts list of request dicts to dict of input batches. It passes **kwargs to inference callable where each named input contains numpy array with batch of requests received by Triton server. We assume that each request has the same set of keys (you can use group_by_keys decorator before using @batch decorator if your requests may have different set of keys).</p> <p>Raises:</p> <ul> <li> <code>PyTritonValidationError</code>             \u2013            <p>If the requests have different set of keys.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the output tensors have different than expected batch sizes. Expected batch size is calculated as a sum of batch sizes of all requests.</p> </li> </ul> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef batch(wrapped, instance, args, kwargs):\n    \"\"\"Decorator for converting list of request dicts to dict of input batches.\n\n    Converts list of request dicts to dict of input batches.\n    It passes **kwargs to inference callable where each named input contains numpy array with batch of requests\n    received by Triton server.\n    We assume that each request has the same set of keys (you can use group_by_keys decorator before\n    using @batch decorator if your requests may have different set of keys).\n\n    Raises:\n        PyTritonValidationError: If the requests have different set of keys.\n        ValueError: If the output tensors have different than expected batch sizes. Expected batch size is\n            calculated as a sum of batch sizes of all requests.\n    \"\"\"\n    telemetry_name = \"pytriton-batch-decorator-span\"\n\n    req_list = args[0]\n    input_names = req_list[0].keys()\n\n    for req_dict2 in req_list[1:]:\n        if input_names != req_dict2.keys():\n            raise PyTritonValidationError(\"Cannot batch requests with different set of inputs keys\")\n\n    inputs = {}\n    for model_input in input_names:\n        concatenated_input_data = np.concatenate([req[model_input] for req in req_list])\n        inputs[model_input] = concatenated_input_data\n\n    args = args[1:]\n    new_kwargs = dict(kwargs)\n    new_kwargs.update(inputs)\n    spans = [start_span_from_span(request.span, telemetry_name) for request in req_list if request.span is not None]\n    try:\n        outputs = wrapped(*args, **new_kwargs)\n    finally:\n        for span in spans:\n            span.end()\n\n    def _split_result(_result):\n        outputs = convert_output(_result, wrapped, instance)\n        output_names = outputs.keys()\n\n        requests_total_batch_size = sum(get_inference_request_batch_size(req) for req in req_list)\n        not_matching_tensors_shapes = {\n            output_name: output_tensor.shape\n            for output_name, output_tensor in outputs.items()\n            if output_tensor.shape[0] != requests_total_batch_size\n        }\n        if not_matching_tensors_shapes:\n            raise ValueError(\n                f\"Received output tensors with different batch sizes: {', '.join(': '.join(map(str, item)) for item in not_matching_tensors_shapes.items())}. \"\n                f\"Expected batch size: {requests_total_batch_size}. \"\n            )\n\n        out_list = []\n        start_idx = 0\n        for request in req_list:\n            # get batch_size of first input for each request - assume that all inputs have same batch_size\n            request_batch_size = get_inference_request_batch_size(request)\n            req_output_dict = {}\n            for _output_ind, output_name in enumerate(output_names):\n                req_output = outputs[output_name][start_idx : start_idx + request_batch_size, ...]\n                req_output_dict[output_name] = req_output\n            out_list.append(req_output_dict)\n            start_idx += request_batch_size\n        return out_list\n\n    if inspect.isgenerator(outputs):\n        return (_split_result(_result) for _result in outputs)\n    else:\n        return _split_result(outputs)\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.convert_output","title":"convert_output","text":"<pre><code>convert_output(outputs: Union[Dict, List, Tuple], wrapped=None, instance=None, model_config: Optional[TritonModelConfig] = None)\n</code></pre> <p>Converts output from tuple ot list to dictionary.</p> <p>It is utility function useful for mapping output list into dictionary of outputs. Currently, it is used in @sample and @batch decorators (we assume that user can return list or tuple of outputs instead of dictionary if this list matches output list in model config (size and order).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def convert_output(\n    outputs: Union[Dict, List, Tuple], wrapped=None, instance=None, model_config: Optional[TritonModelConfig] = None\n):\n    \"\"\"Converts output from tuple ot list to dictionary.\n\n    It is utility function useful for mapping output list into dictionary of outputs.\n    Currently, it is used in @sample and @batch decorators (we assume that user can return list or tuple of outputs\n    instead of dictionary if this list matches output list in model config (size and order).\n    \"\"\"\n    if isinstance(outputs, dict):\n        return outputs\n    elif isinstance(outputs, (list, tuple)):\n        if model_config is None:\n            model_config = get_model_config(wrapped, instance)\n        if len(outputs) != len(model_config.outputs):\n            raise PyTritonValidationError(\"Outputs length different than config outputs length\")\n        outputs = {config_output.name: output for config_output, output in zip(model_config.outputs, outputs)}\n        return outputs\n    else:\n        raise PyTritonValidationError(f\"Unsupported output type {type(outputs)}.\")\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.fill_optionals","title":"fill_optionals","text":"<pre><code>fill_optionals(**defaults)\n</code></pre> <p>This decorator ensures that any missing inputs in requests are filled with default values specified by the user.</p> <p>Default values should be NumPy arrays without batch axis.</p> <p>If you plan to group requests ex. with @group_by_keys or @group_by_vales decorators provide default values for optional parameters at the beginning of decorators stack. The other decorators can then group requests into bigger batches resulting in a better model performance.</p> <p>Typical use: <pre><code>@fill_optionals()\n@group_by_keys()\n@batch\ndef infer_fun(**inputs):\n    ...\n    return outputs\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>defaults</code>           \u2013            <p>keyword arguments containing default values for missing inputs</p> </li> </ul> <p>If you have default values for some optional parameter it is good idea to provide them at the very beginning, so the other decorators (e.g. @group_by_keys) can make bigger consistent groups.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def fill_optionals(**defaults):\n    \"\"\"This decorator ensures that any missing inputs in requests are filled with default values specified by the user.\n\n    Default values should be NumPy arrays without batch axis.\n\n    If you plan to group requests ex. with\n    [@group_by_keys][pytriton.decorators.group_by_keys] or\n    [@group_by_vales][pytriton.decorators.group_by_values] decorators\n    provide default values for optional parameters at the beginning of decorators stack.\n    The other decorators can then group requests into bigger batches resulting in a better model performance.\n\n    Typical use:\n    ```python\n    @fill_optionals()\n    @group_by_keys()\n    @batch\n    def infer_fun(**inputs):\n        ...\n        return outputs\n    ```\n\n    Args:\n        defaults: keyword arguments containing default values for missing inputs\n\n\n    If you have default values for some optional parameter it is good idea to provide them at the very beginning,\n    so the other decorators (e.g. @group_by_keys) can make bigger consistent groups.\n    \"\"\"\n\n    def _verify_defaults(model_config: TritonModelConfig):\n        inputs = {spec.name: spec for spec in model_config.inputs}\n        not_matching_default_names = sorted(set(defaults) - set(inputs))\n        if not_matching_default_names:\n            raise PyTritonBadParameterError(f\"Could not found {', '.join(not_matching_default_names)} inputs\")\n\n        non_numpy_items = {k: v for k, v in defaults.items() if not isinstance(v, np.ndarray)}\n        if non_numpy_items:\n            raise PyTritonBadParameterError(\n                f\"Could not use {', '.join([f'{k}={v}' for k, v in non_numpy_items.items()])} defaults \"\n                \"as they are not NumPy arrays\"\n            )\n\n        not_matching_dtypes = {k: (v.dtype, inputs[k].dtype) for k, v in defaults.items() if v.dtype != inputs[k].dtype}\n        if not_matching_dtypes:\n            non_matching_dtypes_str_list = [\n                f\"{name}: dtype={have_dtype} expected_dtype={expected_dtype}\"\n                for name, (have_dtype, expected_dtype) in not_matching_dtypes.items()\n            ]\n            raise PyTritonBadParameterError(\n                f\"Could not use {', '.join(non_matching_dtypes_str_list)} \"\n                f\"defaults as they have different than input signature dtypes\"\n            )\n\n        def _shape_match(_have_shape, _expected_shape):\n            return len(_have_shape) == len(_expected_shape) and all(\n                e == -1 or h == e for h, e in zip(_have_shape, _expected_shape)\n            )\n\n        not_matching_shapes = {\n            k: (v.shape, inputs[k].shape) for k, v in defaults.items() if not _shape_match(v.shape, inputs[k].shape)\n        }\n        if not_matching_shapes:\n            non_matching_shapes_str_list = [\n                f\"{name}: shape={have_shape} expected_shape={expected_shape}\"\n                for name, (have_shape, expected_shape) in not_matching_shapes.items()\n            ]\n            raise PyTritonBadParameterError(\n                f\"Could not use {', '.join(non_matching_shapes_str_list)} \"\n                f\"defaults as they have different than input signature shapes\"\n            )\n\n    @wrapt.decorator\n    def _wrapper(wrapped, instance, args, kwargs):\n        model_config = get_model_config(wrapped, instance)\n        _verify_defaults(model_config)\n        # verification if not after group wrappers is in group wrappers\n\n        (requests,) = args\n\n        model_supports_batching = model_config.batching\n        for request in requests:\n            batch_size = get_inference_request_batch_size(request) if model_supports_batching else None\n            for default_key, default_value in defaults.items():\n                if default_key in request:\n                    continue\n\n                if model_supports_batching:\n                    ones_reps = (1,) * default_value.ndim  # repeat once default_value on each axis\n                    axis_reps = (batch_size,) + ones_reps  # ... except on batch axis. we repeat it batch_size times\n                    default_value = np.tile(default_value, axis_reps)\n\n                request[default_key] = default_value\n        return wrapped(*args, **kwargs)\n\n    return _wrapper\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.first_value","title":"first_value","text":"<pre><code>first_value(*keys: str, squeeze_single_values=True, strict: bool = True)\n</code></pre> <p>This decorator overwrites selected inputs with first element of the given input.</p> <p>It can be used in two ways:</p> <ol> <li> <p>Wrapping a single request inference callable by chaining with @batch decorator:     <pre><code>@batch\n@first_value(\"temperature\")\ndef infer_fn(**inputs):\n    ...\n    return result\n</code></pre></p> </li> <li> <p>Wrapping a multiple requests inference callable:     <pre><code>@first_value(\"temperature\")\ndef infer_fn(requests):\n    ...\n    return results\n</code></pre></p> </li> </ol> <p>By default, the decorator squeezes single value arrays to scalars. This behavior can be disabled by setting the <code>squeeze_single_values</code> flag to False.</p> <p>By default, the decorator checks the equality of the values on selected values. This behavior can be disabled by setting the <code>strict</code> flag to False.</p> <p>Wrapper can only be used with models that support batching.</p> <p>Parameters:</p> <ul> <li> <code>keys</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>The input keys selected for conversion.</p> </li> <li> <code>squeeze_single_values</code>           \u2013            <p>squeeze single value ND array to scalar values. Defaults to True.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>enable checking if all values on single selected input of request are equal. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PyTritonRuntimeError</code>             \u2013            <p>if not all values on a single selected input of the request are equal</p> </li> <li> <code>PyTritonBadParameterError</code>             \u2013            <p>if any of the keys passed to the decorator are not allowed.</p> </li> </ul> Source code in <code>pytriton/decorators.py</code> <pre><code>def first_value(*keys: str, squeeze_single_values=True, strict: bool = True):\n    \"\"\"This decorator overwrites selected inputs with first element of the given input.\n\n    It can be used in two ways:\n\n    1. Wrapping a single request inference callable by chaining with @batch decorator:\n        ```python\n        @batch\n        @first_value(\"temperature\")\n        def infer_fn(**inputs):\n            ...\n            return result\n        ```\n\n    2. Wrapping a multiple requests inference callable:\n        ```python\n        @first_value(\"temperature\")\n        def infer_fn(requests):\n            ...\n            return results\n        ```\n\n    By default, the decorator squeezes single value arrays to scalars.\n    This behavior can be disabled by setting the `squeeze_single_values` flag to False.\n\n    By default, the decorator checks the equality of the values on selected values.\n    This behavior can be disabled by setting the `strict` flag to False.\n\n    Wrapper can only be used with models that support batching.\n\n    Args:\n        keys: The input keys selected for conversion.\n        squeeze_single_values: squeeze single value ND array to scalar values. Defaults to True.\n        strict: enable checking if all values on single selected input of request are equal. Defaults to True.\n\n    Raises:\n        PyTritonRuntimeError: if not all values on a single selected input of the request are equal\n        and the strict flag is set to True. Additionally, if the decorator is used with a model that doesn't support batching,\n        PyTritonBadParameterError: if any of the keys passed to the decorator are not allowed.\n    \"\"\"\n    if any(k in _SPECIAL_KEYS for k in keys):\n        not_allowed_keys = [key for key in keys if key in _SPECIAL_KEYS]\n        raise PyTritonBadParameterError(\n            f\"The keys {', '.join(not_allowed_keys)} are not allowed as keys for @first_value wrapper. \"\n            f\"The set of not allowed keys are {', '.join(_SPECIAL_KEYS)}\"\n        )\n\n    @wrapt.decorator\n    def wrapper(wrapped, instance, args, kwargs):\n        model_config = get_model_config(wrapped, instance)\n        if not model_config.batching:\n            raise PyTritonRuntimeError(\"The @first_value decorator can only be used with models that support batching.\")\n\n        def _replace_inputs_with_first_value(_request):\n            for input_name in keys:\n                if input_name not in _request:\n                    continue\n\n                values = _request[input_name]\n                if strict:\n                    # do not set axis for arrays with strings (object) or models not supporting batching\n                    axis_of_uniqueness = None if values.dtype == object else 0\n                    unique_values = np.unique(values, axis=axis_of_uniqueness)\n                    if len(unique_values) &gt; 1:\n                        raise PyTritonRuntimeError(\n                            f\"The values on the {input_name!r} input are not equal. \"\n                            \"To proceed, either disable strict mode in @first_value wrapper \"\n                            \"or ensure that the values always are consistent. \"\n                            f\"The current values of {input_name!r} are {_request[input_name]!r}.\"\n                        )\n\n                _first_value = values[0]\n                if (\n                    squeeze_single_values\n                    and not np.isscalar(_first_value)\n                    and all(dim == 1 for dim in _first_value.shape)\n                ):\n                    _dim_0_array = np.squeeze(_first_value)\n                    _first_value = _dim_0_array[()]  # obtain scalar from 0-dim array with numpy type\n\n                _request[input_name] = _first_value\n            return _request\n\n        inputs_names = set(kwargs) - set(_SPECIAL_KEYS)\n        if inputs_names:\n            kwargs = _replace_inputs_with_first_value(kwargs)\n            return wrapped(*args, **kwargs)\n        else:\n            requests, *other_args = args\n            requests = [_replace_inputs_with_first_value(request) for request in requests]\n            return wrapped(requests, *other_args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.get_inference_request_batch_size","title":"get_inference_request_batch_size","text":"<pre><code>get_inference_request_batch_size(inference_request: InferenceRequest) -&gt; int\n</code></pre> <p>Get batch size from triton request.</p> <p>Parameters:</p> <ul> <li> <code>inference_request</code>               (<code>InferenceRequest</code>)           \u2013            <p>Triton request.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>Batch size.</p> </li> </ul> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_inference_request_batch_size(inference_request: InferenceRequest) -&gt; int:\n    \"\"\"Get batch size from triton request.\n\n    Args:\n        inference_request (InferenceRequest): Triton request.\n\n    Returns:\n        int: Batch size.\n    \"\"\"\n    first_input_value = next(iter(inference_request.values()))\n    batch_size, *_dims = first_input_value.shape\n    return batch_size\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.get_model_config","title":"get_model_config","text":"<pre><code>get_model_config(wrapped, instance) -&gt; TritonModelConfig\n</code></pre> <p>Retrieves instance of TritonModelConfig from callable.</p> <p>It is internally used in convert_output function to get output list from model. You can use this in custom decorators if you need access to model_config information. If you use @triton_context decorator you do not need this function (you can get model_config directly from triton_context passing function/callable to dictionary getter).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_model_config(wrapped, instance) -&gt; TritonModelConfig:\n    \"\"\"Retrieves instance of TritonModelConfig from callable.\n\n    It is internally used in convert_output function to get output list from model.\n    You can use this in custom decorators if you need access to model_config information.\n    If you use @triton_context decorator you do not need this function (you can get model_config directly\n    from triton_context passing function/callable to dictionary getter).\n    \"\"\"\n    return get_triton_context(wrapped, instance).model_configs[wrapped]\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.get_triton_context","title":"get_triton_context","text":"<pre><code>get_triton_context(wrapped, instance) -&gt; TritonContext\n</code></pre> <p>Retrieves triton context from callable.</p> <p>It is used in @triton_context to get triton context registered by triton binding in inference callable. If you use @triton_context decorator you do not need this function.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_triton_context(wrapped, instance) -&gt; TritonContext:\n    \"\"\"Retrieves triton context from callable.\n\n    It is used in @triton_context to get triton context registered by triton binding in inference callable.\n    If you use @triton_context decorator you do not need this function.\n    \"\"\"\n    caller = instance or wrapped\n    if not hasattr(caller, \"__triton_context__\"):\n        raise PyTritonValidationError(\"Wrapped function or object must bound with triton to get  __triton_context__\")\n    return caller.__triton_context__\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.group_by_keys","title":"group_by_keys","text":"<pre><code>group_by_keys(wrapped, instance, args, kwargs)\n</code></pre> <p>Group by keys.</p> <p>Decorator prepares groups of requests with the same set of keys and calls wrapped function for each group separately (it is convenient to use this decorator before batching, because the batching decorator requires consistent set of inputs as it stacks them into batches).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef group_by_keys(wrapped, instance, args, kwargs):\n    \"\"\"Group by keys.\n\n    Decorator prepares groups of requests with the same set of keys and calls wrapped function\n    for each group separately (it is convenient to use this decorator before batching, because the batching decorator\n    requires consistent set of inputs as it stacks them into batches).\n    \"\"\"\n    inputs = args[0]\n    idx_inputs = [(idx, tuple(sorted(input.keys())), input) for idx, input in enumerate(inputs)]\n    idx_inputs.sort(key=operator.itemgetter(1))\n    idx_groups_res = []\n    for _, group in itertools.groupby(idx_inputs, key=operator.itemgetter(1)):\n        idx, _key, sample_list = zip(*group)\n        args = (list(sample_list),) + args[1:]\n        out = wrapped(*args, **kwargs)\n        idx_groups_res.extend(zip(idx, out))\n\n    idx_groups_res.sort(key=operator.itemgetter(0))\n    res_flat = [r[1] for r in idx_groups_res]\n    return res_flat\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.group_by_values","title":"group_by_values","text":"<pre><code>group_by_values(*keys, pad_fn: Optional[Callable[[InferenceRequests], InferenceRequests]] = None)\n</code></pre> <p>Decorator for grouping requests by values of selected keys.</p> <p>This function splits a batch into multiple sub-batches based on the specified keys values and calls the decorated function with each sub-batch. This is particularly useful when working with models that require dynamic parameters sent by the user.</p> <p>For example, given an input of the form:</p> <pre><code>{\"sentences\": [b\"Sentence1\", b\"Sentence2\", b\"Sentence3\"], \"param1\": [1, 1, 2], \"param2\": [1, 1, 1]}\n</code></pre> <p>Using @group_by_values(\"param1\", \"param2\") will split the batch into two sub-batches:</p> <pre><code>[\n    {\"sentences\": [b\"Sentence1\", b\"Sentence2\"], \"param1\": [1, 1], \"param2\": [1, 1]},\n    {\"sentences\": [b\"Sentence3\"], \"param1\": [2], \"param2\": [1]}\n]\n</code></pre> <p>This decorator should be used after the @batch decorator.</p> <p>Example usage: <pre><code>@batch\n@group_by_values(\"param1\", \"param2\")\ndef infer_fun(**inputs):\n    ...\n    return outputs\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>*keys</code>           \u2013            <p>List of keys to group by.</p> </li> <li> <code>pad_fn</code>               (<code>Optional[Callable[[InferenceRequests], InferenceRequests]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional function to pad the batch to the same size before merging again to a single batch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The decorator function.</p> </li> </ul> Source code in <code>pytriton/decorators.py</code> <pre><code>def group_by_values(*keys, pad_fn: typing.Optional[typing.Callable[[InferenceRequests], InferenceRequests]] = None):\n    \"\"\"Decorator for grouping requests by values of selected keys.\n\n    This function splits a batch into multiple sub-batches based on the specified keys values and\n    calls the decorated function with each sub-batch. This is particularly useful when working with models\n    that require dynamic parameters sent by the user.\n\n    For example, given an input of the form:\n\n    ```python\n    {\"sentences\": [b\"Sentence1\", b\"Sentence2\", b\"Sentence3\"], \"param1\": [1, 1, 2], \"param2\": [1, 1, 1]}\n    ```\n\n    Using @group_by_values(\"param1\", \"param2\") will split the batch into two sub-batches:\n\n    ```python\n    [\n        {\"sentences\": [b\"Sentence1\", b\"Sentence2\"], \"param1\": [1, 1], \"param2\": [1, 1]},\n        {\"sentences\": [b\"Sentence3\"], \"param1\": [2], \"param2\": [1]}\n    ]\n    ```\n\n    This decorator should be used after the @batch decorator.\n\n    Example usage:\n    ```python\n    @batch\n    @group_by_values(\"param1\", \"param2\")\n    def infer_fun(**inputs):\n        ...\n        return outputs\n    ```\n\n    Args:\n        *keys: List of keys to group by.\n        pad_fn: Optional function to pad the batch to the same size before merging again to a single batch.\n\n    Returns:\n        The decorator function.\n    \"\"\"\n\n    def value_to_key(value):\n        if isinstance(value, np.ndarray):\n            if value.dtype == np.object_ or value.dtype.type == np.bytes_:\n                return _serialize_byte_tensor(value)\n            else:\n                return value.tobytes()\n        return value\n\n    def _get_sort_key_for_sample(_request, _sample_idx: int):\n        return tuple(value_to_key(_request[_key][_sample_idx]) for _key in keys)\n\n    def _group_request(_request: InferenceRequest, _batch_size: int):\n        idx_inputs = [(sample_idx, _get_sort_key_for_sample(_request, sample_idx)) for sample_idx in range(_batch_size)]\n        idx_inputs.sort(key=operator.itemgetter(1))\n        for _, group in itertools.groupby(idx_inputs, key=operator.itemgetter(1)):\n            _samples_idxes, _ = zip(*group)\n            grouped_request = {input_name: value[_samples_idxes, ...] for input_name, value in _request.items()}\n            yield _samples_idxes, grouped_request\n\n    @wrapt.decorator\n    def _wrapper(wrapped, instance, args, kwargs):\n        wrappers_stack = [\n            callable_with_wrapper.wrapper\n            for callable_with_wrapper in _get_wrapt_stack(wrapped)\n            if callable_with_wrapper.wrapper is not None\n        ]\n        if batch in wrappers_stack:\n            raise PyTritonRuntimeError(\"The @group_by_values decorator must be used after the @batch decorator.\")\n\n        request = {k: v for k, v in kwargs.items() if k not in _SPECIAL_KEYS}\n        other_kwargs = {k: v for k, v in kwargs.items() if k in _SPECIAL_KEYS}\n\n        batch_size = get_inference_request_batch_size(request)\n        sample_indices_with_interim_result = []\n        for sample_indices, _grouped_sub_request in _group_request(request, batch_size):\n            interim_result = wrapped(*args, **_grouped_sub_request, **other_kwargs)\n            sample_indices_with_interim_result.append((sample_indices, interim_result))\n\n        if pad_fn is not None:\n            indices, results = tuple(map(tuple, zip(*sample_indices_with_interim_result)))\n            results = pad_fn(results)\n            sample_indices_with_interim_result = tuple(zip(indices, results))\n\n        _, first_result_data = sample_indices_with_interim_result[0]\n        result = {\n            output_name: np.zeros((batch_size,) + data.shape[1:], dtype=data.dtype)\n            for output_name, data in first_result_data.items()\n        }\n        for indices, results in sample_indices_with_interim_result:\n            for output_name, data in results.items():\n                result[output_name][indices, ...] = data\n\n        return result\n\n    return _wrapper\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.pad_batch","title":"pad_batch","text":"<pre><code>pad_batch(wrapped, instance, args, kwargs)\n</code></pre> <p>Add padding to the inputs batches.</p> <p>Decorator appends last rows to the inputs multiple times to get desired batch size (preferred batch size or max batch size from model config whatever is closer to current input size).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef pad_batch(wrapped, instance, args, kwargs):\n    \"\"\"Add padding to the inputs batches.\n\n    Decorator appends last rows to the inputs multiple times to get desired batch size (preferred batch size or\n    max batch size from model config whatever is closer to current input size).\n    \"\"\"\n    inputs = {k: v for k, v in kwargs.items() if k != \"__triton_context__\"}\n    first_input = next(iter(inputs.values()))\n    config = get_model_config(wrapped, instance)\n    batch_sizes = (\n        []\n        if (config.batcher is None or config.batcher.preferred_batch_size is None)\n        else sorted(config.batcher.preferred_batch_size)\n    )\n    batch_sizes.append(config.max_batch_size)\n    batch_size = batch_sizes[bisect_left(batch_sizes, first_input.shape[0])]\n\n    new_inputs = {\n        input_name: np.repeat(\n            input_array,\n            np.concatenate([\n                np.ones(input_array.shape[0] - 1),\n                np.array([batch_size - input_array.shape[0] + 1]),\n            ]).astype(np.int64),\n            axis=0,\n        )\n        for input_name, input_array in inputs.items()\n    }\n\n    kwargs.update(new_inputs)\n    return wrapped(*args, **kwargs)\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.sample","title":"sample","text":"<pre><code>sample(wrapped, instance, args, kwargs)\n</code></pre> <p>Decorator is used for non-batched inputs to convert from one element list of requests to request kwargs.</p> <p>Decorator takes first request and convert it into named inputs. Useful with non-batching models - instead of one element list of request, we will get named inputs - <code>kwargs</code>.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef sample(wrapped, instance, args, kwargs):\n    \"\"\"Decorator is used for non-batched inputs to convert from one element list of requests to request kwargs.\n\n    Decorator takes first request and convert it into named inputs.\n    Useful with non-batching models - instead of one element list of request, we will get named inputs - `kwargs`.\n    \"\"\"\n    kwargs.update(args[0][0])\n    outputs = wrapped(*args[1:], **kwargs)\n    outputs = convert_output(outputs, wrapped, instance)\n    return [outputs]\n</code></pre>"},{"location":"reference/decorators/#pytriton.decorators.triton_context","title":"triton_context","text":"<pre><code>triton_context(wrapped, instance, args, kwargs)\n</code></pre> <p>Adds triton context.</p> <p>It gives you additional argument passed to the function in **kwargs called 'triton_context'. You can read model config from it and in the future possibly have some interaction with triton.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef triton_context(wrapped, instance, args, kwargs):\n    \"\"\"Adds triton context.\n\n    It gives you additional argument passed to the function in **kwargs called 'triton_context'.\n    You can read model config from it and in the future possibly have some interaction with triton.\n    \"\"\"\n    kwargs[TRITON_CONTEXT_FIELD_NAME] = get_triton_context(wrapped, instance)\n    return wrapped(*args, **kwargs)\n</code></pre>"},{"location":"reference/model_config/","title":"Model Config","text":""},{"location":"reference/model_config/#model-config","title":"Model Config","text":""},{"location":"reference/model_config/#pytriton.model_config.ModelConfig","title":"pytriton.model_config.ModelConfig  <code>dataclass</code>","text":"<pre><code>ModelConfig(batching: bool = True, max_batch_size: int = 4, batcher: DynamicBatcher = DynamicBatcher(), response_cache: bool = False, decoupled: bool = False)\n</code></pre> <p>Additional model configuration for running model through Triton Inference Server.</p> <p>Parameters:</p> <ul> <li> <code>batching</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to enable/disable batching for model.</p> </li> <li> <code>max_batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The maximal batch size that would be handled by model.</p> </li> <li> <code>batcher</code>               (<code>DynamicBatcher</code>, default:                   <code>DynamicBatcher()</code> )           \u2013            <p>Configuration of Dynamic Batching for the model.</p> </li> <li> <code>response_cache</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag to enable/disable response cache for the model</p> </li> <li> <code>decoupled</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag to enable/disable decoupled from requests execution</p> </li> </ul>"},{"location":"reference/model_config/#pytriton.model_config.Tensor","title":"pytriton.model_config.Tensor  <code>dataclass</code>","text":"<pre><code>Tensor(shape: tuple, dtype: Union[dtype, Type[dtype], Type[object]], name: Optional[str] = None, optional: Optional[bool] = False)\n</code></pre> <p>Model input and output definition for Triton deployment.</p> <p>Parameters:</p> <ul> <li> <code>shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of the input/output tensor.</p> </li> <li> <code>dtype</code>               (<code>Union[dtype, Type[dtype], Type[object]]</code>)           \u2013            <p>Data type of the input/output tensor.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the input/output of model.</p> </li> <li> <code>optional</code>               (<code>Optional[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Flag to mark if input is optional.</p> </li> </ul>"},{"location":"reference/model_config/#pytriton.model_config.Tensor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Override object values on post init or field override.</p> Source code in <code>pytriton/model_config/tensor.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Override object values on post init or field override.\"\"\"\n    if isinstance(self.dtype, np.dtype):\n        object.__setattr__(self, \"dtype\", self.dtype.type)  # pytype: disable=attribute-error\n</code></pre>"},{"location":"reference/model_config/#pytriton.model_config.DeviceKind","title":"pytriton.model_config.DeviceKind","text":"<p>               Bases: <code>Enum</code></p> <p>Device kind for model deployment.</p> <p>Parameters:</p> <ul> <li> <code>KIND_AUTO</code>           \u2013            <p>Automatically select the device for model deployment.</p> </li> <li> <code>KIND_CPU</code>           \u2013            <p>Model is deployed on CPU.</p> </li> <li> <code>KIND_GPU</code>           \u2013            <p>Model is deployed on GPU.</p> </li> </ul>"},{"location":"reference/model_config/#pytriton.model_config.DynamicBatcher","title":"pytriton.model_config.DynamicBatcher  <code>dataclass</code>","text":"<pre><code>DynamicBatcher(max_queue_delay_microseconds: int = 0, preferred_batch_size: Optional[list] = None, preserve_ordering: bool = False, priority_levels: int = 0, default_priority_level: int = 0, default_queue_policy: Optional[QueuePolicy] = None, priority_queue_policy: Optional[Dict[int, QueuePolicy]] = None)\n</code></pre> <p>Dynamic batcher configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> <ul> <li> <code>max_queue_delay_microseconds</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> </li> <li> <code>preferred_batch_size</code>               (<code>Optional[list]</code>, default:                   <code>None</code> )           \u2013            <p>Preferred batch sizes for dynamic batching.</p> </li> <li> <code>preserve_ordering</code>           \u2013            <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> </li> <li> <code>priority_levels</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The number of priority levels to be enabled for the model.</p> </li> <li> <code>default_priority_level</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The priority level used for requests that don't specify their priority.</p> </li> <li> <code>default_queue_policy</code>               (<code>Optional[QueuePolicy]</code>, default:                   <code>None</code> )           \u2013            <p>The default queue policy used for requests.</p> </li> <li> <code>priority_queue_policy</code>               (<code>Optional[Dict[int, QueuePolicy]]</code>, default:                   <code>None</code> )           \u2013            <p>Specify the queue policy for the priority level.</p> </li> </ul>"},{"location":"reference/model_config/#pytriton.model_config.QueuePolicy","title":"pytriton.model_config.QueuePolicy  <code>dataclass</code>","text":"<pre><code>QueuePolicy(timeout_action: TimeoutAction = REJECT, default_timeout_microseconds: int = 0, allow_timeout_override: bool = False, max_queue_size: int = 0)\n</code></pre> <p>Model queue policy configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> <ul> <li> <code>timeout_action</code>               (<code>TimeoutAction</code>, default:                   <code>REJECT</code> )           \u2013            <p>The action applied to timed-out request.</p> </li> <li> <code>default_timeout_microseconds</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The default timeout for every request, in microseconds.</p> </li> <li> <code>allow_timeout_override</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether individual request can override the default timeout value.</p> </li> <li> <code>max_queue_size</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum queue size for holding requests.</p> </li> </ul>"},{"location":"reference/model_config/#pytriton.model_config.TimeoutAction","title":"pytriton.model_config.TimeoutAction","text":"<p>               Bases: <code>Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Parameters:</p> <ul> <li> <code>REJECT</code>           \u2013            <p>Reject the request and return error message accordingly.</p> </li> <li> <code>DELAY</code>           \u2013            <p>Delay the request until all other requests at the same (or higher) priority levels that have not reached their timeouts are processed.</p> </li> </ul>"},{"location":"reference/triton/","title":"Triton","text":""},{"location":"reference/triton/#triton","title":"Triton","text":""},{"location":"reference/triton/#pytriton.triton.Triton","title":"pytriton.triton.Triton","text":"<pre><code>Triton(*, config: Optional[TritonConfig] = None, workspace: Union[Workspace, str, Path, None] = None, triton_lifecycle_policy: Optional[TritonLifecyclePolicy] = None)\n</code></pre> <p>               Bases: <code>TritonBase</code></p> <p>Triton Inference Server for Python models.</p> <p>Initialize Triton Inference Server context for starting server and loading models.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>Optional[TritonConfig]</code>, default:                   <code>None</code> )           \u2013            <p>TritonConfig object with optional customizations for Triton Inference Server. Configuration can be passed also through environment variables. See TritonConfig.from_env() class method for details.</p> <p>Order of precedence:</p> <ul> <li>config defined through <code>config</code> parameter of init method.</li> <li>config defined in environment variables</li> <li>default TritonConfig values</li> </ul> </li> <li> <code>workspace</code>               (<code>Union[Workspace, str, Path, None]</code>, default:                   <code>None</code> )           \u2013            <p>workspace or path where the Triton Model Store and files used by pytriton will be created. If workspace is <code>None</code> random workspace will be created. Workspace will be deleted in Triton.stop().</p> </li> <li> <code>triton_lifecycle_policy</code>               (<code>Optional[TritonLifecyclePolicy]</code>, default:                   <code>None</code> )           \u2013            <p>policy indicating when Triton server is launched and where the model store is located (locally or remotely managed by Triton server). If triton_lifecycle_policy is None, DefaultTritonLifecyclePolicy is used by default (Triton server is launched on startup and model store is not local). Only if triton_lifecycle_policy is None and config.allow_vertex_ai is True, VertextAILifecyclePolicy is used instead.</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def __init__(\n    self,\n    *,\n    config: Optional[TritonConfig] = None,\n    workspace: Union[Workspace, str, pathlib.Path, None] = None,\n    triton_lifecycle_policy: Optional[TritonLifecyclePolicy] = None,\n):\n    \"\"\"Initialize Triton Inference Server context for starting server and loading models.\n\n    Args:\n        config: TritonConfig object with optional customizations for Triton Inference Server.\n            Configuration can be passed also through environment variables.\n            See [TritonConfig.from_env()][pytriton.triton.TritonConfig.from_env] class method for details.\n\n            Order of precedence:\n\n              - config defined through `config` parameter of init method.\n              - config defined in environment variables\n              - default TritonConfig values\n        workspace: workspace or path where the Triton Model Store and files used by pytriton will be created.\n            If workspace is `None` random workspace will be created.\n            Workspace will be deleted in [Triton.stop()][pytriton.triton.Triton.stop].\n        triton_lifecycle_policy:  policy indicating when Triton server is launched and where the model store is located\n            (locally or remotely managed by Triton server). If triton_lifecycle_policy is None,\n            DefaultTritonLifecyclePolicy is used by default (Triton server is launched on startup and model store is not local).\n            Only if triton_lifecycle_policy is None and config.allow_vertex_ai is True, VertextAILifecyclePolicy is used instead.\n    \"\"\"\n    _triton_lifecycle_policy = (\n        VertextAILifecyclePolicy\n        if triton_lifecycle_policy is None and config is not None and config.allow_vertex_ai\n        else triton_lifecycle_policy\n    ) or DefaultTritonLifecyclePolicy\n\n    def _without_none_values(_d):\n        return {name: value for name, value in _d.items() if value is not None}\n\n    default_config_dict = _without_none_values(TritonConfig().to_dict())\n    env_config_dict = _without_none_values(TritonConfig.from_env().to_dict())\n    explicit_config_dict = _without_none_values(config.to_dict() if config else {})\n    config_dict = {**default_config_dict, **env_config_dict, **explicit_config_dict}\n    self._config = TritonConfig(**config_dict)\n    workspace_instance = workspace if isinstance(workspace, Workspace) else Workspace(workspace)\n    self._prepare_triton_config(workspace_instance)\n    endpoint_protocol = \"http\" if self._config.allow_http in [True, None] else \"grpc\"\n    super().__init__(\n        url=endpoint_utils.get_endpoint(self._triton_server_config, endpoint_protocol),\n        workspace=workspace_instance,\n        triton_lifecycle_policy=_triton_lifecycle_policy,\n    )\n    self._triton_server = None\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Triton\n</code></pre> <p>Entering the context launches the triton server.</p> <p>Returns:</p> <ul> <li> <code>Triton</code>           \u2013            <p>A Triton object</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def __enter__(self) -&gt; \"Triton\":\n    \"\"\"Entering the context launches the triton server.\n\n    Returns:\n        A Triton object\n    \"\"\"\n    if self._triton_lifecycle_policy.launch_triton_on_startup:\n        self._run_server()\n    super().__enter__()\n    return self\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.__exit__","title":"__exit__","text":"<pre><code>__exit__(*_) -&gt; None\n</code></pre> <p>Exit the context stopping the process and cleaning the workspace.</p> <p>Parameters:</p> <ul> <li> <code>*_</code>           \u2013            <p>unused arguments</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def __exit__(self, *_) -&gt; None:\n    \"\"\"Exit the context stopping the process and cleaning the workspace.\n\n    Args:\n        *_: unused arguments\n    \"\"\"\n    self.stop()\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.bind","title":"bind","text":"<pre><code>bind(model_name: str, infer_func: Union[Callable, Sequence[Callable]], inputs: Sequence[Tensor], outputs: Sequence[Tensor], model_version: int = 1, config: Optional[ModelConfig] = None, strict: bool = False, trace_config: Optional[List[str]] = None) -&gt; None\n</code></pre> <p>Create a model with given name and inference callable binding into Triton Inference Server.</p> <p>More information about model configuration: https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md</p> <p>Parameters:</p> <ul> <li> <code>infer_func</code>               (<code>Union[Callable, Sequence[Callable]]</code>)           \u2013            <p>Inference callable to handle request/response from Triton Inference Server</p> </li> <li> <code>inputs</code>               (<code>Sequence[Tensor]</code>)           \u2013            <p>Definition of model inputs</p> </li> <li> <code>outputs</code>               (<code>Sequence[Tensor]</code>)           \u2013            <p>Definition of model outputs</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Name under which model is available in Triton Inference Server. It can only contain</p> </li> <li> <code>model_version</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Version of model</p> </li> <li> <code>config</code>               (<code>Optional[ModelConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Model configuration for Triton Inference Server deployment</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable strict validation between model config outputs and inference function result</p> </li> <li> <code>trace_config</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of trace config parameters</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def bind(\n    self,\n    model_name: str,\n    infer_func: Union[Callable, Sequence[Callable]],\n    inputs: Sequence[Tensor],\n    outputs: Sequence[Tensor],\n    model_version: int = 1,\n    config: Optional[ModelConfig] = None,\n    strict: bool = False,\n    trace_config: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"Create a model with given name and inference callable binding into Triton Inference Server.\n\n    More information about model configuration:\n    https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md\n\n    Args:\n        infer_func: Inference callable to handle request/response from Triton Inference Server\n        (or list of inference callable for multi instance model)\n        inputs: Definition of model inputs\n        outputs: Definition of model outputs\n        model_name: Name under which model is available in Triton Inference Server. It can only contain\n        alphanumeric characters, dots, underscores and dashes.\n        model_version: Version of model\n        config: Model configuration for Triton Inference Server deployment\n        strict: Enable strict validation between model config outputs and inference function result\n        trace_config: List of trace config parameters\n    \"\"\"\n    self._validate_model_name(model_name)\n    model_kwargs = {}\n    if trace_config is None:\n        triton_config = getattr(self, \"_config\", None)\n        if triton_config is not None:\n            trace_config = getattr(triton_config, \"trace_config\", None)\n            if trace_config is not None:\n                LOGGER.info(f\"Using trace config from TritonConfig: {trace_config}\")\n                model_kwargs[\"trace_config\"] = trace_config\n    else:\n        model_kwargs[\"trace_config\"] = trace_config\n    telemetry_tracer = get_telemetry_tracer()\n\n    # Automatically set telemetry tracer if not set at the proxy side\n    if telemetry_tracer is None and trace_config is not None:\n        LOGGER.info(\"Setting telemetry tracer from TritonConfig\")\n        telemetry_tracer = build_proxy_tracer_from_triton_config(trace_config)\n        set_telemetry_tracer(telemetry_tracer)\n\n    model = Model(\n        model_name=model_name,\n        model_version=model_version,\n        inference_fn=infer_func,\n        inputs=inputs,\n        outputs=outputs,\n        config=config if config else ModelConfig(),\n        workspace=self._workspace,\n        triton_context=self._triton_context,\n        strict=strict,\n        **model_kwargs,\n    )\n    model.on_model_event(self._on_model_event)\n\n    self._model_manager.add_model(model, self.is_connected())\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Connect to Triton Inference Server.</p> <p>Raises:</p> <ul> <li> <code>TimeoutError</code>             \u2013            <p>if Triton Inference Server is not ready after timeout</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Connect to Triton Inference Server.\n\n    Raises:\n        TimeoutError: if Triton Inference Server is not ready after timeout\n    \"\"\"\n    with self._cv:\n        if self._connected:\n            LOGGER.debug(\"Triton Inference already connected.\")\n            return\n\n        self._wait_for_server()\n        if self._triton_lifecycle_policy.local_model_store:\n            self._model_manager.setup_models()\n        else:\n            self._model_manager.load_models()\n\n        self._wait_for_models()\n        self._connected = True\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.is_alive","title":"is_alive","text":"<pre><code>is_alive() -&gt; bool\n</code></pre> <p>Check if Triton Inference Server is alive.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def is_alive(self) -&gt; bool:\n    \"\"\"Check if Triton Inference Server is alive.\"\"\"\n    if not self._is_alive_impl():\n        return False\n\n    for model in self._model_manager.models:\n        if not model.is_alive():\n            return False\n    return True\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.is_connected","title":"is_connected","text":"<pre><code>is_connected() -&gt; bool\n</code></pre> <p>Check if Triton Inference Server is connected.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Check if Triton Inference Server is connected.\"\"\"\n    with self._cv:\n        return self._connected\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run Triton Inference Server.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Run Triton Inference Server.\"\"\"\n    self._run_server()\n    self.connect()\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.serve","title":"serve","text":"<pre><code>serve(monitoring_period_s: float = MONITORING_PERIOD_S) -&gt; None\n</code></pre> <p>Run Triton Inference Server and lock thread for serving requests/response.</p> <p>Parameters:</p> <ul> <li> <code>monitoring_period_s</code>               (<code>float</code>, default:                   <code>MONITORING_PERIOD_S</code> )           \u2013            <p>the timeout of monitoring if Triton and models are available. Every monitoring_period_s seconds main thread wakes up and check if triton server and proxy backend are still alive and sleep again. If triton or proxy is not alive - method returns.</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def serve(self, monitoring_period_s: float = MONITORING_PERIOD_S) -&gt; None:\n    \"\"\"Run Triton Inference Server and lock thread for serving requests/response.\n\n    Args:\n        monitoring_period_s: the timeout of monitoring if Triton and models are available.\n            Every monitoring_period_s seconds main thread wakes up and check if triton server and proxy backend\n            are still alive and sleep again. If triton or proxy is not alive - method returns.\n    \"\"\"\n    self._run_server()\n    super().serve(monitoring_period_s=monitoring_period_s)\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.Triton.stop","title":"stop","text":"<pre><code>stop() -&gt; bool\n</code></pre> <p>Stop Triton Inference Server and clean workspace.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def stop(self) -&gt; bool:\n    \"\"\"Stop Triton Inference Server and clean workspace.\"\"\"\n    with self._cv:\n        if self._stopped:\n            LOGGER.debug(\"Triton Inference already stopped.\")\n            return False\n        self._stopped = True\n        self._connected = False\n        atexit.unregister(self.stop)\n    self._pre_stop_impl()\n    self._model_manager.clean()\n    self._workspace.clean()\n\n    with self._cv:\n        self._cv.notify_all()\n    LOGGER.debug(\"Stopped Triton Inference server and proxy backends\")\n    self._log_level_checker.check(skip_update=True)\n\n    return True\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton","title":"pytriton.triton.RemoteTriton","text":"<pre><code>RemoteTriton(url: str, workspace: Union[Workspace, str, Path, None] = None)\n</code></pre> <p>               Bases: <code>TritonBase</code></p> <p>RemoteTriton connects to Triton Inference Server running on remote host.</p> <p>Initialize RemoteTriton.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>Triton Inference Server URL in form of ://: If scheme is not provided, http is used as default. If port is not provided, 8000 is used as default for http and 8001 for grpc. <li> <code>workspace</code>               (<code>Union[Workspace, str, Path, None]</code>, default:                   <code>None</code> )           \u2013            <p>path to be created where the files used by pytriton will be stored (e.g. socket files for communication). If workspace is <code>None</code> temporary workspace will be created. Workspace should be created in shared filesystem space between RemoteTriton and Triton Inference Server to allow access to socket files (if you use containers, folder must be shared between containers).</p> </li> Source code in <code>pytriton/triton.py</code> <pre><code>def __init__(self, url: str, workspace: Union[Workspace, str, pathlib.Path, None] = None):\n    \"\"\"Initialize RemoteTriton.\n\n    Args:\n        url: Triton Inference Server URL in form of &lt;scheme&gt;://&lt;host&gt;:&lt;port&gt;\n            If scheme is not provided, http is used as default.\n            If port is not provided, 8000 is used as default for http and 8001 for grpc.\n        workspace: path to be created where the files used by pytriton will be stored\n            (e.g. socket files for communication).\n            If workspace is `None` temporary workspace will be created.\n            Workspace should be created in shared filesystem space between RemoteTriton\n            and Triton Inference Server to allow access to socket files\n            (if you use containers, folder must be shared between containers).\n\n    \"\"\"\n    super().__init__(\n        url=TritonUrl.from_url(url).with_scheme,\n        workspace=workspace,\n        triton_lifecycle_policy=TritonLifecyclePolicy(launch_triton_on_startup=True, local_model_store=False),\n    )\n\n    with self._cv:\n        self._stopped = False\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; RemoteTriton\n</code></pre> <p>Entering the context connects to remote Triton server.</p> <p>Returns:</p> <ul> <li> <code>RemoteTriton</code>           \u2013            <p>A RemoteTriton object</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def __enter__(self) -&gt; \"RemoteTriton\":\n    \"\"\"Entering the context connects to remote Triton server.\n\n    Returns:\n        A RemoteTriton object\n    \"\"\"\n    super().__enter__()\n    return self\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton.__exit__","title":"__exit__","text":"<pre><code>__exit__(*_) -&gt; None\n</code></pre> <p>Exit the context stopping the process and cleaning the workspace.</p> <p>Parameters:</p> <ul> <li> <code>*_</code>           \u2013            <p>unused arguments</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def __exit__(self, *_) -&gt; None:\n    \"\"\"Exit the context stopping the process and cleaning the workspace.\n\n    Args:\n        *_: unused arguments\n    \"\"\"\n    self.stop()\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton.bind","title":"bind","text":"<pre><code>bind(model_name: str, infer_func: Union[Callable, Sequence[Callable]], inputs: Sequence[Tensor], outputs: Sequence[Tensor], model_version: int = 1, config: Optional[ModelConfig] = None, strict: bool = False, trace_config: Optional[List[str]] = None) -&gt; None\n</code></pre> <p>Create a model with given name and inference callable binding into Triton Inference Server.</p> <p>More information about model configuration: https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md</p> <p>Parameters:</p> <ul> <li> <code>infer_func</code>               (<code>Union[Callable, Sequence[Callable]]</code>)           \u2013            <p>Inference callable to handle request/response from Triton Inference Server</p> </li> <li> <code>inputs</code>               (<code>Sequence[Tensor]</code>)           \u2013            <p>Definition of model inputs</p> </li> <li> <code>outputs</code>               (<code>Sequence[Tensor]</code>)           \u2013            <p>Definition of model outputs</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Name under which model is available in Triton Inference Server. It can only contain</p> </li> <li> <code>model_version</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Version of model</p> </li> <li> <code>config</code>               (<code>Optional[ModelConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Model configuration for Triton Inference Server deployment</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable strict validation between model config outputs and inference function result</p> </li> <li> <code>trace_config</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of trace config parameters</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def bind(\n    self,\n    model_name: str,\n    infer_func: Union[Callable, Sequence[Callable]],\n    inputs: Sequence[Tensor],\n    outputs: Sequence[Tensor],\n    model_version: int = 1,\n    config: Optional[ModelConfig] = None,\n    strict: bool = False,\n    trace_config: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"Create a model with given name and inference callable binding into Triton Inference Server.\n\n    More information about model configuration:\n    https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md\n\n    Args:\n        infer_func: Inference callable to handle request/response from Triton Inference Server\n        (or list of inference callable for multi instance model)\n        inputs: Definition of model inputs\n        outputs: Definition of model outputs\n        model_name: Name under which model is available in Triton Inference Server. It can only contain\n        alphanumeric characters, dots, underscores and dashes.\n        model_version: Version of model\n        config: Model configuration for Triton Inference Server deployment\n        strict: Enable strict validation between model config outputs and inference function result\n        trace_config: List of trace config parameters\n    \"\"\"\n    self._validate_model_name(model_name)\n    model_kwargs = {}\n    if trace_config is None:\n        triton_config = getattr(self, \"_config\", None)\n        if triton_config is not None:\n            trace_config = getattr(triton_config, \"trace_config\", None)\n            if trace_config is not None:\n                LOGGER.info(f\"Using trace config from TritonConfig: {trace_config}\")\n                model_kwargs[\"trace_config\"] = trace_config\n    else:\n        model_kwargs[\"trace_config\"] = trace_config\n    telemetry_tracer = get_telemetry_tracer()\n\n    # Automatically set telemetry tracer if not set at the proxy side\n    if telemetry_tracer is None and trace_config is not None:\n        LOGGER.info(\"Setting telemetry tracer from TritonConfig\")\n        telemetry_tracer = build_proxy_tracer_from_triton_config(trace_config)\n        set_telemetry_tracer(telemetry_tracer)\n\n    model = Model(\n        model_name=model_name,\n        model_version=model_version,\n        inference_fn=infer_func,\n        inputs=inputs,\n        outputs=outputs,\n        config=config if config else ModelConfig(),\n        workspace=self._workspace,\n        triton_context=self._triton_context,\n        strict=strict,\n        **model_kwargs,\n    )\n    model.on_model_event(self._on_model_event)\n\n    self._model_manager.add_model(model, self.is_connected())\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Connect to Triton Inference Server.</p> <p>Raises:</p> <ul> <li> <code>TimeoutError</code>             \u2013            <p>if Triton Inference Server is not ready after timeout</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Connect to Triton Inference Server.\n\n    Raises:\n        TimeoutError: if Triton Inference Server is not ready after timeout\n    \"\"\"\n    with self._cv:\n        if self._connected:\n            LOGGER.debug(\"Triton Inference already connected.\")\n            return\n\n        self._wait_for_server()\n        if self._triton_lifecycle_policy.local_model_store:\n            self._model_manager.setup_models()\n        else:\n            self._model_manager.load_models()\n\n        self._wait_for_models()\n        self._connected = True\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton.is_alive","title":"is_alive","text":"<pre><code>is_alive() -&gt; bool\n</code></pre> <p>Check if Triton Inference Server is alive.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def is_alive(self) -&gt; bool:\n    \"\"\"Check if Triton Inference Server is alive.\"\"\"\n    if not self._is_alive_impl():\n        return False\n\n    for model in self._model_manager.models:\n        if not model.is_alive():\n            return False\n    return True\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton.is_connected","title":"is_connected","text":"<pre><code>is_connected() -&gt; bool\n</code></pre> <p>Check if Triton Inference Server is connected.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Check if Triton Inference Server is connected.\"\"\"\n    with self._cv:\n        return self._connected\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton.serve","title":"serve","text":"<pre><code>serve(monitoring_period_s: float = MONITORING_PERIOD_S) -&gt; None\n</code></pre> <p>Run Triton Inference Server and lock thread for serving requests/response.</p> <p>Parameters:</p> <ul> <li> <code>monitoring_period_s</code>               (<code>float</code>, default:                   <code>MONITORING_PERIOD_S</code> )           \u2013            <p>the timeout of monitoring if Triton and models are available. Every monitoring_period_s seconds main thread wakes up and check if triton server and proxy backend are still alive and sleep again. If triton or proxy is not alive - method returns.</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>def serve(self, monitoring_period_s: float = MONITORING_PERIOD_S) -&gt; None:\n    \"\"\"Run Triton Inference Server and lock thread for serving requests/response.\n\n    Args:\n        monitoring_period_s: the timeout of monitoring if Triton and models are available.\n            Every monitoring_period_s seconds main thread wakes up and check if triton server and proxy backend\n            are still alive and sleep again. If triton or proxy is not alive - method returns.\n    \"\"\"\n    self.connect()\n    with self._cv:\n        try:\n            while self.is_alive():\n                self._cv.wait(timeout=monitoring_period_s)\n        except KeyboardInterrupt:\n            LOGGER.info(\"SIGINT received, exiting.\")\n        self.stop()\n</code></pre>"},{"location":"reference/triton/#pytriton.triton.RemoteTriton.stop","title":"stop","text":"<pre><code>stop() -&gt; bool\n</code></pre> <p>Stop Triton Inference Server and clean workspace.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def stop(self) -&gt; bool:\n    \"\"\"Stop Triton Inference Server and clean workspace.\"\"\"\n    with self._cv:\n        if self._stopped:\n            LOGGER.debug(\"Triton Inference already stopped.\")\n            return False\n        self._stopped = True\n        self._connected = False\n        atexit.unregister(self.stop)\n    self._pre_stop_impl()\n    self._model_manager.clean()\n    self._workspace.clean()\n\n    with self._cv:\n        self._cv.notify_all()\n    LOGGER.debug(\"Stopped Triton Inference server and proxy backends\")\n    self._log_level_checker.check(skip_update=True)\n\n    return True\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request","title":"pytriton.proxy.types.Request  <code>dataclass</code>","text":"<pre><code>Request(data: Dict[str, ndarray], parameters: Optional[Dict[str, Union[str, int, bool]]] = None, span: Optional[Any] = None, requested_output_names: Optional[List[str]] = None)\n</code></pre> <p>Data class for request data including numpy array inputs.</p>"},{"location":"reference/triton/#pytriton.proxy.types.Request.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: Dict[str, ndarray]\n</code></pre> <p>Input data for the request.</p>"},{"location":"reference/triton/#pytriton.proxy.types.Request.parameters","title":"parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameters: Optional[Dict[str, Union[str, int, bool]]] = None\n</code></pre> <p>Parameters for the request.</p>"},{"location":"reference/triton/#pytriton.proxy.types.Request.requested_output_names","title":"requested_output_names  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requested_output_names: Optional[List[str]] = None\n</code></pre> <p>Requested output names for the request.</p>"},{"location":"reference/triton/#pytriton.proxy.types.Request.span","title":"span  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>span: Optional[Any] = None\n</code></pre> <p>Telemetry span for request</p>"},{"location":"reference/triton/#pytriton.proxy.types.Request.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(input_name: str)\n</code></pre> <p>Delete input data from request.</p> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def __delitem__(self, input_name: str):\n    \"\"\"Delete input data from request.\"\"\"\n    del self.data[input_name]\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(input_name: str) -&gt; ndarray\n</code></pre> <p>Get input data.</p> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def __getitem__(self, input_name: str) -&gt; np.ndarray:\n    \"\"\"Get input data.\"\"\"\n    return self.data[input_name]\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate over input names.</p> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over input names.\"\"\"\n    return iter(self.data)\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Get number of inputs.</p> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def __len__(self):\n    \"\"\"Get number of inputs.\"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(input_name: str, input_data: ndarray)\n</code></pre> <p>Set input data.</p> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def __setitem__(self, input_name: str, input_data: np.ndarray):\n    \"\"\"Set input data.\"\"\"\n    self.data[input_name] = input_data\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>Iterate over input names and data.</p> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def items(self):\n    \"\"\"Iterate over input names and data.\"\"\"\n    return self.data.items()\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>Iterate over input names.</p> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def keys(self):\n    \"\"\"Iterate over input names.\"\"\"\n    return self.data.keys()\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request.traced_span","title":"traced_span","text":"<pre><code>traced_span(span_name)\n</code></pre> <p>Yields Open Telemetry a span for the request.</p> <p>Parameters:</p> <ul> <li> <code>span_name</code>               (<code>str</code>)           \u2013            <p>Name of the span</p> </li> </ul> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def traced_span(self, span_name):\n    \"\"\"Yields Open Telemetry a span for the request.\n\n    Args:\n        span_name (str): Name of the span\n    \"\"\"\n    return traced_span(self, span_name)\n</code></pre>"},{"location":"reference/triton/#pytriton.proxy.types.Request.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>Iterate over input data.</p> Source code in <code>pytriton/proxy/types.py</code> <pre><code>def values(self):\n    \"\"\"Iterate over input data.\"\"\"\n    return self.data.values()\n</code></pre>"},{"location":"reference/triton_config/","title":"Triton Config","text":""},{"location":"reference/triton_config/#tritonconfig","title":"TritonConfig","text":""},{"location":"reference/triton_config/#pytriton.triton.TritonConfig","title":"pytriton.triton.TritonConfig  <code>dataclass</code>","text":"<pre><code>TritonConfig(model_repository: Optional[Path] = None, id: Optional[str] = None, log_verbose: Optional[int] = None, log_file: Optional[Path] = None, exit_timeout_secs: Optional[int] = None, exit_on_error: Optional[bool] = None, strict_readiness: Optional[bool] = None, allow_http: Optional[bool] = None, http_address: Optional[str] = None, http_port: Optional[int] = None, http_header_forward_pattern: Optional[str] = None, http_thread_count: Optional[int] = None, allow_grpc: Optional[bool] = None, grpc_address: Optional[str] = None, grpc_port: Optional[int] = None, grpc_header_forward_pattern: Optional[str] = None, grpc_infer_allocation_pool_size: Optional[int] = None, grpc_use_ssl: Optional[bool] = None, grpc_use_ssl_mutual: Optional[bool] = None, grpc_server_cert: Optional[Path] = None, grpc_server_key: Optional[Path] = None, grpc_root_cert: Optional[Path] = None, grpc_infer_response_compression_level: Optional[str] = None, grpc_keepalive_time: Optional[int] = None, grpc_keepalive_timeout: Optional[int] = None, grpc_keepalive_permit_without_calls: Optional[bool] = None, grpc_http2_max_pings_without_data: Optional[int] = None, grpc_http2_min_recv_ping_interval_without_data: Optional[int] = None, grpc_http2_max_ping_strikes: Optional[int] = None, allow_metrics: Optional[bool] = None, allow_gpu_metrics: Optional[bool] = None, allow_cpu_metrics: Optional[bool] = None, metrics_interval_ms: Optional[int] = None, metrics_port: Optional[int] = None, metrics_address: Optional[str] = None, allow_sagemaker: Optional[bool] = None, sagemaker_port: Optional[int] = None, sagemaker_safe_port_range: Optional[str] = None, sagemaker_thread_count: Optional[int] = None, allow_vertex_ai: Optional[bool] = None, vertex_ai_port: Optional[int] = None, vertex_ai_thread_count: Optional[int] = None, vertex_ai_default_model: Optional[str] = None, metrics_config: Optional[List[str]] = None, trace_config: Optional[List[str]] = None, cache_config: Optional[List[str]] = None, cache_directory: Optional[str] = None, buffer_manager_thread_count: Optional[int] = None)\n</code></pre> <p>Triton Inference Server configuration class for customization of server execution.</p> <p>The arguments are optional. If value is not provided the defaults for Triton Inference Server are used. Please, refer to https://github.com/triton-inference-server/server/ for more details.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Identifier for this server.</p> </li> <li> <code>log_verbose</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Set verbose logging level. Zero (0) disables verbose logging and values &gt;= 1 enable verbose logging.</p> </li> <li> <code>log_file</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Set the name of the log output file.</p> </li> <li> <code>exit_timeout_secs</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Timeout (in seconds) when exiting to wait for in-flight inferences to finish.</p> </li> <li> <code>exit_on_error</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Exit the inference server if an error occurs during initialization.</p> </li> <li> <code>strict_readiness</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>If true /v2/health/ready endpoint indicates ready if the server is responsive and all models are available.</p> </li> <li> <code>allow_http</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Allow the server to listen for HTTP requests.</p> </li> <li> <code>http_address</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The address for the http server to bind to. Default is 0.0.0.0.</p> </li> <li> <code>http_port</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The port for the server to listen on for HTTP requests. Default is 8000.</p> </li> <li> <code>http_header_forward_pattern</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The regular expression pattern that will be used for forwarding HTTP headers as inference request parameters.</p> </li> <li> <code>http_thread_count</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads handling HTTP requests.</p> </li> <li> <code>allow_grpc</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Allow the server to listen for GRPC requests.</p> </li> <li> <code>grpc_address</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The address for the grpc server to binds to. Default is 0.0.0.0.</p> </li> <li> <code>grpc_port</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The port for the server to listen on for GRPC requests. Default is 8001.</p> </li> <li> <code>grpc_header_forward_pattern</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The regular expression pattern that will be used for forwarding GRPC headers as inference request parameters.</p> </li> <li> <code>grpc_infer_allocation_pool_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of inference request/response objects that remain allocated for reuse. As long as the number of in-flight requests doesn't exceed this value there will be no allocation/deallocation of request/response objects.</p> </li> <li> <code>grpc_use_ssl</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Use SSL authentication for GRPC requests. Default is false.</p> </li> <li> <code>grpc_use_ssl_mutual</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Use mututal SSL authentication for GRPC requests. This option will preempt grpc_use_ssl if it is also specified. Default is false.</p> </li> <li> <code>grpc_server_cert</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>File holding PEM-encoded server certificate. Ignored unless grpc_use_ssl is true.</p> </li> <li> <code>grpc_server_key</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Path to file holding PEM-encoded server key. Ignored unless grpc_use_ssl is true.</p> </li> <li> <code>grpc_root_cert</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Path to file holding PEM-encoded root certificate. Ignored unless grpc_use_ssl is true.</p> </li> <li> <code>grpc_infer_response_compression_level</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The compression level to be used while returning the inference response to the peer. Allowed values are none, low, medium and high. Default is none.</p> </li> <li> <code>grpc_keepalive_time</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The period (in milliseconds) after which a keepalive ping is sent on the transport.</p> </li> <li> <code>grpc_keepalive_timeout</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The period (in milliseconds) the sender of the keepalive ping waits for an acknowledgement.</p> </li> <li> <code>grpc_keepalive_permit_without_calls</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Allows keepalive pings to be sent even if there are no calls in flight</p> </li> <li> <code>grpc_http2_max_pings_without_data</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of pings that can be sent when there is no data/header frame to be sent.</p> </li> <li> <code>grpc_http2_min_recv_ping_interval_without_data</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>If there are no data/header frames being sent on the transport, this channel argument on the server side controls the minimum time (in milliseconds) that gRPC Core would expect between receiving successive pings.</p> </li> <li> <code>grpc_http2_max_ping_strikes</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of bad pings that the server will tolerate before sending an HTTP2 GOAWAY frame and closing the transport.</p> </li> <li> <code>grpc_restricted_protocol</code>           \u2013            <p>Specify restricted GRPC protocol setting. The format of this flag is <code>&lt;protocols&gt;,&lt;key&gt;=&lt;value&gt;</code>. Where <code>&lt;protocol&gt;</code> is a comma-separated list of protocols to be restricted. <code>&lt;key&gt;</code> will be additional header key to be checked when a GRPC request is received, and <code>&lt;value&gt;</code> is the value expected to be matched.</p> </li> <li> <code>allow_metrics</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Allow the server to provide prometheus metrics.</p> </li> <li> <code>allow_gpu_metrics</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Allow the server to provide GPU metrics.</p> </li> <li> <code>allow_cpu_metrics</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Allow the server to provide CPU metrics.</p> </li> <li> <code>metrics_interval_ms</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Metrics will be collected once every <code>&lt;metrics-interval-ms&gt;</code> milliseconds.</p> </li> <li> <code>metrics_port</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The port reporting prometheus metrics.</p> </li> <li> <code>metrics_address</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The address for the metrics server to bind to. Default is the same as http_address.</p> </li> <li> <code>allow_sagemaker</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Allow the server to listen for Sagemaker requests.</p> </li> <li> <code>sagemaker_port</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The port for the server to listen on for Sagemaker requests.</p> </li> <li> <code>sagemaker_safe_port_range</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Set the allowed port range for endpoints other than the SageMaker endpoints.</p> </li> <li> <code>sagemaker_thread_count</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads handling Sagemaker requests.</p> </li> <li> <code>allow_vertex_ai</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Allow the server to listen for Vertex AI requests.</p> </li> <li> <code>vertex_ai_port</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The port for the server to listen on for Vertex AI requests.</p> </li> <li> <code>vertex_ai_thread_count</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads handling Vertex AI requests.</p> </li> <li> <code>vertex_ai_default_model</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the model to use for single-model inference requests.</p> </li> <li> <code>metrics_config</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Specify a metrics-specific configuration setting. The format of this flag is <code>&lt;setting&gt;=&lt;value&gt;</code>. It can be specified multiple times</p> </li> <li> <code>trace_config</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Specify global or trace mode specific configuration setting. The format of this flag is <code>&lt;mode&gt;,&lt;setting&gt;=&lt;value&gt;</code>. Where <code>&lt;mode&gt;</code> is either 'triton' or 'opentelemetry'. The default is 'triton'. To specify global trace settings (level, rate, count, or mode), the format would be <code>&lt;setting&gt;=&lt;value&gt;</code>. For 'triton' mode, the server will use Triton's Trace APIs. For 'opentelemetry' mode, the server will use OpenTelemetry's APIs to generate, collect and export traces for individual inference requests. More details, including supported settings can be found at Triton trace guide.</p> </li> <li> <code>cache_config</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Specify a cache-specific configuration setting. The format of this flag is <code>&lt;cache_name&gt;,&lt;setting&gt;=&lt;value&gt;</code>. Where <code>&lt;cache_name&gt;</code> is the name of the cache, such as 'local' or 'redis'. Example: <code>local,size=1048576</code> will configure a 'local' cache implementation with a fixed buffer pool of size 1048576 bytes.</p> </li> <li> <code>cache_directory</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The global directory searched for cache shared libraries. Default is '/opt/tritonserver/caches'. This directory is expected to contain a cache implementation as a shared library with the name 'libtritoncache.so'.</p> </li> <li> <code>buffer_manager_thread_count</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The number of threads used to accelerate copies and other operations required to manage input and output tensor contents.</p> </li> </ul>"},{"location":"reference/triton_config/#pytriton.triton.TritonConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate configuration for early error handling.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate configuration for early error handling.\"\"\"\n    if self.allow_http not in [True, None] and self.allow_grpc not in [True, None]:\n        raise PyTritonValidationError(\"The `http` or `grpc` endpoint has to be allowed.\")\n</code></pre>"},{"location":"reference/triton_config/#pytriton.triton.TritonConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config: Dict[str, Any]) -&gt; TritonConfig\n</code></pre> <p>Creates a <code>TritonConfig</code> instance from an input dictionary. Values are converted into correct types.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>a dictionary with all required fields</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TritonConfig</code>           \u2013            <p>a <code>TritonConfig</code> instance</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: Dict[str, Any]) -&gt; \"TritonConfig\":\n    \"\"\"Creates a ``TritonConfig`` instance from an input dictionary. Values are converted into correct types.\n\n    Args:\n        config: a dictionary with all required fields\n\n    Returns:\n        a ``TritonConfig`` instance\n    \"\"\"\n    fields: Dict[str, dataclasses.Field] = {field.name: field for field in dataclasses.fields(cls)}\n    unknown_config_parameters = {name: value for name, value in config.items() if name not in fields}\n    for name, value in unknown_config_parameters.items():\n        LOGGER.warning(\n            f\"Ignoring {name}={value} as could not find matching config field. \"\n            f\"Available fields: {', '.join(map(str, fields))}\"\n        )\n\n    def _cast_value(_field, _value):\n        field_type = _field.type\n        is_optional = typing_inspect.is_optional_type(field_type)\n        if is_optional:\n            field_type = field_type.__args__[0]\n        if hasattr(field_type, \"__origin__\") and field_type.__origin__ is list:\n            return list(_value) if _value is not None else None\n        elif isinstance(_value, str) and isinstance(field_type, type) and issubclass(field_type, list):\n            return _value.split(\",\")\n        return field_type(_value)\n\n    config_with_casted_values = {\n        name: _cast_value(fields[name], value) for name, value in config.items() if name in fields\n    }\n    return cls(**config_with_casted_values)\n</code></pre>"},{"location":"reference/triton_config/#pytriton.triton.TritonConfig.from_env","title":"from_env  <code>classmethod</code>","text":"<pre><code>from_env() -&gt; TritonConfig\n</code></pre> <p>Creates TritonConfig from environment variables.</p> <p>Environment variables should start with <code>PYTRITON_TRITON_CONFIG_</code> prefix. For example:</p> <pre><code>PYTRITON_TRITON_CONFIG_GRPC_PORT=45436\nPYTRITON_TRITON_CONFIG_LOG_VERBOSE=4\n</code></pre> <p>Typical use:</p> <pre><code>triton_config = TritonConfig.from_env()\n</code></pre> <p>Returns:</p> <ul> <li> <code>TritonConfig</code>           \u2013            <p>TritonConfig class instantiated from environment variables.</p> </li> </ul> Source code in <code>pytriton/triton.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; \"TritonConfig\":\n    \"\"\"Creates TritonConfig from environment variables.\n\n    Environment variables should start with `PYTRITON_TRITON_CONFIG_` prefix. For example:\n\n        PYTRITON_TRITON_CONFIG_GRPC_PORT=45436\n        PYTRITON_TRITON_CONFIG_LOG_VERBOSE=4\n\n    Typical use:\n\n        triton_config = TritonConfig.from_env()\n\n    Returns:\n        TritonConfig class instantiated from environment variables.\n    \"\"\"\n    prefix = \"PYTRITON_TRITON_CONFIG_\"\n    config = {}\n    list_pattern = re.compile(r\"^(.+?)_(\\d+)$\")\n\n    for name, value in os.environ.items():\n        if name.startswith(prefix):\n            key = name[len(prefix) :].lower()\n            match = list_pattern.match(key)\n            if match:\n                list_key, index = match.groups()\n                index = int(index)\n                if list_key not in config:\n                    config[list_key] = []\n                if len(config[list_key]) &lt;= index:\n                    config[list_key].extend([None] * (index + 1 - len(config[list_key])))\n                config[list_key][index] = value\n            else:\n                config[key] = value\n\n    # Remove None values from lists (in case of non-sequential indexes)\n    for key in config:\n        if isinstance(config[key], list):\n            config[key] = [item for item in config[key] if item is not None]\n\n    return cls.from_dict(config)\n</code></pre>"},{"location":"reference/triton_config/#pytriton.triton.TritonConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Map config object to dictionary.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def to_dict(self):\n    \"\"\"Map config object to dictionary.\"\"\"\n    return dataclasses.asdict(self)\n</code></pre>"}]}